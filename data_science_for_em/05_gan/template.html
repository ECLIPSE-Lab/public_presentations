<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-927edb3cde42616945691bbf0360b549.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.30">

  <meta name="author" content="Philipp Pelz">
  <title>ECLIPSE Presentations – Data Science for Electron Microscopy  Lecture 5: Rotationally Invariant Variational Autoencoders &amp; Generative Adversarial Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #97947a;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #97947a;  padding-left: 4px; }
    div.sourceCode
      { color: #f8f8f2; background-color: #2b2b2b; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #f8f8f2; } /* Normal */
    code span.al { color: #dcc6e0; } /* Alert */
    code span.an { color: #d4d0ab; } /* Annotation */
    code span.at { color: #ffd700; } /* Attribute */
    code span.bn { color: #dcc6e0; } /* BaseN */
    code span.bu { color: #f5ab35; } /* BuiltIn */
    code span.cf { color: #ffa07a; } /* ControlFlow */
    code span.ch { color: #abe338; } /* Char */
    code span.cn { color: #ffa07a; } /* Constant */
    code span.co { color: #d4d0ab; } /* Comment */
    code span.cv { color: #d4d0ab; font-style: italic; } /* CommentVar */
    code span.do { color: #d4d0ab; font-style: italic; } /* Documentation */
    code span.dt { color: #dcc6e0; } /* DataType */
    code span.dv { color: #dcc6e0; } /* DecVal */
    code span.er { color: #dcc6e0; } /* Error */
    code span.ex { color: #ffd700; } /* Extension */
    code span.fl { color: #f5ab35; } /* Float */
    code span.fu { color: #ffd700; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #d4d0ab; } /* Information */
    code span.kw { color: #ffa07a; } /* Keyword */
    code span.op { color: #00e0e0; } /* Operator */
    code span.ot { color: #ffa07a; } /* Other */
    code span.pp { color: #dcc6e0; } /* Preprocessor */
    code span.sc { color: #00e0e0; } /* SpecialChar */
    code span.ss { color: #abe338; } /* SpecialString */
    code span.st { color: #abe338; } /* String */
    code span.va { color: #f5ab35; } /* Variable */
    code span.vs { color: #abe338; } /* VerbatimString */
    code span.wa { color: #d4d0ab; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-ce804447b3af982f2c55816970ecc9a3.css">
  <link rel="stylesheet" href="custom.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
    <h1 class="title"><p>Data Science for Electron Microscopy<br> Lecture 5: Rotationally Invariant Variational Autoencoders &amp; Generative Adversarial Networks</p></h1>
    
  <div class="quarto-title-authors">
    <div class="quarto-title-author">
  <div class="quarto-title-author-name">
  Philipp Pelz 
  </div>
                <p class="quarto-title-affiliation">
                FAU Erlangen-Nürnberg
              </p>
          </div>
    </div>
  
    <div class="footer-logos1">
    <img src="logos/FAU.png" alt="FAU Logo" width="20%">
    <img src="logos/imn.png" alt="IMN Logo" width="20%">
    <img src="logos/cenem.png" alt="CENEM Logo" width="20%">
    <img src="logos/erc.jpg" alt="Elettra Logo" width="20%">
  </div>
  </section>
<section id="vae-example-exploring-order-parameters-and-dynamic-processes-in-disordered-systems-via-variational-autoencoders" class="slide level2">
<h2>VAE example: Exploring Order Parameters and Dynamic Processes in Disordered Systems via Variational Autoencoders</h2>
<p><strong>Authors:</strong> Sergei V. Kalinin, Ondrej Dyck, Stephen Jesse, Maxim Ziatdinov<br>
<strong>Published in:</strong> Science Advances (2021)<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1126/sciadv.abd5084">10.1126/sciadv.abd5084</a></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img2/abd5084-f2.jpeg" style="width:80.0%"></p>
<figcaption>Single image from dynamic STEM dataset corresponding to 10th frame</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span></p>
</div></aside></section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<ul>
<li><strong>Objective</strong>: Analyze dynamic processes and order parameters in disordered systems.</li>
<li><strong>Approach</strong>: Use rotationally invariant variational autoencoders (rVAEs).</li>
<li><strong>Application</strong>: Studied e-beam induced processes in silicon-doped graphene.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img2/abd5084-f3.jpeg" style="width:50.0%"></p>
<figcaption>Evolution of graphene under e-beam irradiation.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span></p>
</div></aside></section>
<section id="rotationally-invariant-vaes" class="slide level2">
<h2>Rotationally Invariant VAEs</h2>
<ul>
<li><strong>Purpose</strong>: Handle rotational invariance in noncrystalline solids.</li>
<li><strong>Method</strong>:
<ul>
<li>Incorporate rotational and translational invariance.</li>
<li>Apply rVAEs to semantically segmented, atomically resolved data.</li>
</ul></li>
<li><strong>Benefit</strong>: Captures maximum original information with reduced representation.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img2/spatialGAN.png" style="width:80.0%"></p>
<figcaption>Diagram of the spatial-VAE framework</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span></p>
</div></aside></section>
<section id="rotationally-invariant-vaes-forward" class="slide level2">
<h2>Rotationally Invariant VAEs Forward</h2>
<div class="sourceCode" id="cb1" data-n="30"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="kw">class</span> SpatialGenerator(nn.Module):</span>
<span id="cb1-2"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, z):</span>
<span id="cb1-3"><a></a>        <span class="co"># x is (batch, num_coords, 2)</span></span>
<span id="cb1-4"><a></a>        <span class="co"># z is (batch, latent_dim)</span></span>
<span id="cb1-5"><a></a></span>
<span id="cb1-6"><a></a>        <span class="cf">if</span> <span class="bu">len</span>(x.size()) <span class="op">&lt;</span> <span class="dv">3</span>:</span>
<span id="cb1-7"><a></a>            x <span class="op">=</span> x.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb1-8"><a></a>        b <span class="op">=</span> x.size(<span class="dv">0</span>)</span>
<span id="cb1-9"><a></a>        n <span class="op">=</span> x.size(<span class="dv">1</span>)</span>
<span id="cb1-10"><a></a>        x <span class="op">=</span> x.view(b<span class="op">*</span>n, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-11"><a></a></span>
<span id="cb1-12"><a></a>        h_x <span class="op">=</span> <span class="va">self</span>.coord_linear(x)</span>
<span id="cb1-13"><a></a>        h_x <span class="op">=</span> h_x.view(b, n, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-14"><a></a></span>
<span id="cb1-15"><a></a>        h_z <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-16"><a></a></span>
<span id="cb1-17"><a></a>        <span class="cf">if</span> <span class="bu">len</span>(z.size()) <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb1-18"><a></a>            z <span class="op">=</span> z.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb1-19"><a></a>        h_z <span class="op">=</span> <span class="va">self</span>.latent_linear(z)</span>
<span id="cb1-20"><a></a>        h_z <span class="op">=</span> h_z.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb1-21"><a></a></span>
<span id="cb1-22"><a></a>        h_bi <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-23"><a></a>        h <span class="op">=</span> h_x <span class="op">+</span> h_z <span class="op">+</span> h_bi <span class="co"># (batch, num_coords, hidden_dim)</span></span>
<span id="cb1-24"><a></a>        h <span class="op">=</span> h.view(b<span class="op">*</span>n, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-25"><a></a></span>
<span id="cb1-26"><a></a>        y <span class="op">=</span> <span class="va">self</span>.layers(h) <span class="co"># (batch*num_coords, nout)</span></span>
<span id="cb1-27"><a></a>        y <span class="op">=</span> y.view(b, n, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-28"><a></a></span>
<span id="cb1-29"><a></a>        <span class="cf">if</span> <span class="va">self</span>.softplus: <span class="co"># only apply softplus to first output</span></span>
<span id="cb1-30"><a></a>            y <span class="op">=</span> torch.cat([F.softplus(y[:,:,:<span class="dv">1</span>]), y[:,:,<span class="dv">1</span>:]], <span class="dv">2</span>)</span>
<span id="cb1-31"><a></a></span>
<span id="cb1-32"><a></a>        <span class="cf">return</span> y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="experimental-setup" class="slide level2">
<h2>Experimental Setup</h2>
<ul>
<li><strong>Sample</strong>: Silicon-doped graphene.</li>
<li><strong>Imaging</strong>: Scanning transmission electron microscopy (STEM).</li>
<li><strong>Procedure</strong>:
<ul>
<li>Use DCNN for initial pixel probability maps.</li>
<li>Extract atomic positions for VAE analysis.</li>
</ul></li>
<li><strong>Data</strong>: Multiple snapshots during dynamic processes.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background:rgb(0, 0, 0)">
<figure>
<p><img data-src="img2/abd5084-f1.jpeg" style="width:35.0%"></p>
<figcaption>Schematic of the overall approach.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span></p>
</div></aside></section>
<section id="data-analysis-with-rvae" class="slide level2">
<h2>Data Analysis with rVAE</h2>
<ul>
<li><strong>Workflow</strong>:
<ol type="1">
<li>DCNN categorizes pixels into atomic types.</li>
<li>Generate subimages centered on atomic positions.</li>
<li>rVAE seeks the most effective reduced representation.</li>
</ol></li>
<li><strong>Output</strong>: Identifies key structural elements and their dynamics.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background:rgb(0, 0, 0)">
<figure>
<p><img data-src="img2/abd5084-f1.jpeg" style="width:35.0%"></p>
<figcaption>Workflow</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span></p>
</div></aside></section>
<section id="results" class="slide level2">
<h2>Results</h2>
<ul>
<li><strong>Findings</strong>:
<ul>
<li>Effective exploration of chemical evolution in the system.</li>
<li>rVAE captured rotationally invariant features.</li>
</ul></li>
</ul>
</section>
<section id="comparison-of-methods-for-construction-of-elementary-descriptors." class="slide level2">
<h2>Comparison of methods for construction of elementary descriptors.</h2>
<ul>
<li>(A to C) small and (D to F) large windows.</li>
<li>(A and D) GMM classes of the data that are decomposed into many independent components that are statistical in nature and often do not allow for direct physical interpretation. In (D), this approach performs poorly at capturing rotation and spreads this information across several components. (B and E) Representation in 2D latent space of convolutional AE. Red and blue regions in (B) indicate clear separation of graphene sublattices with remainder of the descriptors encoding lateral shifts, defects, and rotations in a convoluted fashion.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background:rgb(0, 0, 0)">
<figure>
<p><img data-src="img2/abd5084-f5.jpeg" style="width:40.0%"></p>
<figcaption>Adapted from <span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span>: Comparison of methods for construction of elementary descriptors.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span></p>
</div></aside></section>
<section id="comparison-of-methods-for-construction-of-elementary-descriptors.-1" class="slide level2">
<h2>Comparison of methods for construction of elementary descriptors.</h2>
<ul>
<li>In (E), the larger window introduces variability that is more difficult to interpret. (C and F) Representation in the 2D latent space of rotationally invariant VAE. Because rotational variation is removed from elementary descriptors, remaining variations within data can be described much more efficiently.</li>
<li>In (C), there are noticeable changes in only one dimension, which can be ascribed to degree of local crystallinity. In (F), the larger window size also captures variations related to proximity of edges. In (B), (C), (E), and (F), the images were generated by applying a corresponding decoder to the uniform grid of discrete point in the latent space.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background:rgb(0, 0, 0)">
<figure>
<p><img data-src="img2/abd5084-f5.jpeg" style="width:40.0%"></p>
<figcaption>Adapted from <span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span>: Comparison of methods for construction of elementary descriptors.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span></p>
</div></aside></section>
<section id="results-1" class="slide level2">
<h2>Results</h2>
<ul>
<li><strong>Evaluation</strong>:
<ul>
<li>Identified structural changes due to e-beam.</li>
<li>Analyzed formation of 5-7 member defect chains.</li>
<li>Detected migration of Si atoms to graphene edges.</li>
</ul></li>
</ul>
</section>
<section id="comparison-with-other-methods" class="slide level2">
<h2>Comparison with Other Methods</h2>
<ul>
<li><strong>GMM Analysis</strong>:
<ul>
<li>Generates independent components.</li>
<li>Effective for imaging but less interpretable structurally.</li>
</ul></li>
<li><strong>Classical AE</strong>:
<ul>
<li>Reduced data to continuous latent variables.</li>
<li>Convoluted rotation and structural variations.</li>
</ul></li>
<li><strong>rVAE</strong>:
<ul>
<li>Separated rotation and structural changes.</li>
<li>Provided clear physical interpretation.</li>
</ul></li>
</ul>
</section>
<section id="comparison-with-other-methods-2" class="slide level2">
<h2>Comparison with Other Methods 2</h2>
<div class="quarto-figure quarto-figure-center" style="background:rgb(0, 0, 0)">
<figure>
<p><img data-src="img2/VAEvsrVAE.png" style="width:60.0%"></p>
<figcaption>Different embeddings</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="kalinin2021exploring">Kalinin, Sergei V et al. <a href="#/references" role="doc-biblioref" onclick="">(2021)</a></span></p>
</div></aside></section>
<section id="sec-conclusion" class="slide level2">
<h2>Conclusion</h2>
<ul>
<li><strong>Significance</strong>:
<ul>
<li>rVAEs provide a robust framework for analyzing disordered systems.</li>
<li>Effective for bottom-up description of dynamic processes.</li>
</ul></li>
</ul>
</section>
<section>
<section id="generative-adversarial-networks" class="title-slide slide level1 center">
<h1>Generative Adversarial Networks</h1>
<ul>
<li><strong>Discriminative Learning:</strong>
<ul>
<li>Predicts labels from data examples.</li>
<li>Examples: classifiers and regressors.</li>
<li>Deep neural networks have revolutionized discriminative learning, achieving human-level accuracy on high-res images.</li>
</ul></li>
<li><strong>Generative Modeling:</strong>
<ul>
<li>Learns a model to capture data characteristics without labels.</li>
<li>Generates synthetic data resembling the training dataset.</li>
<li>Example: Generating photorealistic images from a dataset of faces.</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><strong>Deep Neural Networks in Generative Modeling:</strong>
<ul>
<li>Recent advances have enabled discriminative models to assist generative tasks.</li>
<li>Example: Recurrent neural network language models.</li>
</ul></li>
<li><strong>Generative Adversarial Networks (GANs):</strong>
<ul>
<li>Introduced in 2014 by Goodfellow et al.</li>
<li>Leverages discriminative models to create generative models.</li>
<li>Concept: A good data generator makes fake data indistinguishable from real data (two-sample test).</li>
<li>GANs use the two-sample test constructively to train generative models.</li>
<li>Aim: Improve the generator until it fools a state-of-the-art classifier.</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<div class="columns">
<div class="column" style="width:40%;">
<div id="fig-gan" class="quarto-float quarto-figure quarto-figure-center" style="background: #333333">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-gan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="./img/gan.svg" style="background: #333333;width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Generative Adversarial Networks
</figcaption>
</figure>
</div>
</div><div class="column" style="width:60%;">
<p>The GAN architecture is illustrated in <a href="#/fig-gan" class="quarto-xref">Figure&nbsp;1</a>.</p>
<ul>
<li><strong>GAN Architecture:</strong>
<ul>
<li><strong>Generator Network:</strong>
<ul>
<li>Generates data resembling real data.</li>
<li>For images: generates images.</li>
<li>For speech: generates audio sequences.</li>
</ul></li>
<li><strong>Discriminator Network:</strong>
<ul>
<li>Distinguishes fake data from real data.</li>
<li>Competes with the generator.</li>
<li>Adaptively improves to distinguish better as the generator improves.</li>
</ul></li>
</ul></li>
</ul>
</div></div>
</section>
<section class="slide level2">

<ul>
<li><strong>Discriminator:</strong>
<ul>
<li>Binary classifier: distinguishes real (<span class="math inline">\(x\)</span>) vs.&nbsp;fake data.</li>
<li>Outputs scalar prediction <span class="math inline">\(o \in \mathbb{R}\)</span> for input <span class="math inline">\(\mathbf{x}\)</span>.</li>
<li>Applies sigmoid function: <span class="math inline">\(D(\mathbf{x}) = \frac{1}{1 + e^{-o}}\)</span>.</li>
<li>True data label <span class="math inline">\(y = 1\)</span>, fake data label <span class="math inline">\(y = 0\)</span>.</li>
<li>Minimize cross-entropy loss: <span class="math display">\[
\min_D \{ - y \log D(\mathbf{x}) - (1-y) \log(1-D(\mathbf{x})) \}
\]</span></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><strong>Generator:</strong>
<ul>
<li>Draws parameter <span class="math inline">\(\mathbf{z} \in \mathbb{R}^d\)</span> from randomness, e.g., <span class="math inline">\(\mathbf{z} \sim \mathcal{N}(0, 1)\)</span> (latent variable).</li>
<li>Generates data: <span class="math inline">\(\mathbf{x}' = G(\mathbf{z})\)</span>.</li>
<li>Aims to fool the discriminator: <span class="math inline">\(D(G(\mathbf{z})) \approx 1\)</span>.</li>
<li>Update parameters to maximize cross-entropy loss for <span class="math inline">\(y = 0\)</span>: <span class="math display">\[
\max_G \{ - \log(1-D(G(\mathbf{z}))) \}
\]</span></li>
<li>Commonly minimize the loss: <span class="math display">\[
\min_G \{ - \log(D(G(\mathbf{z}))) \}
\]</span>
<ul>
<li>This is feeding <span class="math inline">\(\mathbf{x}' = G(\mathbf{z})\)</span> into the discriminator but giving label <span class="math inline">\(y = 1\)</span>.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><strong>Minimax Game:</strong>
<ul>
<li><span class="math inline">\(D\)</span> and <span class="math inline">\(G\)</span> play a “minimax” game with the objective function: <span class="math display">\[
\min_D \max_G \{ -E_{x \sim \text{Data}} \log D(\mathbf{x}) - E_{z \sim \text{Noise}} \log(1 - D(G(\mathbf{z}))) \}
\]</span></li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Many GAN applications are in the context of images.</li>
<li>Demonstration: fitting a simpler distribution.</li>
<li>Example: Using GANs to estimate parameters for a Gaussian.</li>
</ul></li>
</ul>
<p>Let’s get started.</p>
<div id="code-1" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="im">import</span> d2l</span>
<span id="cb2-2"><a></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="generate-some-real-data" class="slide level2">
<h2>Generate Some “Real” Data</h2>
<p>Since this is going to be the world’s lamest example, we simply generate data drawn from a Gaussian.</p>
<div id="c62a7100" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>X <span class="op">=</span> d2l.normal(<span class="fl">0.0</span>, <span class="dv">1</span>, (<span class="dv">1000</span>, <span class="dv">2</span>))</span>
<span id="cb3-2"><a></a>A <span class="op">=</span> d2l.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.5</span>]])</span>
<span id="cb3-3"><a></a>b <span class="op">=</span> d2l.tensor([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb3-4"><a></a>data <span class="op">=</span> d2l.matmul(X, A) <span class="op">+</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see what we got. This should be a Gaussian shifted in some rather arbitrary way with mean <span class="math inline">\(b\)</span> and covariance matrix <span class="math inline">\(A^TA\)</span>.</p>
<div id="79be2488" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a>d2l.set_figsize()</span>
<span id="cb4-2"><a></a>d2l.plt.scatter(d2l.numpy(data[:<span class="dv">100</span>, <span class="dv">0</span>]), d2l.numpy(data[:<span class="dv">100</span>, <span class="dv">1</span>]))<span class="op">;</span></span>
<span id="cb4-3"><a></a><span class="bu">print</span>(<span class="ss">f'The covariance matrix is</span><span class="ch">\n</span><span class="sc">{</span>d2l<span class="sc">.</span>matmul(A.T, A)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The covariance matrix is
tensor([[1.0100, 1.9500],
        [1.9500, 4.2500]])</code></pre>
</div>

</div>
<img data-src="template_files/figure-revealjs/cell-4-output-2.svg" class="r-stretch"><div id="code-2" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb6-2"><a></a>data_iter <span class="op">=</span> d2l.load_array((data,), batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="the-generator" class="slide level2">
<h2>The Generator</h2>
<h3 id="generator-architecture-overview">Generator Architecture Overview</h3>
<ul>
<li><strong>Input</strong>: Noise vector <span class="math inline">\(\mathbf{z} \in \mathbb{R}^d\)</span> (latent space)</li>
<li><strong>Output</strong>: RGB image <span class="math inline">\(64 \times 64\)</span> pixels</li>
<li><strong>Method</strong>: Transposed convolutions to upsample from noise to image</li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Key Components</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Transposed Convolution</strong>: Upsamples spatial dimensions</li>
<li><strong>Batch Normalization</strong>: Stabilizes training</li>
<li><strong>ReLU Activation</strong>: Introduces non-linearity</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="generator-block-implementation">Generator Block Implementation</h3>
<div id="e71fbf03" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="kw">class</span> G_block(nn.Module):</span>
<span id="cb7-2"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_channels, in_channels<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb7-3"><a></a>                 padding<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb7-4"><a></a>        <span class="bu">super</span>(G_block, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb7-5"><a></a>        <span class="va">self</span>.conv2d_trans <span class="op">=</span> nn.ConvTranspose2d(in_channels, out_channels,</span>
<span id="cb7-6"><a></a>                                kernel_size, strides, padding, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-7"><a></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb7-8"><a></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.ReLU()</span>
<span id="cb7-9"><a></a></span>
<span id="cb7-10"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb7-11"><a></a>        <span class="cf">return</span> <span class="va">self</span>.activation(<span class="va">self</span>.batch_norm(<span class="va">self</span>.conv2d_trans(X)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<h3 id="transposed-convolution-parameters">Transposed Convolution Parameters</h3>
<p><strong>Default Configuration:</strong> - Kernel size: <span class="math inline">\(4 \times 4\)</span> - Stride: <span class="math inline">\(2 \times 2\)</span> - Padding: <span class="math inline">\(1 \times 1\)</span></p>
<p><strong>Effect</strong>: Doubles spatial dimensions</p>
<p><strong>Example</strong>: <span class="math inline">\(16 \times 16 \rightarrow 32 \times 32\)</span></p>
<p><span class="math display">\[
\begin{aligned}
n_h^{'} \times n_w^{'} &amp;= [(k_h + s_h (n_h-1)- 2p_h] \times [(k_w + s_w (n_w-1)- 2p_w]\\
  &amp;= [(4 + 2 \times (16-1)- 2 \times 1] \times [(4 + 2 \times (16-1)- 2 \times 1]\\
  &amp;= 32 \times 32
\end{aligned}
\]</span></p>
<div id="adb92cd0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb8-2"><a></a>g_blk <span class="op">=</span> G_block(<span class="dv">20</span>)</span>
<span id="cb8-3"><a></a>g_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>torch.Size([2, 20, 32, 32])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="alternative-configuration">Alternative Configuration</h3>
<p><strong>Parameters:</strong> - Kernel: <span class="math inline">\(4 \times 4\)</span> - Stride: <span class="math inline">\(1 \times 1\)</span> - Padding: <span class="math inline">\(0 \times 0\)</span></p>
<p><strong>Effect</strong>: Increases dimensions by 3</p>
<p><strong>Example</strong>: <span class="math inline">\(1 \times 1 \rightarrow 4 \times 4\)</span></p>
<div id="41c34c30" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb10-2"><a></a>g_blk <span class="op">=</span> G_block(<span class="dv">20</span>, strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-3"><a></a>g_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>torch.Size([2, 20, 4, 4])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="complete-generator-architecture">Complete Generator Architecture</h3>
<p><strong>Progressive Upsampling:</strong> - Start: <span class="math inline">\(1 \times 1\)</span> (latent vector) - End: <span class="math inline">\(64 \times 64\)</span> (final image) - Channel progression: <span class="math inline">\(100 \rightarrow 512 \rightarrow 256 \rightarrow 128 \rightarrow 64 \rightarrow 3\)</span></p>
<div id="dfa0843e" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>n_G <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb12-2"><a></a>net_G <span class="op">=</span> nn.Sequential(</span>
<span id="cb12-3"><a></a>    G_block(in_channels<span class="op">=</span><span class="dv">100</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">8</span>,</span>
<span id="cb12-4"><a></a>            strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>),                  <span class="co"># Output: (512, 4, 4)</span></span>
<span id="cb12-5"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">8</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">4</span>), <span class="co"># Output: (256, 8, 8)</span></span>
<span id="cb12-6"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">4</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">2</span>), <span class="co"># Output: (128, 16, 16)</span></span>
<span id="cb12-7"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">2</span>, out_channels<span class="op">=</span>n_G),   <span class="co"># Output: (64, 32, 32)</span></span>
<span id="cb12-8"><a></a>    nn.ConvTranspose2d(in_channels<span class="op">=</span>n_G, out_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-9"><a></a>                       kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb12-10"><a></a>    nn.Tanh())  <span class="co"># Output: (3, 64, 64)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Final Layer Details</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>ConvTranspose2d</strong>: Final upsampling to <span class="math inline">\(64 \times 64\)</span></li>
<li><strong>Tanh</strong>: Output activation for <span class="math inline">\([-1, 1]\)</span> range</li>
<li><strong>No BatchNorm</strong>: Standard practice for final layer</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="verification">Verification</h3>
<p>Test the generator with a sample latent vector:</p>
<div id="0080b634" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb13-2"><a></a>net_G(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>torch.Size([1, 3, 64, 64])</code></pre>
</div>
</div>
</section>
<section id="discriminator" class="slide level2">
<h2>Discriminator</h2>
<p>The discriminator is a normal convolutional network network except that it uses a leaky ReLU as its activation function. Given <span class="math inline">\(\alpha \in[0, 1]\)</span>, its definition is</p>
<p><span class="math display">\[\textrm{leaky ReLU}(x) = \begin{cases}x &amp; \text{if}\ x &gt; 0\\ \alpha x &amp;\text{otherwise}\end{cases}.\]</span></p>
<p>As it can be seen, it is normal ReLU if <span class="math inline">\(\alpha=0\)</span>, and an identity function if <span class="math inline">\(\alpha=1\)</span>. For <span class="math inline">\(\alpha \in (0, 1)\)</span>, leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the “dying ReLU” problem that a neuron might always output a negative value and therefore cannot make any progress since the gradient of ReLU is 0.</p>
<div id="cf58f0e8" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a>alphas <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">.2</span>, <span class="fl">.4</span>, <span class="fl">.6</span>, <span class="fl">.8</span>, <span class="dv">1</span>]</span>
<span id="cb15-2"><a></a>x <span class="op">=</span> d2l.arange(<span class="op">-</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)</span>
<span id="cb15-3"><a></a>Y <span class="op">=</span> [d2l.numpy(nn.LeakyReLU(alpha)(x)) <span class="cf">for</span> alpha <span class="kw">in</span> alphas]</span>
<span id="cb15-4"><a></a>d2l.plot(d2l.numpy(x), Y, <span class="st">'x'</span>, <span class="st">'y'</span>, alphas)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="template_files/figure-revealjs/cell-11-output-1.svg" class="r-stretch"></section>
<section class="slide level2">

<p>The basic block of the discriminator is a convolution layer followed by a batch normalization layer and a leaky ReLU activation. The hyperparameters of the convolution layer are similar to the transpose convolution layer in the generator block.</p>
<div id="32c5d8ba" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a><span class="kw">class</span> D_block(nn.Module):</span>
<span id="cb16-2"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_channels, in_channels<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-3"><a></a>                padding<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, <span class="op">**</span>kwargs):</span>
<span id="cb16-4"><a></a>        <span class="bu">super</span>(D_block, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb16-5"><a></a>        <span class="va">self</span>.conv2d <span class="op">=</span> nn.Conv2d(in_channels, out_channels, kernel_size,</span>
<span id="cb16-6"><a></a>                                strides, padding, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-7"><a></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb16-8"><a></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.LeakyReLU(alpha, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-9"><a></a></span>
<span id="cb16-10"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb16-11"><a></a>        <span class="cf">return</span> <span class="va">self</span>.activation(<span class="va">self</span>.batch_norm(<span class="va">self</span>.conv2d(X)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<p>A basic block with default settings will halve the width and height of the inputs, as we demonstrated in the Padding section. For example, given a input shape <span class="math inline">\(n_h = n_w = 16\)</span>, with a kernel shape <span class="math inline">\(k_h = k_w = 4\)</span>, a stride shape <span class="math inline">\(s_h = s_w = 2\)</span>, and a padding shape <span class="math inline">\(p_h = p_w = 1\)</span>, the output shape will be:</p>
<p><span class="math display">\[
\begin{aligned}
n_h^{'} \times n_w^{'} &amp;= \lfloor(n_h-k_h+2p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+2p_w+s_w)/s_w\rfloor\\
  &amp;= \lfloor(16-4+2\times 1+2)/2\rfloor \times \lfloor(16-4+2\times 1+2)/2\rfloor\\
  &amp;= 8 \times 8 .\\
\end{aligned}
\]</span></p>
<div id="9eb9c431" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb17-2"><a></a>d_blk <span class="op">=</span> D_block(<span class="dv">20</span>)</span>
<span id="cb17-3"><a></a>d_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>torch.Size([2, 20, 8, 8])</code></pre>
</div>
</div>
<div id="2420eddb" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a>n_D <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb19-2"><a></a>net_D <span class="op">=</span> nn.Sequential(</span>
<span id="cb19-3"><a></a>    D_block(n_D),  <span class="co"># Output: (64, 32, 32)</span></span>
<span id="cb19-4"><a></a>    D_block(in_channels<span class="op">=</span>n_D, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">2</span>),  <span class="co"># Output: (64 * 2, 16, 16)</span></span>
<span id="cb19-5"><a></a>    D_block(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">2</span>, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">4</span>),  <span class="co"># Output: (64 * 4, 8, 8)</span></span>
<span id="cb19-6"><a></a>    D_block(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">4</span>, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">8</span>),  <span class="co"># Output: (64 * 8, 4, 4)</span></span>
<span id="cb19-7"><a></a>    nn.Conv2d(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">8</span>, out_channels<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb19-8"><a></a>              kernel_size<span class="op">=</span><span class="dv">4</span>, bias<span class="op">=</span><span class="va">False</span>))  <span class="co"># Output: (1, 1, 1)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It uses a convolution layer with output channel <span class="math inline">\(1\)</span> as the last layer to obtain a single prediction value.</p>
<div id="b93697c8" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">64</span>))</span>
<span id="cb20-2"><a></a>net_D(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>torch.Size([1, 1, 1, 1])</code></pre>
</div>
</div>
</section>
<section id="training" class="slide level2">
<h2>Training</h2>
<p>Compared to the basic GAN, we use the same learning rate for both generator and discriminator since they are similar to each other. In addition, we change <span class="math inline">\(\beta_1\)</span> in Adam from <span class="math inline">\(0.9\)</span> to <span class="math inline">\(0.5\)</span>. It decreases the smoothness of the momentum, the exponentially weighted moving average of past gradients, to take care of the rapid changing gradients because the generator and the discriminator fight with each other. Besides, the random generated noise <code>Z</code>, is a 4-D tensor and we are using GPU to accelerate the computation.</p>
<div id="b83a2dbf" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a></a><span class="kw">def</span> train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,</span>
<span id="cb22-2"><a></a>          device<span class="op">=</span>d2l.try_gpu()):</span>
<span id="cb22-3"><a></a>    loss <span class="op">=</span> nn.BCEWithLogitsLoss(reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb22-4"><a></a>    <span class="cf">for</span> w <span class="kw">in</span> net_D.parameters():</span>
<span id="cb22-5"><a></a>        nn.init.normal_(w, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb22-6"><a></a>    <span class="cf">for</span> w <span class="kw">in</span> net_G.parameters():</span>
<span id="cb22-7"><a></a>        nn.init.normal_(w, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb22-8"><a></a>    net_D, net_G <span class="op">=</span> net_D.to(device), net_G.to(device)</span>
<span id="cb22-9"><a></a>    trainer_hp <span class="op">=</span> {<span class="st">'lr'</span>: lr, <span class="st">'betas'</span>: [<span class="fl">0.5</span>,<span class="fl">0.999</span>]}</span>
<span id="cb22-10"><a></a>    trainer_D <span class="op">=</span> torch.optim.Adam(net_D.parameters(), <span class="op">**</span>trainer_hp)</span>
<span id="cb22-11"><a></a>    trainer_G <span class="op">=</span> torch.optim.Adam(net_G.parameters(), <span class="op">**</span>trainer_hp)</span>
<span id="cb22-12"><a></a>    animator <span class="op">=</span> d2l.Animator(xlabel<span class="op">=</span><span class="st">'epoch'</span>, ylabel<span class="op">=</span><span class="st">'loss'</span>,</span>
<span id="cb22-13"><a></a>                            xlim<span class="op">=</span>[<span class="dv">1</span>, num_epochs], nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb22-14"><a></a>                            legend<span class="op">=</span>[<span class="st">'discriminator'</span>, <span class="st">'generator'</span>])</span>
<span id="cb22-15"><a></a>    animator.fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb22-16"><a></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb22-17"><a></a>        <span class="co"># Train one epoch</span></span>
<span id="cb22-18"><a></a>        timer <span class="op">=</span> d2l.Timer()</span>
<span id="cb22-19"><a></a>        metric <span class="op">=</span> d2l.Accumulator(<span class="dv">3</span>)  <span class="co"># loss_D, loss_G, num_examples</span></span>
<span id="cb22-20"><a></a>        <span class="cf">for</span> X, _ <span class="kw">in</span> data_iter:</span>
<span id="cb22-21"><a></a>            batch_size <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb22-22"><a></a>            Z <span class="op">=</span> torch.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(batch_size, latent_dim, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb22-23"><a></a>            X, Z <span class="op">=</span> X.to(device), Z.to(device)</span>
<span id="cb22-24"><a></a>            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),</span>
<span id="cb22-25"><a></a>                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),</span>
<span id="cb22-26"><a></a>                       batch_size)</span>
<span id="cb22-27"><a></a>        <span class="co"># Show generated examples</span></span>
<span id="cb22-28"><a></a>        Z <span class="op">=</span> torch.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(<span class="dv">21</span>, latent_dim, <span class="dv">1</span>, <span class="dv">1</span>), device<span class="op">=</span>device)</span>
<span id="cb22-29"><a></a>        <span class="co"># Normalize the synthetic data to N(0, 1)</span></span>
<span id="cb22-30"><a></a>        fake_x <span class="op">=</span> net_G(Z).permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb22-31"><a></a>        imgs <span class="op">=</span> torch.cat(</span>
<span id="cb22-32"><a></a>            [torch.cat([</span>
<span id="cb22-33"><a></a>                fake_x[i <span class="op">*</span> <span class="dv">7</span> <span class="op">+</span> j].cpu().detach() <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">7</span>)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb22-34"><a></a>             <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(fake_x)<span class="op">//</span><span class="dv">7</span>)], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-35"><a></a>        animator.axes[<span class="dv">1</span>].cla()</span>
<span id="cb22-36"><a></a>        animator.axes[<span class="dv">1</span>].imshow(imgs)</span>
<span id="cb22-37"><a></a>        <span class="co"># Show the losses</span></span>
<span id="cb22-38"><a></a>        loss_D, loss_G <span class="op">=</span> metric[<span class="dv">0</span>] <span class="op">/</span> metric[<span class="dv">2</span>], metric[<span class="dv">1</span>] <span class="op">/</span> metric[<span class="dv">2</span>]</span>
<span id="cb22-39"><a></a>        animator.add(epoch, (loss_D, loss_G))</span>
<span id="cb22-40"><a></a>    <span class="bu">print</span>(<span class="ss">f'loss_D </span><span class="sc">{</span>loss_D<span class="sc">:.3f}</span><span class="ss">, loss_G </span><span class="sc">{</span>loss_G<span class="sc">:.3f}</span><span class="ss">, '</span></span>
<span id="cb22-41"><a></a>          <span class="ss">f'</span><span class="sc">{</span>metric[<span class="dv">2</span>] <span class="op">/</span> timer<span class="sc">.</span>stop()<span class="sc">:.1f}</span><span class="ss"> examples/sec on </span><span class="sc">{</span><span class="bu">str</span>(device)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<p>We train the model with a small number of epochs just for demonstration. For better performance, the variable <code>num_epochs</code> can be set to a larger number.</p>
<div id="training" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a>latent_dim, lr, num_epochs <span class="op">=</span> <span class="dv">100</span>, <span class="fl">0.005</span>, <span class="dv">20</span></span>
<span id="cb23-2"><a></a><span class="co"># train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>Generative adversarial networks (GANs) composes of two deep networks, the generator and the discriminator.</li>
<li>The generator generates the image as much closer to the true image as possible to fool the discriminator, via maximizing the cross-entropy loss, <em>i.e.</em>, <span class="math inline">\(\max \log(D(\mathbf{x'}))\)</span>.</li>
<li>The discriminator tries to distinguish the generated images from the true images, via minimizing the cross-entropy loss, <em>i.e.</em>, <span class="math inline">\(\min - y \log D(\mathbf{x}) - (1-y)\log(1-D(\mathbf{x}))\)</span>.</li>
</ul>
</section>
<section id="exercises" class="slide level2">
<h2>Exercises</h2>
<ul>
<li>Does an equilibrium exist where the generator wins, <em>i.e.</em> the discriminator ends up unable to distinguish the two distributions on finite samples?</li>
</ul>
</section></section>
<section>
<section id="deep-convolutional-generative-adversarial-networks" class="title-slide slide level1 center">
<h1>Deep Convolutional Generative Adversarial Networks</h1>
<ul>
<li><strong>Introduction to GANs:</strong>
<ul>
<li>Basic idea: Transform samples from simple distributions (uniform, normal) to match dataset distributions.</li>
<li>Previous example: Matching a 2D Gaussian distribution.</li>
</ul></li>
<li><strong>Photorealistic Image Generation:</strong>
<ul>
<li>Use GANs to generate photorealistic images.</li>
<li>Based on deep convolutional GANs (DCGAN) from Radford et al.&nbsp;(2015).</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><strong>DCGAN Architecture:</strong>
<ul>
<li>Leverages convolutional architecture.</li>
<li>Successful for discriminative computer vision tasks.</li>
<li>Adapted for generative tasks to produce realistic images.</li>
</ul></li>
</ul>
<div id="code-3" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a><span class="im">import</span> d2l</span>
<span id="cb24-2"><a></a><span class="im">import</span> torch</span>
<span id="cb24-3"><a></a><span class="im">import</span> torchvision</span>
<span id="cb24-4"><a></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb24-5"><a></a><span class="im">import</span> warnings</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<h3 id="sec-pokemon">The Pokemon Dataset</h3>
<p>The dataset we will use is a collection of Pokemon sprites obtained from <a href="https://pokemondb.net/sprites">pokemondb</a>. First download, extract and load this dataset.</p>
<div id="266f57bf" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a></a>d2l.DATA_HUB[<span class="st">'pokemon'</span>] <span class="op">=</span> (d2l.DATA_URL <span class="op">+</span> <span class="st">'pokemon.zip'</span>,</span>
<span id="cb25-2"><a></a>                           <span class="st">'c065c0e2593b8b161a2d7873e42418bf6a21106c'</span>)</span>
<span id="cb25-3"><a></a></span>
<span id="cb25-4"><a></a>data_dir <span class="op">=</span> d2l.download_extract(<span class="st">'pokemon'</span>)</span>
<span id="cb25-5"><a></a><span class="bu">print</span>(data_dir)</span>
<span id="cb25-6"><a></a>pokemon <span class="op">=</span> torchvision.datasets.ImageFolder(data_dir)</span>
<span id="cb25-7"><a></a><span class="co"># pokemon = torchvision.datasets.EMNIST(root='./', split='digits', download=True)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>../data/pokemon</code></pre>
</div>
</div>
</section>
<section id="data-preprocessing-for-dcgan" class="slide level2">
<h2>Data Preprocessing for DCGAN</h2>
<h3 id="image-resizing-and-normalization">Image Resizing and Normalization</h3>
<ul>
<li><strong>Image Size</strong>: Resize to <span class="math inline">\(64 \times 64\)</span> pixels</li>
<li><strong>Value Range Transformation</strong>:
<ul>
<li><code>ToTensor()</code>: Maps pixel values to <span class="math inline">\([0, 1]\)</span></li>
<li>Generator output: Uses <code>tanh</code> activation for <span class="math inline">\([-1, 1]\)</span> range</li>
<li><strong>Solution</strong>: Normalize with mean <span class="math inline">\(0.5\)</span> and std <span class="math inline">\(0.5\)</span></li>
</ul></li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Why This Normalization?</strong></p>
</div>
<div class="callout-content">
<p>The normalization ensures compatibility between:</p>
<ul>
<li>Input data range: <span class="math inline">\([0, 1]\)</span> (from <code>ToTensor()</code>)</li>
<li>Generator output range: <span class="math inline">\([-1, 1]\)</span> (from <code>tanh</code> activation)</li>
<li>Result: Data effectively mapped to <span class="math inline">\([-1, 1]\)</span> range</li>
<li></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="data-transformation-pipeline">Data Transformation Pipeline</h3>
<div id="b4607e1a" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a></a>batch_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb27-2"><a></a>transformer <span class="op">=</span> torchvision.transforms.Compose([</span>
<span id="cb27-3"><a></a>    torchvision.transforms.Resize((<span class="dv">64</span>, <span class="dv">64</span>)),      <span class="co"># Resize images</span></span>
<span id="cb27-4"><a></a>    torchvision.transforms.ToTensor(),            <span class="co"># Scale to [0,1]</span></span>
<span id="cb27-5"><a></a>    torchvision.transforms.Normalize(<span class="fl">0.5</span>, <span class="fl">0.5</span>)    <span class="co"># Map to [-1,1]</span></span>
<span id="cb27-6"><a></a>])</span>
<span id="cb27-7"><a></a>pokemon.transform <span class="op">=</span> transformer</span>
<span id="cb27-8"><a></a>data_iter <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb27-9"><a></a>    pokemon, batch_size<span class="op">=</span>batch_size,</span>
<span id="cb27-10"><a></a>    shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<h3 id="sample-images-from-dataset">Sample Images from Dataset</h3>
<div id="46d4fa35" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb28-2"><a></a>d2l.set_figsize((<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb28-3"><a></a><span class="cf">for</span> X, y <span class="kw">in</span> data_iter:</span>
<span id="cb28-4"><a></a>    imgs <span class="op">=</span> X[:<span class="dv">20</span>,:,:,:].permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span><span class="op">+</span><span class="fl">0.5</span></span>
<span id="cb28-5"><a></a>    d2l.show_images(imgs, num_rows<span class="op">=</span><span class="dv">4</span>, num_cols<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb28-6"><a></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="template_files/figure-revealjs/cell-21-output-1.svg" class="r-stretch"><div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Key Points</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Batch Size</strong>: 256 for efficient training</li>
<li><strong>Data Augmentation</strong>: None (GANs are sensitive to augmentations)</li>
<li><strong>Shuffling</strong>: Enabled for better training dynamics</li>
<li><strong>Workers</strong>: Parallel data loading for speed</li>
</ul>
</div>
</div>
</div>
</section>
<section id="the-generator-1" class="slide level2">
<h2>The Generator</h2>
<p>The generator needs to map the noise variable <span class="math inline">\(\mathbf z\in\mathbb R^d\)</span>, a length-<span class="math inline">\(d\)</span> vector, to a RGB image with width and height to be <span class="math inline">\(64\times 64\)</span> . In :numref:<code>sec_fcn</code> we introduced the fully convolutional network that uses transposed convolution layer (refer to :numref:<code>sec_transposed_conv</code>) to enlarge input size. The basic block of the generator contains a transposed convolution layer followed by the batch normalization and ReLU activation.</p>
<div id="f7539a31" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a></a><span class="kw">class</span> G_block(nn.Module):</span>
<span id="cb29-2"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_channels, in_channels<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb29-3"><a></a>                 padding<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb29-4"><a></a>        <span class="bu">super</span>(G_block, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb29-5"><a></a>        <span class="va">self</span>.conv2d_trans <span class="op">=</span> nn.ConvTranspose2d(in_channels, out_channels,</span>
<span id="cb29-6"><a></a>                                kernel_size, strides, padding, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb29-7"><a></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb29-8"><a></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.ReLU()</span>
<span id="cb29-9"><a></a></span>
<span id="cb29-10"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb29-11"><a></a>        <span class="cf">return</span> <span class="va">self</span>.activation(<span class="va">self</span>.batch_norm(<span class="va">self</span>.conv2d_trans(X)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<p>In default, the transposed convolution layer uses a <span class="math inline">\(k_h = k_w = 4\)</span> kernel, a <span class="math inline">\(s_h = s_w = 2\)</span> strides, and a <span class="math inline">\(p_h = p_w = 1\)</span> padding. With a input shape of <span class="math inline">\(n_h^{'} \times n_w^{'} = 16 \times 16\)</span>, the generator block will double input’s width and height.</p>
<p><span class="math display">\[
\begin{aligned}
n_h^{'} \times n_w^{'} &amp;= [(n_h k_h - (n_h-1)(k_h-s_h)- 2p_h] \times [(n_w k_w - (n_w-1)(k_w-s_w)- 2p_w]\\
  &amp;= [(k_h + s_h (n_h-1)- 2p_h] \times [(k_w + s_w (n_w-1)- 2p_w]\\
  &amp;= [(4 + 2 \times (16-1)- 2 \times 1] \times [(4 + 2 \times (16-1)- 2 \times 1]\\
  &amp;= 32 \times 32 .\\
\end{aligned}
\]</span></p>
<div id="6bb728ce" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb30-2"><a></a>g_blk <span class="op">=</span> G_block(<span class="dv">20</span>)</span>
<span id="cb30-3"><a></a>g_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>torch.Size([2, 20, 32, 32])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<p>If changing the transposed convolution layer to a <span class="math inline">\(4\times 4\)</span> kernel, <span class="math inline">\(1\times 1\)</span> strides and zero padding. With a input size of <span class="math inline">\(1 \times 1\)</span>, the output will have its width and height increased by 3 respectively.</p>
<div id="593e241f" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb32-2"><a></a>g_blk <span class="op">=</span> G_block(<span class="dv">20</span>, strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb32-3"><a></a>g_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>torch.Size([2, 20, 4, 4])</code></pre>
</div>
</div>
<p>The generator consists of four basic blocks that increase input’s both width and height from 1 to 32. At the same time, it first projects the latent variable into <span class="math inline">\(64\times 8\)</span> channels, and then halve the channels each time. At last, a transposed convolution layer is used to generate the output. It further doubles the width and height to match the desired <span class="math inline">\(64\times 64\)</span> shape, and reduces the channel size to <span class="math inline">\(3\)</span>. The tanh activation function is applied to project output values into the <span class="math inline">\((-1, 1)\)</span> range.</p>
</section>
<section class="slide level2">

<div id="43c16541" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a>n_G <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb34-2"><a></a>net_G <span class="op">=</span> nn.Sequential(</span>
<span id="cb34-3"><a></a>    G_block(in_channels<span class="op">=</span><span class="dv">100</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">8</span>,</span>
<span id="cb34-4"><a></a>            strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>),                  <span class="co"># Output: (64 * 8, 4, 4)</span></span>
<span id="cb34-5"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">8</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">4</span>), <span class="co"># Output: (64 * 4, 8, 8)</span></span>
<span id="cb34-6"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">4</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">2</span>), <span class="co"># Output: (64 * 2, 16, 16)</span></span>
<span id="cb34-7"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">2</span>, out_channels<span class="op">=</span>n_G),   <span class="co"># Output: (64, 32, 32)</span></span>
<span id="cb34-8"><a></a>    nn.ConvTranspose2d(in_channels<span class="op">=</span>n_G, out_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb34-9"><a></a>                       kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb34-10"><a></a>    nn.Tanh())  <span class="co"># Output: (3, 64, 64)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Generate a 100 dimensional latent variable to verify the generator’s output shape.</p>
<div id="777e79e5" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb35-2"><a></a>net_G(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>torch.Size([1, 3, 64, 64])</code></pre>
</div>
</div>
</section>
<section id="discriminator-1" class="slide level2">
<h2>Discriminator</h2>
<p>The discriminator is a normal convolutional network network except that it uses a leaky ReLU as its activation function. Given <span class="math inline">\(\alpha \in[0, 1]\)</span>, its definition is</p>
<p><span class="math display">\[\textrm{leaky ReLU}(x) = \begin{cases}x &amp; \text{if}\ x &gt; 0\\ \alpha x &amp;\text{otherwise}\end{cases}.\]</span></p>
<p>As it can be seen, it is normal ReLU if <span class="math inline">\(\alpha=0\)</span>, and an identity function if <span class="math inline">\(\alpha=1\)</span>. For <span class="math inline">\(\alpha \in (0, 1)\)</span>, leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the “dying ReLU” problem and helps the gradients flow easier through the architecture.</p>
<div id="bfb41ad7" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a></a>alphas <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">.2</span>, <span class="fl">.4</span>, <span class="fl">.6</span>, <span class="fl">.8</span>, <span class="dv">1</span>]</span>
<span id="cb37-2"><a></a>x <span class="op">=</span> d2l.arange(<span class="op">-</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)</span>
<span id="cb37-3"><a></a>Y <span class="op">=</span> [d2l.numpy(nn.LeakyReLU(alpha)(x)) <span class="cf">for</span> alpha <span class="kw">in</span> alphas]</span>
<span id="cb37-4"><a></a>d2l.plot(d2l.numpy(x), Y, <span class="st">'x'</span>, <span class="st">'y'</span>, alphas)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="template_files/figure-revealjs/cell-27-output-1.svg" class="r-stretch"></section>
<section class="slide level2">

<p>The basic block of the discriminator is a convolution layer followed by a batch normalization layer and a leaky ReLU activation. The hyperparameters of the convolution layer are similar to the transpose convolution layer in the generator block.</p>
<div id="2a80ed01" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a></a><span class="kw">class</span> D_block(nn.Module):</span>
<span id="cb38-2"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_channels, in_channels<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb38-3"><a></a>                padding<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, <span class="op">**</span>kwargs):</span>
<span id="cb38-4"><a></a>        <span class="bu">super</span>(D_block, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb38-5"><a></a>        <span class="va">self</span>.conv2d <span class="op">=</span> nn.Conv2d(in_channels, out_channels, kernel_size,</span>
<span id="cb38-6"><a></a>                                strides, padding, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb38-7"><a></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb38-8"><a></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.LeakyReLU(alpha, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-9"><a></a></span>
<span id="cb38-10"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb38-11"><a></a>        <span class="cf">return</span> <span class="va">self</span>.activation(<span class="va">self</span>.batch_norm(<span class="va">self</span>.conv2d(X)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<p>A basic block with default settings will halve the width and height of the inputs, as we demonstrated in :numref:<code>sec_padding</code>. For example, given a input shape <span class="math inline">\(n_h = n_w = 16\)</span>, with a kernel shape <span class="math inline">\(k_h = k_w = 4\)</span>, a stride shape <span class="math inline">\(s_h = s_w = 2\)</span>, and a padding shape <span class="math inline">\(p_h = p_w = 1\)</span>, the output shape will be:</p>
<p><span class="math display">\[
\begin{aligned}
n_h^{'} \times n_w^{'} &amp;= \lfloor(n_h-k_h+2p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+2p_w+s_w)/s_w\rfloor\\
  &amp;= \lfloor(16-4+2\times 1+2)/2\rfloor \times \lfloor(16-4+2\times 1+2)/2\rfloor\\
  &amp;= 8 \times 8 .\\
\end{aligned}
\]</span></p>
<div id="ad6f14e0" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb39-2"><a></a>d_blk <span class="op">=</span> D_block(<span class="dv">20</span>)</span>
<span id="cb39-3"><a></a>d_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>torch.Size([2, 20, 8, 8])</code></pre>
</div>
</div>
<div id="21f89b80" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a></a>n_D <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb41-2"><a></a>net_D <span class="op">=</span> nn.Sequential(</span>
<span id="cb41-3"><a></a>    D_block(n_D),  <span class="co"># Output: (64, 32, 32)</span></span>
<span id="cb41-4"><a></a>    D_block(in_channels<span class="op">=</span>n_D, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">2</span>),  <span class="co"># Output: (64 * 2, 16, 16)</span></span>
<span id="cb41-5"><a></a>    D_block(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">2</span>, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">4</span>),  <span class="co"># Output: (64 * 4, 8, 8)</span></span>
<span id="cb41-6"><a></a>    D_block(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">4</span>, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">8</span>),  <span class="co"># Output: (64 * 8, 4, 4)</span></span>
<span id="cb41-7"><a></a>    nn.Conv2d(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">8</span>, out_channels<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb41-8"><a></a>              kernel_size<span class="op">=</span><span class="dv">4</span>, bias<span class="op">=</span><span class="va">False</span>))  <span class="co"># Output: (1, 1, 1)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It uses a convolution layer with output channel <span class="math inline">\(1\)</span> as the last layer to obtain a single prediction value.</p>
<div id="fd711d6d" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">64</span>))</span>
<span id="cb42-2"><a></a>net_D(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>torch.Size([1, 1, 1, 1])</code></pre>
</div>
</div>
</section>
<section id="training-1" class="slide level2">
<h2>Training</h2>
<p>Compared to the basic GAN in :numref:<code>sec_basic_gan</code>, we use the same learning rate for both generator and discriminator since they are similar to each other. In addition, we change <span class="math inline">\(\beta_1\)</span> in Adam (:numref:<code>sec_adam</code>) from <span class="math inline">\(0.9\)</span> to <span class="math inline">\(0.5\)</span>. It decreases the smoothness of the momentum, the exponentially weighted moving average of past gradients, to take care of the rapid changing gradients because the generator and the discriminator fight with each other. Besides, the random generated noise <code>Z</code>, is a 4-D tensor and we are using GPU to accelerate the computation.</p>
<div id="a607b548" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a></a><span class="kw">def</span> train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,</span>
<span id="cb44-2"><a></a>          device<span class="op">=</span>d2l.try_gpu()):</span>
<span id="cb44-3"><a></a>    loss <span class="op">=</span> nn.BCEWithLogitsLoss(reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb44-4"><a></a>    <span class="cf">for</span> w <span class="kw">in</span> net_D.parameters():</span>
<span id="cb44-5"><a></a>        nn.init.normal_(w, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb44-6"><a></a>    <span class="cf">for</span> w <span class="kw">in</span> net_G.parameters():</span>
<span id="cb44-7"><a></a>        nn.init.normal_(w, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb44-8"><a></a>    net_D, net_G <span class="op">=</span> net_D.to(device), net_G.to(device)</span>
<span id="cb44-9"><a></a>    trainer_hp <span class="op">=</span> {<span class="st">'lr'</span>: lr, <span class="st">'betas'</span>: [<span class="fl">0.5</span>,<span class="fl">0.999</span>]}</span>
<span id="cb44-10"><a></a>    trainer_D <span class="op">=</span> torch.optim.Adam(net_D.parameters(), <span class="op">**</span>trainer_hp)</span>
<span id="cb44-11"><a></a>    trainer_G <span class="op">=</span> torch.optim.Adam(net_G.parameters(), <span class="op">**</span>trainer_hp)</span>
<span id="cb44-12"><a></a>    animator <span class="op">=</span> d2l.Animator(xlabel<span class="op">=</span><span class="st">'epoch'</span>, ylabel<span class="op">=</span><span class="st">'loss'</span>,</span>
<span id="cb44-13"><a></a>                            xlim<span class="op">=</span>[<span class="dv">1</span>, num_epochs], nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb44-14"><a></a>                            legend<span class="op">=</span>[<span class="st">'discriminator'</span>, <span class="st">'generator'</span>])</span>
<span id="cb44-15"><a></a>    animator.fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb44-16"><a></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb44-17"><a></a>        <span class="co"># Train one epoch</span></span>
<span id="cb44-18"><a></a>        timer <span class="op">=</span> d2l.Timer()</span>
<span id="cb44-19"><a></a>        metric <span class="op">=</span> d2l.Accumulator(<span class="dv">3</span>)  <span class="co"># loss_D, loss_G, num_examples</span></span>
<span id="cb44-20"><a></a>        <span class="cf">for</span> X, _ <span class="kw">in</span> data_iter:</span>
<span id="cb44-21"><a></a>            batch_size <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb44-22"><a></a>            Z <span class="op">=</span> torch.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(batch_size, latent_dim, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb44-23"><a></a>            X, Z <span class="op">=</span> X.to(device), Z.to(device)</span>
<span id="cb44-24"><a></a>            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),</span>
<span id="cb44-25"><a></a>                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),</span>
<span id="cb44-26"><a></a>                       batch_size)</span>
<span id="cb44-27"><a></a>        <span class="co"># Show generated examples</span></span>
<span id="cb44-28"><a></a>        Z <span class="op">=</span> torch.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(<span class="dv">21</span>, latent_dim, <span class="dv">1</span>, <span class="dv">1</span>), device<span class="op">=</span>device)</span>
<span id="cb44-29"><a></a>        <span class="co"># Normalize the synthetic data to N(0, 1)</span></span>
<span id="cb44-30"><a></a>        fake_x <span class="op">=</span> net_G(Z).permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb44-31"><a></a>        imgs <span class="op">=</span> torch.cat(</span>
<span id="cb44-32"><a></a>            [torch.cat([</span>
<span id="cb44-33"><a></a>                fake_x[i <span class="op">*</span> <span class="dv">7</span> <span class="op">+</span> j].cpu().detach() <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">7</span>)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb44-34"><a></a>             <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(fake_x)<span class="op">//</span><span class="dv">7</span>)], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-35"><a></a>        animator.axes[<span class="dv">1</span>].cla()</span>
<span id="cb44-36"><a></a>        animator.axes[<span class="dv">1</span>].imshow(imgs)</span>
<span id="cb44-37"><a></a>        <span class="co"># Show the losses</span></span>
<span id="cb44-38"><a></a>        loss_D, loss_G <span class="op">=</span> metric[<span class="dv">0</span>] <span class="op">/</span> metric[<span class="dv">2</span>], metric[<span class="dv">1</span>] <span class="op">/</span> metric[<span class="dv">2</span>]</span>
<span id="cb44-39"><a></a>        animator.add(epoch, (loss_D, loss_G))</span>
<span id="cb44-40"><a></a>    <span class="bu">print</span>(<span class="ss">f'loss_D </span><span class="sc">{</span>loss_D<span class="sc">:.3f}</span><span class="ss">, loss_G </span><span class="sc">{</span>loss_G<span class="sc">:.3f}</span><span class="ss">, '</span></span>
<span id="cb44-41"><a></a>          <span class="ss">f'</span><span class="sc">{</span>metric[<span class="dv">2</span>] <span class="op">/</span> timer<span class="sc">.</span>stop()<span class="sc">:.1f}</span><span class="ss"> examples/sec on </span><span class="sc">{</span><span class="bu">str</span>(device)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<p>We train the model with a small number of epochs just for demonstration. For better performance, the variable <code>num_epochs</code> can be set to a larger number.</p>
<div id="training-dcgan" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a></a>latent_dim, lr, num_epochs <span class="op">=</span> <span class="dv">100</span>, <span class="fl">0.005</span>, <span class="dv">20</span></span>
<span id="cb45-2"><a></a></span>
<span id="cb45-3"><a></a><span class="co">#train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="summary-1" class="slide level2">
<h2>Summary</h2>
<ul>
<li>DCGAN architecture has four convolutional layers for the Discriminator and four “fractionally-strided” convolutional layers for the Generator.</li>
<li>The Discriminator is a 4-layer strided convolutions with batch normalization (except its input layer) and leaky ReLU activations.</li>
<li>Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the “dying ReLU” problem and helps the gradients flow easier through the architecture.</li>
</ul>
</section>
<section id="exercises-1" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>What will happen if we use standard ReLU activation rather than leaky ReLU?</li>
<li>Apply DCGAN on Fashion-MNIST and see which category works well and which does not.</li>
</ol>
</section></section>
<section>
<section id="application-examples" class="title-slide slide level1 center">
<h1>Application examples</h1>

</section>
<section id="example-1-leveraging-generative-adversarial-networks-to-create-realistic-scanning-transmission-electron-microscopy-images" class="slide level2">
<h2>Example 1: Leveraging Generative Adversarial Networks to Create Realistic Scanning Transmission Electron Microscopy Images</h2>
<p><strong>Authors:</strong> Abid Khan, Chia-Hao Lee, Pinshane Y. Huang, Bryan K. Clark<br>
<strong>Published in:</strong> npj Computational Materials (2023)<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1038/s41524-023-01042-3">10.1038/s41524-023-01042-3</a></p>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang1.png" style="width:80.0%"></p>
<figcaption>Generative Adversarial Networks</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="introduction-1" class="slide level2">
<h2>Introduction</h2>
<ul>
<li>Machine learning (ML) in electron microscopy:
<ul>
<li>Atom localization</li>
<li>Defect identification</li>
<li>Image denoising</li>
</ul></li>
<li>Challenge: High-quality training data with ground truth for supervised learning.</li>
<li>Solution: Use CycleGANs to bridge the gap between simulated and experimental STEM images. <img data-src="img2/huang4.png" style="background: #333333;width:80.0%" alt="Generative Adversarial Networks"></li>
</ul>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="generating-realistic-images-with-cyclegans" class="slide level2">
<h2>Generating Realistic Images with CycleGANs</h2>
<ul>
<li>CycleGANs are used for image-to-image translation.</li>
<li>Training involves both experimental and simulated images.</li>
<li>CycleGANs produce images that match experimental data in terms of noise, distortions, and other artifacts.</li>
<li>Results show significant improvement in realism compared to unoptimized simulated images.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang2.png" style="width:80.0%"></p>
<figcaption>CycleGAN</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="variations-within-experimental-stem-data" class="slide level2">
<h2>variations within experimental STEM data</h2>
<ul>
<li>Experimental STEM images acquired on Day A (a–c) and B (e–g) showing variations within and between days due to sample contamination and microscope instabilities.</li>
<li>d, h Power spectra obtained with FFT of a and e demonstrate the variation of image resolution, from 96 pm to 110 pm, determined by the highest transferred spatial frequency marked with white circles.</li>
<li>The power spectra are displayed in log scale for visibility. Scale bars are equal to 1 nm.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang5.png" style="width:80.0%"></p>
<figcaption>Example of variations within experimental STEM data sets taken on two different days, Day A and Day B.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="cyclegan-architecture-and-optimization" class="slide level2">
<h2>CycleGAN Architecture and Optimization</h2>
<ul>
<li>CycleGAN translates images between two domains:
<ol type="1">
<li>the experiment domain X and</li>
<li>the simulation domain Y, and has two main components: generators and discriminators.</li>
</ol></li>
<li>generator G converts input experimental images x to simulation-like images G(x), and the generator F converts input simulated images y to experiment-like images F(y).</li>
<li>generated images (F(y) and G(x)) are then fed into four discriminators (Dx,img, Dx,FFT, Dy,img, Dy,FFT) along with the raw images (x and y) to evaluate the quality of generated images.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang2.png" style="width:80.0%"></p>
<figcaption>Schematic of the major components in a CycleGAN.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section class="slide level2">

<ul>
<li>Both input images and their FFTs are examined by the discriminators to calculate the adversarial losses (Ladv), which are used to optimize both generators F and G respectively.</li>
<li>By passing the raw images (x and y) with combinations of both generators, identity images (F(x) and G(y)) and cycled images (F(G(x)) and G(F(y))) are also generated.</li>
<li>corresponding identity loss Lid and cycle consistency loss Lcyc are added to ensure the identity and cycle consistency mapping of the generators.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang2.png" style="width:80.0%"></p>
<figcaption>Schematic of the major components in a CycleGAN.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="evaluation-metrics" class="slide level2">
<h2>Evaluation Metrics</h2>
<ul>
<li>Quantitative evaluation using:
<ul>
<li>Fréchet Inception Distance (FID)</li>
<li>Kullback–Leibler (KL) divergence</li>
</ul></li>
<li>Results:
<ul>
<li>CycleGAN-processed images have the lowest FID scores and KL divergence, indicating high similarity to experimental images.</li>
</ul></li>
</ul>
</section>
<section id="fréchet-inception-distance-fid" class="slide level2">
<h2>Fréchet Inception Distance (FID)</h2>
<ul>
<li><strong>Purpose</strong>: Measure similarity between generated images and real images.</li>
<li><strong>Calculation</strong>:
<ul>
<li>Preprocess the images. Ensure the two images are compatible using basic processing.</li>
<li><strong>Extract feature representations</strong>. Pass the real and generated images through the Inception-v3 model. This transforms the raw pixels into numerical vectors to represent aspects of the images, such as lines, edges and higher-order shapes.</li>
<li><strong>Calculate statistics</strong>. Statistical analysis is performed to determine the mean and covariance matrix of the features in each image.</li>
<li>Treats images as multivariate Gaussian distributions</li>
<li><span class="math inline">\(d_{F}(\mathcal N(\mu, \Sigma), \mathcal N(\mu', \Sigma'))^2 = \lVert \mu - \mu' \rVert^2_2 + \operatorname{tr}\left(\Sigma + \Sigma' -2\left(\Sigma \Sigma'  \right)^\frac{1}{2} \right)\)</span></li>
</ul></li>
<li><strong>Interpretation</strong>:
<ul>
<li>Lower FID indicates higher similarity and better quality of generated images.</li>
</ul></li>
</ul>
</section>
<section id="quantitative-measurements-of-data-set-quality-using-fid-and-kl" class="slide level2">
<h2>Quantitative measurements of data set quality using FID and KL</h2>
<ul>
<li>Images are generated from (a) experiments, (b) simulation without noise, (c) simulation with manually optimized noise, and (d) CycleGAN.</li>
<li>FID score measures the dissimilarity between image data sets, so a smaller FID score implies higher similarity between image data sets.</li>
<li>e–h Histograms of normalized pixel intensity are calculated for each image data set. Each histogram is normalized so that the probability distribution sums to unity.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang3.png" style="width:80.0%"></p>
<figcaption>Quantitative measurements of data set quality using FID and KL</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="quantitative-measurements-of-data-set-quality-using-fid-and-kl-2" class="slide level2">
<h2>Quantitative measurements of data set quality using FID and KL 2</h2>
<ul>
<li>KL divergence DKL(P∣∣Q) of each data set with the experimental histogram are labeled as DKL at the top right corner of each histogram, with the lowest non-zero value marked in red.</li>
<li>both the FID score and KL divergence of intensity histograms are calculated for the entire data set with respect to the experimental data set, where each data set contains roughly 1700 image patches with 256 × 256 pixels.</li>
<li>CycleGAN generated image set exhibits the best FID score and lowest KL divergence, indicating it is the best match for experimental data.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang3.png" style="width:80.0%"></p>
<figcaption>Quantitative measurements of data set quality using FID and KL</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="defect-identification-with-cyclegan-and-fcn" class="slide level2">
<h2>Defect Identification with CycleGAN and FCN</h2>
<ul>
<li>Workflow:
<ol type="1">
<li>Acquire experimental STEM images.</li>
<li>Simulate STEM images with known defects.</li>
<li>Train CycleGAN with both experimental and simulated images.</li>
</ol></li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang4.png" style="width:80.0%"></p>
<figcaption>Schematic of the major components in a CycleGAN.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="defect-identification-with-cyclegan-and-fcn-1" class="slide level2">
<h2>Defect Identification with CycleGAN and FCN</h2>
<ul>
<li>Workflow:
<ol start="4" type="1">
<li>Generate realistic images using CycleGAN.</li>
<li>Train Fully Convolutional Network (FCN) on CycleGAN-processed images.</li>
<li>Use FCN to identify defects in experimental images.</li>
</ol></li>
<li>Results: High precision and recall in defect identification with minimal human intervention.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background: #333333">
<figure>
<p><img data-src="img2/huang6.png" style="width:80.0%"></p>
<figcaption>The CycleGAN-processed images preserve the defect types and positions from the input simulated images.</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="khan2023leveraging">Khan, Abid et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="conclusion" class="slide level2">
<h2>Conclusion</h2>
<ul>
<li>CycleGANs effectively bridge the gap between simulated and experimental STEM images.</li>
<li>FCNs trained on CycleGAN-processed images achieve high accuracy in defect identification.</li>
<li>Potential for real-time, automated microscopy data processing.</li>
</ul>
</section>
<section id="example-2-generation-of-highly-realistic-microstructural-images-of-alloys-from-limited-data-with-a-style-based-generative-adversarial-network" class="slide level2">
<h2>Example 2: Generation of Highly Realistic Microstructural Images of Alloys from Limited Data with a Style-Based Generative Adversarial Network</h2>
<p><strong>Authors:</strong> Guillaume Lambard, Kazuhiko Yamazaki, Masahiko Demura<br>
<strong>Published in:</strong> Scientific Reports (2023)<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1038/s41598-023-27574-8">10.1038/s41598-023-27574-8</a></p>
</section>
<section id="introduction-2" class="slide level2">
<h2>Introduction</h2>
<ul>
<li><strong>Microstructural Characterization</strong>: Essential for understanding material properties.</li>
<li><strong>Challenges</strong>: Limited availability of high-quality SEM images.</li>
<li><strong>Solution</strong>: Use StyleGAN2 with ADA to generate synthetic SEM images from a small dataset.</li>
</ul>
</section>
<section id="stylegan2-with-ada" class="slide level2">
<h2>StyleGAN2 with ADA</h2>
<ul>
<li><strong>StyleGAN2 Architecture</strong>:
<ul>
<li>Adaptive Discriminator Augmentation (ADA)</li>
<li>Effective in low data regimes</li>
</ul></li>
<li><strong>Training</strong>:
<ul>
<li>Dataset: 3000 SEM images of ferrite-martensite dual-phase steel</li>
<li>Image size: 512x512 pixels</li>
<li>Augmentations: Pixel blitting, geometrical transformations</li>
</ul></li>
</ul>
</section>
<section id="stylegan2-with-ada-1" class="slide level2">
<h2>StyleGAN2 with ADA</h2>
<ul>
<li><strong>Problem</strong>: GANs require large datasets to avoid discriminator overfitting.</li>
<li><strong>Solution</strong>: Adaptive Discriminator Augmentation (ADA) to stabilize training with limited data.</li>
<li><strong>Key Insight</strong>: ADA prevents overfitting without changing loss functions or network architectures.</li>
</ul>
</section>
<section id="adaptive-discriminator-augmentation-ada" class="slide level2">
<h2>Adaptive Discriminator Augmentation (ADA)</h2>
<ul>
<li><strong>Objective</strong>: Apply augmentations to prevent discriminator overfitting.</li>
<li><strong>Mechanism</strong>:
<ul>
<li>Stochastic augmentation of discriminator inputs.</li>
<li>Augmentations include geometric and color transformations.</li>
<li>Adaptive control based on overfitting heuristics.</li>
</ul></li>
<li><strong>Benefits</strong>:
<ul>
<li>Effective even with few thousand training images.</li>
<li>Maintains image quality and diversity.</li>
</ul></li>
</ul>
</section>
<section id="augmentations-and-training" class="slide level2">
<h2>Augmentations and Training</h2>
<ul>
<li><strong>Augmentation Strategies</strong>:
<ul>
<li>Pixel blitting and geometrical transformations improved FID by ~77%</li>
<li>Other augmentations like color transformations and additive noise were less effective.</li>
</ul></li>
<li><strong>Target Heuristic (rt)</strong>:
<ul>
<li>Optimal value: 0.5</li>
<li>Balances between reducing overfitting and maintaining diversity</li>
</ul></li>
</ul>
</section>
<section id="results-and-evaluation" class="slide level2">
<h2>Results and Evaluation</h2>
<ul>
<li><strong>Evaluation Metrics</strong>:
<ul>
<li>Fréchet Inception Distance (FID)</li>
<li>Recall Metric</li>
</ul></li>
</ul>
</section>
<section id="results-and-evaluation2" class="slide level2">
<h2>Results and Evaluation2</h2>
<ul>
<li><strong>Results</strong>:
<ul>
<li>Best FID: 6.59 with 3000 images</li>
<li>High-quality and diverse SEM images</li>
<li>Successful interpolation between microstructures</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background:rgb(0, 0, 0)">
<figure>
<p><img data-src="img2/alloy5.png" style="width:50.0%"></p>
<figcaption>Samples of non-curated SEM images generated with the StyleGAN2 with ADA</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="lambard2023generation">Lambard, Guillaume et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="interpolation-and-diversity" class="slide level2">
<h2>Interpolation and Diversity</h2>
<ul>
<li><strong>Latent Space Interpolation</strong>:
<ul>
<li>Smooth transitions between different microstructures</li>
<li>Demonstrates the potential for exploring new microstructural features</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center" style="background:rgb(0, 0, 0)">
<figure>
<p><img data-src="img2/alloy8.png" style="width:30.0%"></p>
<figcaption>Selection of 4 non-curated generated SEM images obtained thanks to a spherical linear interpolation</figcaption>
</figure>
</div>

<aside><div>
<p><span class="citation" data-cites="lambard2023generation">Lambard, Guillaume et al. <a href="#/references" role="doc-biblioref" onclick="">(2023)</a></span></p>
</div></aside></section>
<section id="interpolation-and-diversity-2" class="slide level2">
<h2>Interpolation and Diversity 2</h2>
<ul>
<li><strong>Generated Images</strong>:
<ul>
<li>High resemblance to real SEM images</li>
<li>Captured both coarse and fine microstructural details</li>
</ul></li>
</ul>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-kalinin2021exploring" class="csl-entry" role="listitem">
Exploring order parameters and dynamic processes in disordered systems via variational autoencoders, Science Advances, Sergei V Kalinin, Ondrej Dyck, Stephen Jesse, &amp; Maxim Ziatdinov.
</div>
<div id="ref-khan2023leveraging" class="csl-entry" role="listitem">
Leveraging generative adversarial networks to create realistic scanning transmission electron microscopy images, npj Computational Materials, Abid Khan, Chia-Hao Lee, Pinshane Y Huang, &amp; Bryan K Clark.
</div>
<div id="ref-lambard2023generation" class="csl-entry" role="listitem">
Generation of highly realistic microstructural images of alloys from limited data with a style-based generative adversarial network, Scientific Reports, Guillaume Lambard, Kazuhiko Yamazaki, &amp; Masahiko Demura.
</div>
</div>
<script>
document.getElementById("marimo-frame").onload = function() {
    try {
        let iframeDoc = document.getElementById("marimo-frame").contentWindow.document;
        let marimoBadge = iframeDoc.querySelector("div.fixed.bottom-0.right-0.z-50");
        if (marimoBadge) {
            marimoBadge.style.display = "none";
            console.log("Marimo badge hidden successfully.");
        } else {
            console.log("Badge not found.");
        }
    } catch (error) {
        console.warn("Unable to modify iframe content due to CORS restrictions.");
    }
};
</script>
</section></section></div>

<div class="quarto-auto-generated-content" style="display: none;">
<p><img src="eclipse_logo_small.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy</p>
</div>
</div>

    </div>
  

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"right","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true,"width":"wide"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: false,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/ECLIPSE-Lab\.github\.io\/public_presentations\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>