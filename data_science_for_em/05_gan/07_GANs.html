<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-927edb3cde42616945691bbf0360b549.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.30">

  <title>ECLIPSE Presentations – gans</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #97947a;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #97947a;  padding-left: 4px; }
    div.sourceCode
      { color: #f8f8f2; background-color: #2b2b2b; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #f8f8f2; } /* Normal */
    code span.al { color: #dcc6e0; } /* Alert */
    code span.an { color: #d4d0ab; } /* Annotation */
    code span.at { color: #ffd700; } /* Attribute */
    code span.bn { color: #dcc6e0; } /* BaseN */
    code span.bu { color: #f5ab35; } /* BuiltIn */
    code span.cf { color: #ffa07a; } /* ControlFlow */
    code span.ch { color: #abe338; } /* Char */
    code span.cn { color: #ffa07a; } /* Constant */
    code span.co { color: #d4d0ab; } /* Comment */
    code span.cv { color: #d4d0ab; font-style: italic; } /* CommentVar */
    code span.do { color: #d4d0ab; font-style: italic; } /* Documentation */
    code span.dt { color: #dcc6e0; } /* DataType */
    code span.dv { color: #dcc6e0; } /* DecVal */
    code span.er { color: #dcc6e0; } /* Error */
    code span.ex { color: #ffd700; } /* Extension */
    code span.fl { color: #f5ab35; } /* Float */
    code span.fu { color: #ffd700; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #d4d0ab; } /* Information */
    code span.kw { color: #ffa07a; } /* Keyword */
    code span.op { color: #00e0e0; } /* Operator */
    code span.ot { color: #ffa07a; } /* Other */
    code span.pp { color: #dcc6e0; } /* Preprocessor */
    code span.sc { color: #00e0e0; } /* SpecialChar */
    code span.ss { color: #abe338; } /* SpecialString */
    code span.st { color: #abe338; } /* String */
    code span.va { color: #f5ab35; } /* Variable */
    code span.vs { color: #abe338; } /* VerbatimString */
    code span.wa { color: #d4d0ab; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-2f779378fda3c029279ce056aae7deee.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">


<section id="data-science-in-electron-microscopy" class="slide level2">
<h2>Data Science in Electron Microscopy</h2>
<br>
<hr>
<h3>
Philipp Pelz
</h3>
<h3>
2024
</h3>
<p><br></p>
<h3>
&nbsp; <a href="https://github.com/ECLIPSE-Lab/WS24_DataScienceForEM">https://github.com/ECLIPSE-Lab/WS24_DataScienceForEM</a>
</h3>
</section>
<section id="vae-example-exploring-order-parameters-and-dynamic-processes-in-disordered-systems-via-variational-autoencoders" class="slide level2">
<h2>VAE example: Exploring Order Parameters and Dynamic Processes in Disordered Systems via Variational Autoencoders</h2>
<p><strong>Authors:</strong> Sergei V. Kalinin, Ondrej Dyck, Stephen Jesse, Maxim Ziatdinov<br>
<strong>Published in:</strong> Science Advances (2021)<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1126/sciadv.abd5084">10.1126/sciadv.abd5084</a></p>

<img data-src="../img2/abd5084-f2.jpeg" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Single image from dynamic STEM dataset corresponding to 10th frame</p></section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<ul>
<li><strong>Objective</strong>: Analyze dynamic processes and order parameters in disordered systems.</li>
<li><strong>Approach</strong>: Use rotationally invariant variational autoencoders (rVAEs).</li>
<li><strong>Application</strong>: Studied e-beam induced processes in silicon-doped graphene.</li>
</ul>

<img data-src="../img2/abd5084-f3.jpeg" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Evolution of graphene under e-beam irradiation.</p></section>
<section id="rotationally-invariant-vaes" class="slide level2">
<h2>Rotationally Invariant VAEs</h2>
<ul>
<li><strong>Purpose</strong>: Handle rotational invariance in noncrystalline solids.</li>
<li><strong>Method</strong>:
<ul>
<li>Incorporate rotational and translational invariance.</li>
<li>Apply rVAEs to semantically segmented, atomically resolved data.</li>
</ul></li>
<li><strong>Benefit</strong>: Captures maximum original information with reduced representation.</li>
</ul>

<img data-src="../img2/spatialGAN.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Diagram of the spatial-VAE framework</p></section>
<section id="rotationally-invariant-vaes-forward" class="slide level2">
<h2>Rotationally Invariant VAEs Forward</h2>
<div class="sourceCode" id="cb1" data-n="30"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="kw">class</span> SpatialGenerator(nn.Module):</span>
<span id="cb1-2"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, z):</span>
<span id="cb1-3"><a></a>        <span class="co"># x is (batch, num_coords, 2)</span></span>
<span id="cb1-4"><a></a>        <span class="co"># z is (batch, latent_dim)</span></span>
<span id="cb1-5"><a></a></span>
<span id="cb1-6"><a></a>        <span class="cf">if</span> <span class="bu">len</span>(x.size()) <span class="op">&lt;</span> <span class="dv">3</span>:</span>
<span id="cb1-7"><a></a>            x <span class="op">=</span> x.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb1-8"><a></a>        b <span class="op">=</span> x.size(<span class="dv">0</span>)</span>
<span id="cb1-9"><a></a>        n <span class="op">=</span> x.size(<span class="dv">1</span>)</span>
<span id="cb1-10"><a></a>        x <span class="op">=</span> x.view(b<span class="op">*</span>n, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-11"><a></a></span>
<span id="cb1-12"><a></a>        h_x <span class="op">=</span> <span class="va">self</span>.coord_linear(x)</span>
<span id="cb1-13"><a></a>        h_x <span class="op">=</span> h_x.view(b, n, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-14"><a></a></span>
<span id="cb1-15"><a></a>        h_z <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-16"><a></a></span>
<span id="cb1-17"><a></a>        <span class="cf">if</span> <span class="bu">len</span>(z.size()) <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb1-18"><a></a>            z <span class="op">=</span> z.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb1-19"><a></a>        h_z <span class="op">=</span> <span class="va">self</span>.latent_linear(z)</span>
<span id="cb1-20"><a></a>        h_z <span class="op">=</span> h_z.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb1-21"><a></a></span>
<span id="cb1-22"><a></a>        h_bi <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-23"><a></a>        h <span class="op">=</span> h_x <span class="op">+</span> h_z <span class="op">+</span> h_bi <span class="co"># (batch, num_coords, hidden_dim)</span></span>
<span id="cb1-24"><a></a>        h <span class="op">=</span> h.view(b<span class="op">*</span>n, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-25"><a></a></span>
<span id="cb1-26"><a></a>        y <span class="op">=</span> <span class="va">self</span>.layers(h) <span class="co"># (batch*num_coords, nout)</span></span>
<span id="cb1-27"><a></a>        y <span class="op">=</span> y.view(b, n, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-28"><a></a></span>
<span id="cb1-29"><a></a>        <span class="cf">if</span> <span class="va">self</span>.softplus: <span class="co"># only apply softplus to first output</span></span>
<span id="cb1-30"><a></a>            y <span class="op">=</span> torch.cat([F.softplus(y[:,:,:<span class="dv">1</span>]), y[:,:,<span class="dv">1</span>:]], <span class="dv">2</span>)</span>
<span id="cb1-31"><a></a></span>
<span id="cb1-32"><a></a>        <span class="cf">return</span> y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="experimental-setup" class="slide level2">
<h2>Experimental Setup</h2>
<ul>
<li><strong>Sample</strong>: Silicon-doped graphene.</li>
<li><strong>Imaging</strong>: Scanning transmission electron microscopy (STEM).</li>
<li><strong>Procedure</strong>:
<ul>
<li>Use DCNN for initial pixel probability maps.</li>
<li>Extract atomic positions for VAE analysis.</li>
</ul></li>
<li><strong>Data</strong>: Multiple snapshots during dynamic processes.</li>
</ul>

<img data-src="../img2/abd5084-f1.jpeg" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Schematic of the overall approach.</p></section>
<section id="data-analysis-with-rvae" class="slide level2">
<h2>Data Analysis with rVAE</h2>
<ul>
<li><strong>Workflow</strong>:
<ol type="1">
<li>DCNN categorizes pixels into atomic types.</li>
<li>Generate subimages centered on atomic positions.</li>
<li>rVAE seeks the most effective reduced representation.</li>
</ol></li>
<li><strong>Output</strong>: Identifies key structural elements and their dynamics.</li>
</ul>

<img data-src="../img2/abd5084-f1.jpeg" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Workflow</p></section>
<section id="results" class="slide level2">
<h2>Results</h2>
<ul>
<li><strong>Findings</strong>:
<ul>
<li>Effective exploration of chemical evolution in the system.</li>
<li>rVAE captured rotationally invariant features.</li>
</ul></li>
</ul>
</section>
<section id="comparison-of-methods-for-construction-of-elementary-descriptors." class="slide level2">
<h2>Comparison of methods for construction of elementary descriptors.</h2>
<ul>
<li>(A to C) small and (D to F) large windows.</li>
<li>(A and D) GMM classes of the data that are decomposed into many independent components that are statistical in nature and often do not allow for direct physical interpretation. In (D), this approach performs poorly at capturing rotation and spreads this information across several components. (B and E) Representation in 2D latent space of convolutional AE. Red and blue regions in (B) indicate clear separation of graphene sublattices with remainder of the descriptors encoding lateral shifts, defects, and rotations in a convoluted fashion.</li>
</ul>

<img data-src="../img2/abd5084-f5.jpeg" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Comparison of methods for construction of elementary descriptors.</p></section>
<section id="comparison-of-methods-for-construction-of-elementary-descriptors.-1" class="slide level2">
<h2>Comparison of methods for construction of elementary descriptors.</h2>
<ul>
<li>In (E), the larger window introduces variability that is more difficult to interpret. (C and F) Representation in the 2D latent space of rotationally invariant VAE. Because rotational variation is removed from elementary descriptors, remaining variations within data can be described much more efficiently.</li>
<li>In (C), there are noticeable changes in only one dimension, which can be ascribed to degree of local crystallinity. In (F), the larger window size also captures variations related to proximity of edges. In (B), (C), (E), and (F), the images were generated by applying a corresponding decoder to the uniform grid of discrete point in the latent space.</li>
</ul>

<img data-src="../img2/abd5084-f5.jpeg" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Comparison of methods for construction of elementary descriptors.</p></section>
<section id="results-1" class="slide level2">
<h2>Results</h2>
<ul>
<li><strong>Evaluation</strong>:
<ul>
<li>Identified structural changes due to e-beam.</li>
<li>Analyzed formation of 5-7 member defect chains.</li>
<li>Detected migration of Si atoms to graphene edges.</li>
</ul></li>
</ul>
</section>
<section id="comparison-with-other-methods" class="slide level2">
<h2>Comparison with Other Methods</h2>
<ul>
<li><strong>GMM Analysis</strong>:
<ul>
<li>Generates independent components.</li>
<li>Effective for imaging but less interpretable structurally.</li>
</ul></li>
<li><strong>Classical AE</strong>:
<ul>
<li>Reduced data to continuous latent variables.</li>
<li>Convoluted rotation and structural variations.</li>
</ul></li>
<li><strong>rVAE</strong>:
<ul>
<li>Separated rotation and structural changes.</li>
<li>Provided clear physical interpretation.</li>
</ul></li>
</ul>
</section>
<section id="comparison-with-other-methods-2" class="slide level2">
<h2>Comparison with Other Methods 2</h2>

<img data-src="../img2/VAEvsrVAE.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Different embeddings</p></section>
<section id="conclusion" class="slide level2">
<h2>Conclusion</h2>
<ul>
<li><strong>Significance</strong>:
<ul>
<li>rVAEs provide a robust framework for analyzing disordered systems.</li>
<li>Effective for bottom-up description of dynamic processes.</li>
</ul></li>
</ul>
</section>
<section>
<section id="generative-adversarial-networks" class="title-slide slide level1">
<h1>Generative Adversarial Networks</h1>
<p>:label:<code>sec_basic_gan</code></p>
<ul>
<li><strong>Discriminative Learning:</strong>
<ul>
<li>Predicts labels from data examples.</li>
<li>Examples: classifiers and regressors.</li>
<li>Deep neural networks have revolutionized discriminative learning, achieving human-level accuracy on high-res images.</li>
</ul></li>
<li><strong>Generative Modeling:</strong>
<ul>
<li>Learns a model to capture data characteristics without labels.</li>
<li>Generates synthetic data resembling the training dataset.</li>
<li>Example: Generating photorealistic images from a dataset of faces.</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><strong>Deep Neural Networks in Generative Modeling:</strong>
<ul>
<li>Recent advances have enabled discriminative models to assist generative tasks.</li>
<li>Example: Recurrent neural network language models.</li>
</ul></li>
<li><strong>Generative Adversarial Networks (GANs):</strong>
<ul>
<li>Introduced in 2014 by Goodfellow et al.</li>
<li>Leverages discriminative models to create generative models.</li>
<li>Concept: A good data generator makes fake data indistinguishable from real data (two-sample test).</li>
<li>GANs use the two-sample test constructively to train generative models.</li>
<li>Aim: Improve the generator until it fools a state-of-the-art classifier.</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="../img/gan.svg" style="background: grey;width:50.0%" alt="Generative Adversarial Networks"> <code>fig_gan</code></p>
</div><div class="column" style="width:60%;">
<p>The GAN architecture is illustrated in :numref:<code>fig_gan</code>.</p>
<ul>
<li><strong>GAN Architecture:</strong>
<ul>
<li><strong>Generator Network:</strong>
<ul>
<li>Generates data resembling real data.</li>
<li>For images: generates images.</li>
<li>For speech: generates audio sequences.</li>
</ul></li>
<li><strong>Discriminator Network:</strong>
<ul>
<li>Distinguishes fake data from real data.</li>
<li>Competes with the generator.</li>
<li>Adaptively improves to distinguish better as the generator improves.</li>
</ul></li>
</ul></li>
</ul>
</div></div>
</section>
<section class="slide level2">

<ul>
<li><strong>Discriminator:</strong>
<ul>
<li>Binary classifier: distinguishes real (<span class="math inline">\(x\)</span>) vs.&nbsp;fake data.</li>
<li>Outputs scalar prediction <span class="math inline">\(o \in \mathbb{R}\)</span> for input <span class="math inline">\(\mathbf{x}\)</span>.</li>
<li>Applies sigmoid function: <span class="math inline">\(D(\mathbf{x}) = \frac{1}{1 + e^{-o}}\)</span>.</li>
<li>True data label <span class="math inline">\(y = 1\)</span>, fake data label <span class="math inline">\(y = 0\)</span>.</li>
<li>Minimize cross-entropy loss: <span class="math display">\[
\min_D \{ - y \log D(\mathbf{x}) - (1-y) \log(1-D(\mathbf{x})) \}
\]</span></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><strong>Generator:</strong>
<ul>
<li>Draws parameter <span class="math inline">\(\mathbf{z} \in \mathbb{R}^d\)</span> from randomness, e.g., <span class="math inline">\(\mathbf{z} \sim \mathcal{N}(0, 1)\)</span> (latent variable).</li>
<li>Generates data: <span class="math inline">\(\mathbf{x}' = G(\mathbf{z})\)</span>.</li>
<li>Aims to fool the discriminator: <span class="math inline">\(D(G(\mathbf{z})) \approx 1\)</span>.</li>
<li>Update parameters to maximize cross-entropy loss for <span class="math inline">\(y = 0\)</span>: <span class="math display">\[
\max_G \{ - \log(1-D(G(\mathbf{z}))) \}
\]</span></li>
<li>Commonly minimize the loss: <span class="math display">\[
\min_G \{ - \log(D(G(\mathbf{z}))) \}
\]</span>
<ul>
<li>This is feeding <span class="math inline">\(\mathbf{x}' = G(\mathbf{z})\)</span> into the discriminator but giving label <span class="math inline">\(y = 1\)</span>.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><strong>Minimax Game:</strong>
<ul>
<li><span class="math inline">\(D\)</span> and <span class="math inline">\(G\)</span> play a “minimax” game with the objective function: <span class="math display">\[
\min_D \max_G \{ -E_{x \sim \text{Data}} \log D(\mathbf{x}) - E_{z \sim \text{Noise}} \log(1 - D(G(\mathbf{z}))) \}
\]</span></li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Many GAN applications are in the context of images.</li>
<li>Demonstration: fitting a simpler distribution.</li>
<li>Example: Using GANs to estimate parameters for a Gaussian.</li>
</ul></li>
</ul>
<p>Let’s get started.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="im">from</span> d2l <span class="im">import</span> torch <span class="im">as</span> d2l</span>
<span id="cb2-2"><a></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="generate-some-real-data" class="slide level2">
<h2>Generate Some “Real” Data</h2>
<p>Since this is going to be the world’s lamest example, we simply generate data drawn from a Gaussian.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>X <span class="op">=</span> d2l.normal(<span class="fl">0.0</span>, <span class="dv">1</span>, (<span class="dv">1000</span>, <span class="dv">2</span>))</span>
<span id="cb3-2"><a></a>A <span class="op">=</span> d2l.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.5</span>]])</span>
<span id="cb3-3"><a></a>b <span class="op">=</span> d2l.tensor([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb3-4"><a></a>data <span class="op">=</span> d2l.matmul(X, A) <span class="op">+</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s see what we got. This should be a Gaussian shifted in some rather arbitrary way with mean <span class="math inline">\(b\)</span> and covariance matrix <span class="math inline">\(A^TA\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a></span>
<span id="cb4-2"><a></a>d2l.set_figsize()</span>
<span id="cb4-3"><a></a>d2l.plt.scatter(d2l.numpy(data[:<span class="dv">100</span>, <span class="dv">0</span>]), d2l.numpy(data[:<span class="dv">100</span>, <span class="dv">1</span>]))<span class="op">;</span></span>
<span id="cb4-4"><a></a><span class="bu">print</span>(<span class="ss">f'The covariance matrix is</span><span class="ch">\n</span><span class="sc">{</span>d2l<span class="sc">.</span>matmul(A.T, A)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a>batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb5-2"><a></a>data_iter <span class="op">=</span> d2l.load_array((data,), batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="generator" class="slide level2">
<h2>Generator</h2>
<p>Our generator network will be the simplest network possible - a single layer linear model. This is since we will be driving that linear network with a Gaussian data generator. Hence, it literally only needs to learn the parameters to fake things perfectly.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>net_G <span class="op">=</span> nn.Sequential(nn.Linear(<span class="dv">2</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<h3 id="discriminator">Discriminator</h3>
<p>For the discriminator we will be a bit more discriminating: we will use an MLP with 3 layers to make things a bit more interesting.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>net_D <span class="op">=</span> nn.Sequential(</span>
<span id="cb7-2"><a></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">5</span>), nn.Tanh(),</span>
<span id="cb7-3"><a></a>    nn.Linear(<span class="dv">5</span>, <span class="dv">3</span>), nn.Tanh(),</span>
<span id="cb7-4"><a></a>    nn.Linear(<span class="dv">3</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training" class="slide level2">
<h2>Training</h2>
<p>First we define a function to update the discriminator.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="kw">def</span> update_D(X, Z, net_D, net_G, loss, trainer_D):</span>
<span id="cb8-2"><a></a>    <span class="co">"""Update discriminator."""</span></span>
<span id="cb8-3"><a></a>    batch_size <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb8-4"><a></a>    ones <span class="op">=</span> torch.ones((batch_size,), device<span class="op">=</span>X.device)</span>
<span id="cb8-5"><a></a>    zeros <span class="op">=</span> torch.zeros((batch_size,), device<span class="op">=</span>X.device)</span>
<span id="cb8-6"><a></a>    trainer_D.zero_grad()</span>
<span id="cb8-7"><a></a>    real_Y <span class="op">=</span> net_D(X)</span>
<span id="cb8-8"><a></a>    fake_X <span class="op">=</span> net_G(Z)</span>
<span id="cb8-9"><a></a>    <span class="co"># Do not need to compute gradient for `net_G`, detach it from</span></span>
<span id="cb8-10"><a></a>    <span class="co"># computing gradients.</span></span>
<span id="cb8-11"><a></a>    fake_Y <span class="op">=</span> net_D(fake_X.detach())</span>
<span id="cb8-12"><a></a>    loss_D <span class="op">=</span> (loss(real_Y, ones.reshape(real_Y.shape)) <span class="op">+</span></span>
<span id="cb8-13"><a></a>              loss(fake_Y, zeros.reshape(fake_Y.shape))) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb8-14"><a></a>    loss_D.backward()</span>
<span id="cb8-15"><a></a>    trainer_D.step()</span>
<span id="cb8-16"><a></a>    <span class="cf">return</span> loss_D</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section class="slide level2">

<p>The generator is updated similarly. Here we reuse the cross-entropy loss but change the label of the fake data from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="kw">def</span> update_G(Z, net_D, net_G, loss, trainer_G):</span>
<span id="cb9-2"><a></a>    <span class="co">"""Update generator."""</span></span>
<span id="cb9-3"><a></a>    batch_size <span class="op">=</span> Z.shape[<span class="dv">0</span>]</span>
<span id="cb9-4"><a></a>    ones <span class="op">=</span> torch.ones((batch_size,), device<span class="op">=</span>Z.device)</span>
<span id="cb9-5"><a></a>    trainer_G.zero_grad()</span>
<span id="cb9-6"><a></a>    <span class="co"># We could reuse `fake_X` from `update_D` to save computation</span></span>
<span id="cb9-7"><a></a>    fake_X <span class="op">=</span> net_G(Z)</span>
<span id="cb9-8"><a></a>    <span class="co"># Recomputing `fake_Y` is needed since `net_D` is changed</span></span>
<span id="cb9-9"><a></a>    fake_Y <span class="op">=</span> net_D(fake_X)</span>
<span id="cb9-10"><a></a>    loss_G <span class="op">=</span> loss(fake_Y, ones.reshape(fake_Y.shape))</span>
<span id="cb9-11"><a></a>    loss_G.backward()</span>
<span id="cb9-12"><a></a>    trainer_G.step()</span>
<span id="cb9-13"><a></a>    <span class="cf">return</span> loss_G</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section class="slide level2">

<p>Both the discriminator and the generator performs a binary logistic regression with the cross-entropy loss. We use Adam to smooth the training process. In each iteration, we first update the discriminator and then the generator. We visualize both losses and generated examples.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a><span class="kw">def</span> train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):</span>
<span id="cb10-2"><a></a>    loss <span class="op">=</span> nn.BCEWithLogitsLoss(reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb10-3"><a></a>    <span class="cf">for</span> w <span class="kw">in</span> net_D.parameters():</span>
<span id="cb10-4"><a></a>        nn.init.normal_(w, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb10-5"><a></a>    <span class="cf">for</span> w <span class="kw">in</span> net_G.parameters():</span>
<span id="cb10-6"><a></a>        nn.init.normal_(w, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb10-7"><a></a>    trainer_D <span class="op">=</span> torch.optim.Adam(net_D.parameters(), lr<span class="op">=</span>lr_D)</span>
<span id="cb10-8"><a></a>    trainer_G <span class="op">=</span> torch.optim.Adam(net_G.parameters(), lr<span class="op">=</span>lr_G)</span>
<span id="cb10-9"><a></a>    animator <span class="op">=</span> d2l.Animator(xlabel<span class="op">=</span><span class="st">'epoch'</span>, ylabel<span class="op">=</span><span class="st">'loss'</span>,</span>
<span id="cb10-10"><a></a>                            xlim<span class="op">=</span>[<span class="dv">1</span>, num_epochs], nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb10-11"><a></a>                            legend<span class="op">=</span>[<span class="st">'discriminator'</span>, <span class="st">'generator'</span>])</span>
<span id="cb10-12"><a></a>    animator.fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-13"><a></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb10-14"><a></a>        <span class="co"># Train one epoch</span></span>
<span id="cb10-15"><a></a>        timer <span class="op">=</span> d2l.Timer()</span>
<span id="cb10-16"><a></a>        metric <span class="op">=</span> d2l.Accumulator(<span class="dv">3</span>)  <span class="co"># loss_D, loss_G, num_examples</span></span>
<span id="cb10-17"><a></a>        <span class="cf">for</span> (X,) <span class="kw">in</span> data_iter:</span>
<span id="cb10-18"><a></a>            batch_size <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb10-19"><a></a>            Z <span class="op">=</span> torch.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(batch_size, latent_dim))</span>
<span id="cb10-20"><a></a>            metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),</span>
<span id="cb10-21"><a></a>                       update_G(Z, net_D, net_G, loss, trainer_G),</span>
<span id="cb10-22"><a></a>                       batch_size)</span>
<span id="cb10-23"><a></a>        <span class="co"># Visualize generated examples</span></span>
<span id="cb10-24"><a></a>        Z <span class="op">=</span> torch.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(<span class="dv">100</span>, latent_dim))</span>
<span id="cb10-25"><a></a>        fake_X <span class="op">=</span> net_G(Z).detach().numpy()</span>
<span id="cb10-26"><a></a>        animator.axes[<span class="dv">1</span>].cla()</span>
<span id="cb10-27"><a></a>        animator.axes[<span class="dv">1</span>].scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>])</span>
<span id="cb10-28"><a></a>        animator.axes[<span class="dv">1</span>].scatter(fake_X[:, <span class="dv">0</span>], fake_X[:, <span class="dv">1</span>])</span>
<span id="cb10-29"><a></a>        animator.axes[<span class="dv">1</span>].legend([<span class="st">'real'</span>, <span class="st">'generated'</span>])</span>
<span id="cb10-30"><a></a>        <span class="co"># Show the losses</span></span>
<span id="cb10-31"><a></a>        loss_D, loss_G <span class="op">=</span> metric[<span class="dv">0</span>]<span class="op">/</span>metric[<span class="dv">2</span>], metric[<span class="dv">1</span>]<span class="op">/</span>metric[<span class="dv">2</span>]</span>
<span id="cb10-32"><a></a>        animator.add(epoch <span class="op">+</span> <span class="dv">1</span>, (loss_D, loss_G))</span>
<span id="cb10-33"><a></a>    <span class="bu">print</span>(<span class="ss">f'loss_D </span><span class="sc">{</span>loss_D<span class="sc">:.3f}</span><span class="ss">, loss_G </span><span class="sc">{</span>loss_G<span class="sc">:.3f}</span><span class="ss">, '</span></span>
<span id="cb10-34"><a></a>          <span class="ss">f'</span><span class="sc">{</span>metric[<span class="dv">2</span>] <span class="op">/</span> timer<span class="sc">.</span>stop()<span class="sc">:.1f}</span><span class="ss"> examples/sec'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now we specify the hyperparameters to fit the Gaussian distribution.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>lr_D, lr_G, latent_dim, num_epochs <span class="op">=</span> <span class="fl">0.05</span>, <span class="fl">0.005</span>, <span class="dv">2</span>, <span class="dv">20</span></span>
<span id="cb11-2"><a></a><span class="co"># train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,</span></span>
<span id="cb11-3"><a></a><span class="co">#       latent_dim, d2l.numpy(data[:100]))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>Generative adversarial networks (GANs) composes of two deep networks, the generator and the discriminator.</li>
<li>The generator generates the image as much closer to the true image as possible to fool the discriminator, via maximizing the cross-entropy loss, <em>i.e.</em>, <span class="math inline">\(\max \log(D(\mathbf{x'}))\)</span>.</li>
<li>The discriminator tries to distinguish the generated images from the true images, via minimizing the cross-entropy loss, <em>i.e.</em>, <span class="math inline">\(\min - y \log D(\mathbf{x}) - (1-y)\log(1-D(\mathbf{x}))\)</span>.</li>
</ul>
</section>
<section id="exercises" class="slide level2">
<h2>Exercises</h2>
<ul>
<li>Does an equilibrium exist where the generator wins, <em>i.e.</em> the discriminator ends up unable to distinguish the two distributions on finite samples?</li>
</ul>
</section></section>
<section>
<section id="deep-convolutional-generative-adversarial-networks" class="title-slide slide level1">
<h1>Deep Convolutional Generative Adversarial Networks</h1>
<ul>
<li><strong>Introduction to GANs:</strong>
<ul>
<li>Basic idea: Transform samples from simple distributions (uniform, normal) to match dataset distributions.</li>
<li>Previous example: Matching a 2D Gaussian distribution.</li>
</ul></li>
<li><strong>Photorealistic Image Generation:</strong>
<ul>
<li>Use GANs to generate photorealistic images.</li>
<li>Based on deep convolutional GANs (DCGAN) from Radford et al.&nbsp;(2015).</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><strong>DCGAN Architecture:</strong>
<ul>
<li>Leverages convolutional architecture.</li>
<li>Successful for discriminative computer vision tasks.</li>
<li>Adapted for generative tasks to produce realistic images.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a><span class="im">from</span> d2l <span class="im">import</span> torch <span class="im">as</span> d2l</span>
<span id="cb12-2"><a></a><span class="im">import</span> torch</span>
<span id="cb12-3"><a></a><span class="im">import</span> torchvision</span>
<span id="cb12-4"><a></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb12-5"><a></a><span class="im">import</span> warnings</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="the-pokemon-dataset" class="slide level2">
<h2>The Pokemon Dataset</h2>
<p>The dataset we will use is a collection of Pokemon sprites obtained from <a href="https://pokemondb.net/sprites">pokemondb</a>. First download, extract and load this dataset.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a><span class="co"># d2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',</span></span>
<span id="cb13-2"><a></a><span class="co">#                            'c065c0e2593b8b161a2d7873e42418bf6a21106c')</span></span>
<span id="cb13-3"><a></a></span>
<span id="cb13-4"><a></a><span class="co"># data_dir = d2l.download_extract('pokemon')</span></span>
<span id="cb13-5"><a></a><span class="co"># print(data_dir)</span></span>
<span id="cb13-6"><a></a><span class="co"># pokemon = torchvision.datasets.ImageFolder(data_dir)</span></span>
<span id="cb13-7"><a></a>pokemon <span class="op">=</span> torchvision.datasets.EMNIST(root<span class="op">=</span><span class="st">'./'</span>, split<span class="op">=</span><span class="st">'digits'</span>, download<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We resize each image into <span class="math inline">\(64\times 64\)</span>. The <code>ToTensor</code> transformation will project the pixel value into <span class="math inline">\([0, 1]\)</span>, while our generator will use the tanh function to obtain outputs in <span class="math inline">\([-1, 1]\)</span>. Therefore we normalize the data with <span class="math inline">\(0.5\)</span> mean and <span class="math inline">\(0.5\)</span> standard deviation to match the value range.</p>
</section>
<section class="slide level2">

<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>batch_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb14-2"><a></a>transformer <span class="op">=</span> torchvision.transforms.Compose([</span>
<span id="cb14-3"><a></a>    torchvision.transforms.Resize((<span class="dv">64</span>, <span class="dv">64</span>)),</span>
<span id="cb14-4"><a></a>    torchvision.transforms.ToTensor(),</span>
<span id="cb14-5"><a></a>    torchvision.transforms.Normalize(<span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb14-6"><a></a>])</span>
<span id="cb14-7"><a></a>pokemon.transform <span class="op">=</span> transformer</span>
<span id="cb14-8"><a></a>data_iter <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb14-9"><a></a>    pokemon, batch_size<span class="op">=</span>batch_size,</span>
<span id="cb14-10"><a></a>    shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb15-2"><a></a>d2l.set_figsize((<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb15-3"><a></a><span class="cf">for</span> X, y <span class="kw">in</span> data_iter:</span>
<span id="cb15-4"><a></a>    imgs <span class="op">=</span> X[:<span class="dv">20</span>,:,:,:].permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span><span class="op">+</span><span class="fl">0.5</span></span>
<span id="cb15-5"><a></a>    d2l.show_images(imgs, num_rows<span class="op">=</span><span class="dv">4</span>, num_cols<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb15-6"><a></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="the-generator" class="slide level2">
<h2>The Generator</h2>
<p>The generator needs to map the noise variable <span class="math inline">\(\mathbf z\in\mathbb R^d\)</span>, a length-<span class="math inline">\(d\)</span> vector, to a RGB image with width and height to be <span class="math inline">\(64\times 64\)</span> . In :numref:<code>sec_fcn</code> we introduced the fully convolutional network that uses transposed convolution layer (refer to :numref:<code>sec_transposed_conv</code>) to enlarge input size. The basic block of the generator contains a transposed convolution layer followed by the batch normalization and ReLU activation.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a><span class="kw">class</span> G_block(nn.Module):</span>
<span id="cb16-2"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_channels, in_channels<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-3"><a></a>                 padding<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs):</span>
<span id="cb16-4"><a></a>        <span class="bu">super</span>(G_block, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb16-5"><a></a>        <span class="va">self</span>.conv2d_trans <span class="op">=</span> nn.ConvTranspose2d(in_channels, out_channels,</span>
<span id="cb16-6"><a></a>                                kernel_size, strides, padding, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-7"><a></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb16-8"><a></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.ReLU()</span>
<span id="cb16-9"><a></a></span>
<span id="cb16-10"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb16-11"><a></a>        <span class="cf">return</span> <span class="va">self</span>.activation(<span class="va">self</span>.batch_norm(<span class="va">self</span>.conv2d_trans(X)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section class="slide level2">

<p>In default, the transposed convolution layer uses a <span class="math inline">\(k_h = k_w = 4\)</span> kernel, a <span class="math inline">\(s_h = s_w = 2\)</span> strides, and a <span class="math inline">\(p_h = p_w = 1\)</span> padding. With a input shape of <span class="math inline">\(n_h^{'} \times n_w^{'} = 16 \times 16\)</span>, the generator block will double input’s width and height.</p>
<p><span class="math display">\[
\begin{aligned}
n_h^{'} \times n_w^{'} &amp;= [(n_h k_h - (n_h-1)(k_h-s_h)- 2p_h] \times [(n_w k_w - (n_w-1)(k_w-s_w)- 2p_w]\\
  &amp;= [(k_h + s_h (n_h-1)- 2p_h] \times [(k_w + s_w (n_w-1)- 2p_w]\\
  &amp;= [(4 + 2 \times (16-1)- 2 \times 1] \times [(4 + 2 \times (16-1)- 2 \times 1]\\
  &amp;= 32 \times 32 .\\
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a></span>
<span id="cb17-2"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb17-3"><a></a>g_blk <span class="op">=</span> G_block(<span class="dv">20</span>)</span>
<span id="cb17-4"><a></a>g_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section class="slide level2">

<p>If changing the transposed convolution layer to a <span class="math inline">\(4\times 4\)</span> kernel, <span class="math inline">\(1\times 1\)</span> strides and zero padding. With a input size of <span class="math inline">\(1 \times 1\)</span>, the output will have its width and height increased by 3 respectively.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a></span>
<span id="cb18-2"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb18-3"><a></a>g_blk <span class="op">=</span> G_block(<span class="dv">20</span>, strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-4"><a></a>g_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The generator consists of four basic blocks that increase input’s both width and height from 1 to 32. At the same time, it first projects the latent variable into <span class="math inline">\(64\times 8\)</span> channels, and then halve the channels each time. At last, a transposed convolution layer is used to generate the output. It further doubles the width and height to match the desired <span class="math inline">\(64\times 64\)</span> shape, and reduces the channel size to <span class="math inline">\(3\)</span>. The tanh activation function is applied to project output values into the <span class="math inline">\((-1, 1)\)</span> range.</p>
</section>
<section class="slide level2">

<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a>n_G <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb19-2"><a></a>net_G <span class="op">=</span> nn.Sequential(</span>
<span id="cb19-3"><a></a>    G_block(in_channels<span class="op">=</span><span class="dv">100</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">8</span>,</span>
<span id="cb19-4"><a></a>            strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>),                  <span class="co"># Output: (64 * 8, 4, 4)</span></span>
<span id="cb19-5"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">8</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">4</span>), <span class="co"># Output: (64 * 4, 8, 8)</span></span>
<span id="cb19-6"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">4</span>, out_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">2</span>), <span class="co"># Output: (64 * 2, 16, 16)</span></span>
<span id="cb19-7"><a></a>    G_block(in_channels<span class="op">=</span>n_G<span class="op">*</span><span class="dv">2</span>, out_channels<span class="op">=</span>n_G),   <span class="co"># Output: (64, 32, 32)</span></span>
<span id="cb19-8"><a></a>    nn.ConvTranspose2d(in_channels<span class="op">=</span>n_G, out_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb19-9"><a></a>                       kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb19-10"><a></a>    nn.Tanh())  <span class="co"># Output: (3, 64, 64)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Generate a 100 dimensional latent variable to verify the generator’s output shape.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a></a></span>
<span id="cb20-2"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb20-3"><a></a>net_G(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="discriminator-1" class="slide level2">
<h2>Discriminator</h2>
<p>The discriminator is a normal convolutional network network except that it uses a leaky ReLU as its activation function. Given <span class="math inline">\(\alpha \in[0, 1]\)</span>, its definition is</p>
<p><span class="math display">\[\textrm{leaky ReLU}(x) = \begin{cases}x &amp; \text{if}\ x &gt; 0\\ \alpha x &amp;\text{otherwise}\end{cases}.\]</span></p>
<p>As it can be seen, it is normal ReLU if <span class="math inline">\(\alpha=0\)</span>, and an identity function if <span class="math inline">\(\alpha=1\)</span>. For <span class="math inline">\(\alpha \in (0, 1)\)</span>, leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the “dying ReLU” problem that a neuron might always output a negative value and therefore cannot make any progress since the gradient of ReLU is 0.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a></a></span>
<span id="cb21-2"><a></a>alphas <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">.2</span>, <span class="fl">.4</span>, <span class="fl">.6</span>, <span class="fl">.8</span>, <span class="dv">1</span>]</span>
<span id="cb21-3"><a></a>x <span class="op">=</span> d2l.arange(<span class="op">-</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)</span>
<span id="cb21-4"><a></a>Y <span class="op">=</span> [d2l.numpy(nn.LeakyReLU(alpha)(x)) <span class="cf">for</span> alpha <span class="kw">in</span> alphas]</span>
<span id="cb21-5"><a></a>d2l.plot(d2l.numpy(x), Y, <span class="st">'x'</span>, <span class="st">'y'</span>, alphas)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section class="slide level2">

<p>The basic block of the discriminator is a convolution layer followed by a batch normalization layer and a leaky ReLU activation. The hyperparameters of the convolution layer are similar to the transpose convolution layer in the generator block.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a></a><span class="kw">class</span> D_block(nn.Module):</span>
<span id="cb22-2"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_channels, in_channels<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb22-3"><a></a>                padding<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, <span class="op">**</span>kwargs):</span>
<span id="cb22-4"><a></a>        <span class="bu">super</span>(D_block, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb22-5"><a></a>        <span class="va">self</span>.conv2d <span class="op">=</span> nn.Conv2d(in_channels, out_channels, kernel_size,</span>
<span id="cb22-6"><a></a>                                strides, padding, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-7"><a></a>        <span class="va">self</span>.batch_norm <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb22-8"><a></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.LeakyReLU(alpha, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-9"><a></a></span>
<span id="cb22-10"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb22-11"><a></a>        <span class="cf">return</span> <span class="va">self</span>.activation(<span class="va">self</span>.batch_norm(<span class="va">self</span>.conv2d(X)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section class="slide level2">

<p>A basic block with default settings will halve the width and height of the inputs, as we demonstrated in :numref:<code>sec_padding</code>. For example, given a input shape <span class="math inline">\(n_h = n_w = 16\)</span>, with a kernel shape <span class="math inline">\(k_h = k_w = 4\)</span>, a stride shape <span class="math inline">\(s_h = s_w = 2\)</span>, and a padding shape <span class="math inline">\(p_h = p_w = 1\)</span>, the output shape will be:</p>
<p><span class="math display">\[
\begin{aligned}
n_h^{'} \times n_w^{'} &amp;= \lfloor(n_h-k_h+2p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+2p_w+s_w)/s_w\rfloor\\
  &amp;= \lfloor(16-4+2\times 1+2)/2\rfloor \times \lfloor(16-4+2\times 1+2)/2\rfloor\\
  &amp;= 8 \times 8 .\\
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a></span>
<span id="cb23-2"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb23-3"><a></a>d_blk <span class="op">=</span> D_block(<span class="dv">20</span>)</span>
<span id="cb23-4"><a></a>d_blk(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a>n_D <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb24-2"><a></a>net_D <span class="op">=</span> nn.Sequential(</span>
<span id="cb24-3"><a></a>    D_block(n_D),  <span class="co"># Output: (64, 32, 32)</span></span>
<span id="cb24-4"><a></a>    D_block(in_channels<span class="op">=</span>n_D, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">2</span>),  <span class="co"># Output: (64 * 2, 16, 16)</span></span>
<span id="cb24-5"><a></a>    D_block(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">2</span>, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">4</span>),  <span class="co"># Output: (64 * 4, 8, 8)</span></span>
<span id="cb24-6"><a></a>    D_block(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">4</span>, out_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">8</span>),  <span class="co"># Output: (64 * 8, 4, 4)</span></span>
<span id="cb24-7"><a></a>    nn.Conv2d(in_channels<span class="op">=</span>n_D<span class="op">*</span><span class="dv">8</span>, out_channels<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb24-8"><a></a>              kernel_size<span class="op">=</span><span class="dv">4</span>, bias<span class="op">=</span><span class="va">False</span>))  <span class="co"># Output: (1, 1, 1)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>It uses a convolution layer with output channel <span class="math inline">\(1\)</span> as the last layer to obtain a single prediction value.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a></a>x <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">64</span>))</span>
<span id="cb25-2"><a></a>net_D(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-1" class="slide level2">
<h2>Training</h2>
<p>Compared to the basic GAN in :numref:<code>sec_basic_gan</code>, we use the same learning rate for both generator and discriminator since they are similar to each other. In addition, we change <span class="math inline">\(\beta_1\)</span> in Adam (:numref:<code>sec_adam</code>) from <span class="math inline">\(0.9\)</span> to <span class="math inline">\(0.5\)</span>. It decreases the smoothness of the momentum, the exponentially weighted moving average of past gradients, to take care of the rapid changing gradients because the generator and the discriminator fight with each other. Besides, the random generated noise <code>Z</code>, is a 4-D tensor and we are using GPU to accelerate the computation.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a></a><span class="kw">def</span> train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,</span>
<span id="cb26-2"><a></a>          device<span class="op">=</span>d2l.try_gpu()):</span>
<span id="cb26-3"><a></a>    loss <span class="op">=</span> nn.BCEWithLogitsLoss(reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb26-4"><a></a>    <span class="cf">for</span> w <span class="kw">in</span> net_D.parameters():</span>
<span id="cb26-5"><a></a>        nn.init.normal_(w, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb26-6"><a></a>    <span class="cf">for</span> w <span class="kw">in</span> net_G.parameters():</span>
<span id="cb26-7"><a></a>        nn.init.normal_(w, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb26-8"><a></a>    net_D, net_G <span class="op">=</span> net_D.to(device), net_G.to(device)</span>
<span id="cb26-9"><a></a>    trainer_hp <span class="op">=</span> {<span class="st">'lr'</span>: lr, <span class="st">'betas'</span>: [<span class="fl">0.5</span>,<span class="fl">0.999</span>]}</span>
<span id="cb26-10"><a></a>    trainer_D <span class="op">=</span> torch.optim.Adam(net_D.parameters(), <span class="op">**</span>trainer_hp)</span>
<span id="cb26-11"><a></a>    trainer_G <span class="op">=</span> torch.optim.Adam(net_G.parameters(), <span class="op">**</span>trainer_hp)</span>
<span id="cb26-12"><a></a>    animator <span class="op">=</span> d2l.Animator(xlabel<span class="op">=</span><span class="st">'epoch'</span>, ylabel<span class="op">=</span><span class="st">'loss'</span>,</span>
<span id="cb26-13"><a></a>                            xlim<span class="op">=</span>[<span class="dv">1</span>, num_epochs], nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb26-14"><a></a>                            legend<span class="op">=</span>[<span class="st">'discriminator'</span>, <span class="st">'generator'</span>])</span>
<span id="cb26-15"><a></a>    animator.fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb26-16"><a></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb26-17"><a></a>        <span class="co"># Train one epoch</span></span>
<span id="cb26-18"><a></a>        timer <span class="op">=</span> d2l.Timer()</span>
<span id="cb26-19"><a></a>        metric <span class="op">=</span> d2l.Accumulator(<span class="dv">3</span>)  <span class="co"># loss_D, loss_G, num_examples</span></span>
<span id="cb26-20"><a></a>        <span class="cf">for</span> X, _ <span class="kw">in</span> data_iter:</span>
<span id="cb26-21"><a></a>            batch_size <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb26-22"><a></a>            Z <span class="op">=</span> torch.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(batch_size, latent_dim, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb26-23"><a></a>            X, Z <span class="op">=</span> X.to(device), Z.to(device)</span>
<span id="cb26-24"><a></a>            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),</span>
<span id="cb26-25"><a></a>                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),</span>
<span id="cb26-26"><a></a>                       batch_size)</span>
<span id="cb26-27"><a></a>        <span class="co"># Show generated examples</span></span>
<span id="cb26-28"><a></a>        Z <span class="op">=</span> torch.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(<span class="dv">21</span>, latent_dim, <span class="dv">1</span>, <span class="dv">1</span>), device<span class="op">=</span>device)</span>
<span id="cb26-29"><a></a>        <span class="co"># Normalize the synthetic data to N(0, 1)</span></span>
<span id="cb26-30"><a></a>        fake_x <span class="op">=</span> net_G(Z).permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb26-31"><a></a>        imgs <span class="op">=</span> torch.cat(</span>
<span id="cb26-32"><a></a>            [torch.cat([</span>
<span id="cb26-33"><a></a>                fake_x[i <span class="op">*</span> <span class="dv">7</span> <span class="op">+</span> j].cpu().detach() <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">7</span>)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb26-34"><a></a>             <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(fake_x)<span class="op">//</span><span class="dv">7</span>)], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-35"><a></a>        animator.axes[<span class="dv">1</span>].cla()</span>
<span id="cb26-36"><a></a>        animator.axes[<span class="dv">1</span>].imshow(imgs)</span>
<span id="cb26-37"><a></a>        <span class="co"># Show the losses</span></span>
<span id="cb26-38"><a></a>        loss_D, loss_G <span class="op">=</span> metric[<span class="dv">0</span>] <span class="op">/</span> metric[<span class="dv">2</span>], metric[<span class="dv">1</span>] <span class="op">/</span> metric[<span class="dv">2</span>]</span>
<span id="cb26-39"><a></a>        animator.add(epoch, (loss_D, loss_G))</span>
<span id="cb26-40"><a></a>    <span class="bu">print</span>(<span class="ss">f'loss_D </span><span class="sc">{</span>loss_D<span class="sc">:.3f}</span><span class="ss">, loss_G </span><span class="sc">{</span>loss_G<span class="sc">:.3f}</span><span class="ss">, '</span></span>
<span id="cb26-41"><a></a>          <span class="ss">f'</span><span class="sc">{</span>metric[<span class="dv">2</span>] <span class="op">/</span> timer<span class="sc">.</span>stop()<span class="sc">:.1f}</span><span class="ss"> examples/sec on </span><span class="sc">{</span><span class="bu">str</span>(device)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section class="slide level2">

<p>We train the model with a small number of epochs just for demonstration. For better performance, the variable <code>num_epochs</code> can be set to a larger number.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a></a>latent_dim, lr, num_epochs <span class="op">=</span> <span class="dv">100</span>, <span class="fl">0.005</span>, <span class="dv">20</span></span>
<span id="cb27-2"><a></a><span class="co"># train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="summary-1" class="slide level2">
<h2>Summary</h2>
<ul>
<li>DCGAN architecture has four convolutional layers for the Discriminator and four “fractionally-strided” convolutional layers for the Generator.</li>
<li>The Discriminator is a 4-layer strided convolutions with batch normalization (except its input layer) and leaky ReLU activations.</li>
<li>Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the “dying ReLU” problem and helps the gradients flow easier through the architecture.</li>
</ul>
</section>
<section id="exercises-1" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>What will happen if we use standard ReLU activation rather than leaky ReLU?</li>
<li>Apply DCGAN on Fashion-MNIST and see which category works well and which does not.</li>
</ol>
</section></section>
<section>
<section id="application-examples" class="title-slide slide level1">
<h1>Application examples</h1>

</section>
<section id="example-1-leveraging-generative-adversarial-networks-to-create-realistic-scanning-transmission-electron-microscopy-images" class="slide level2">
<h2>Example 1: Leveraging Generative Adversarial Networks to Create Realistic Scanning Transmission Electron Microscopy Images</h2>
<p><strong>Authors:</strong> Abid Khan, Chia-Hao Lee, Pinshane Y. Huang, Bryan K. Clark<br>
<strong>Published in:</strong> npj Computational Materials (2023)<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1038/s41524-023-01042-3">10.1038/s41524-023-01042-3</a></p>

<img data-src="../img2/huang1.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Generative Adversarial Networks</p></section>
<section id="introduction-1" class="slide level2">
<h2>Introduction</h2>
<ul>
<li>Machine learning (ML) in electron microscopy:
<ul>
<li>Atom localization</li>
<li>Defect identification</li>
<li>Image denoising</li>
</ul></li>
<li>Challenge: High-quality training data with ground truth for supervised learning.</li>
<li>Solution: Use CycleGANs to bridge the gap between simulated and experimental STEM images. <img data-src="../img2/huang4.png" style="background: grey;width:80.0%" alt="Generative Adversarial Networks"></li>
</ul>
</section>
<section id="generating-realistic-images-with-cyclegans" class="slide level2">
<h2>Generating Realistic Images with CycleGANs</h2>
<ul>
<li>CycleGANs are used for image-to-image translation.</li>
<li>Training involves both experimental and simulated images.</li>
<li>CycleGANs produce images that match experimental data in terms of noise, distortions, and other artifacts.</li>
<li>Results show significant improvement in realism compared to unoptimized simulated images.</li>
</ul>

<img data-src="../img2/huang2.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">CycleGAN</p></section>
<section id="variations-within-experimental-stem-data" class="slide level2">
<h2>variations within experimental STEM data</h2>
<ul>
<li>Experimental STEM images acquired on Day A (a–c) and B (e–g) showing variations within and between days due to sample contamination and microscope instabilities.</li>
<li>d, h Power spectra obtained with FFT of a and e demonstrate the variation of image resolution, from 96 pm to 110 pm, determined by the highest transferred spatial frequency marked with white circles.</li>
<li>The power spectra are displayed in log scale for visibility. Scale bars are equal to 1 nm.</li>
</ul>

<img data-src="../img2/huang5.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Example of variations within experimental STEM data sets taken on two different days, Day A and Day B.</p></section>
<section id="cyclegan-architecture-and-optimization" class="slide level2">
<h2>CycleGAN Architecture and Optimization</h2>
<ul>
<li>CycleGAN translates images between two domains:
<ol type="1">
<li>the experiment domain X and</li>
<li>the simulation domain Y, and has two main components: generators and discriminators.</li>
</ol></li>
<li>generator G converts input experimental images x to simulation-like images G(x), and the generator F converts input simulated images y to experiment-like images F(y).</li>
<li>generated images (F(y) and G(x)) are then fed into four discriminators (Dx,img, Dx,FFT, Dy,img, Dy,FFT) along with the raw images (x and y) to evaluate the quality of generated images.</li>
</ul>

<img data-src="../img2/huang2.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Schematic of the major components in a CycleGAN.</p></section>
<section class="slide level2">

<ul>
<li>Both input images and their FFTs are examined by the discriminators to calculate the adversarial losses (Ladv), which are used to optimize both generators F and G respectively.</li>
<li>By passing the raw images (x and y) with combinations of both generators, identity images (F(x) and G(y)) and cycled images (F(G(x)) and G(F(y))) are also generated.</li>
<li>corresponding identity loss Lid and cycle consistency loss Lcyc are added to ensure the identity and cycle consistency mapping of the generators.</li>
</ul>

<img data-src="../img2/huang2.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Schematic of the major components in a CycleGAN.</p></section>
<section id="evaluation-metrics" class="slide level2">
<h2>Evaluation Metrics</h2>
<ul>
<li>Quantitative evaluation using:
<ul>
<li>Fréchet Inception Distance (FID)</li>
<li>Kullback–Leibler (KL) divergence</li>
</ul></li>
<li>Results:
<ul>
<li>CycleGAN-processed images have the lowest FID scores and KL divergence, indicating high similarity to experimental images.</li>
</ul></li>
</ul>
</section>
<section id="fréchet-inception-distance-fid" class="slide level2">
<h2>Fréchet Inception Distance (FID)</h2>
<ul>
<li><strong>Purpose</strong>: Measure similarity between generated images and real images.</li>
<li><strong>Calculation</strong>:
<ul>
<li>Preprocess the images. Ensure the two images are compatible using basic processing.</li>
<li><strong>Extract feature representations</strong>. Pass the real and generated images through the Inception-v3 model. This transforms the raw pixels into numerical vectors to represent aspects of the images, such as lines, edges and higher-order shapes.</li>
<li><strong>Calculate statistics</strong>. Statistical analysis is performed to determine the mean and covariance matrix of the features in each image.</li>
<li>Treats images as multivariate Gaussian distributions</li>
<li><span class="math inline">\(d_{F}(\mathcal N(\mu, \Sigma), \mathcal N(\mu', \Sigma'))^2 = \lVert \mu - \mu' \rVert^2_2 + \operatorname{tr}\left(\Sigma + \Sigma' -2\left(\Sigma \Sigma'  \right)^\frac{1}{2} \right)\)</span></li>
</ul></li>
<li><strong>Interpretation</strong>:
<ul>
<li>Lower FID indicates higher similarity and better quality of generated images.</li>
</ul></li>
</ul>
</section>
<section id="quantitative-measurements-of-data-set-quality-using-fid-and-kl" class="slide level2">
<h2>Quantitative measurements of data set quality using FID and KL</h2>
<ul>
<li>Images are generated from (a) experiments, (b) simulation without noise, (c) simulation with manually optimized noise, and (d) CycleGAN.</li>
<li>FID score measures the dissimilarity between image data sets, so a smaller FID score implies higher similarity between image data sets.</li>
<li>e–h Histograms of normalized pixel intensity are calculated for each image data set. Each histogram is normalized so that the probability distribution sums to unity.</li>
</ul>

<img data-src="../img2/huang3.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Quantitative measurements of data set quality using FID and KL</p></section>
<section id="quantitative-measurements-of-data-set-quality-using-fid-and-kl-2" class="slide level2">
<h2>Quantitative measurements of data set quality using FID and KL 2</h2>
<ul>
<li>KL divergence DKL(P∣∣Q) of each data set with the experimental histogram are labeled as DKL at the top right corner of each histogram, with the lowest non-zero value marked in red.</li>
<li>both the FID score and KL divergence of intensity histograms are calculated for the entire data set with respect to the experimental data set, where each data set contains roughly 1700 image patches with 256 × 256 pixels.</li>
<li>CycleGAN generated image set exhibits the best FID score and lowest KL divergence, indicating it is the best match for experimental data.</li>
</ul>

<img data-src="../img2/huang3.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Quantitative measurements of data set quality using FID and KL</p></section>
<section id="defect-identification-with-cyclegan-and-fcn" class="slide level2">
<h2>Defect Identification with CycleGAN and FCN</h2>
<ul>
<li>Workflow:
<ol type="1">
<li>Acquire experimental STEM images.</li>
<li>Simulate STEM images with known defects.</li>
<li>Train CycleGAN with both experimental and simulated images.</li>
</ol></li>
</ul>

<img data-src="../img2/huang4.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Schematic of the major components in a CycleGAN.</p></section>
<section id="defect-identification-with-cyclegan-and-fcn-1" class="slide level2">
<h2>Defect Identification with CycleGAN and FCN</h2>
<ul>
<li>Workflow:
<ol start="4" type="1">
<li>Generate realistic images using CycleGAN.</li>
<li>Train Fully Convolutional Network (FCN) on CycleGAN-processed images.</li>
<li>Use FCN to identify defects in experimental images.</li>
</ol></li>
<li>Results: High precision and recall in defect identification with minimal human intervention.</li>
</ul>

<img data-src="../img2/huang6.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">The CycleGAN-processed images preserve the defect types and positions from the input simulated images.</p></section>
<section id="conclusion-1" class="slide level2">
<h2>Conclusion</h2>
<ul>
<li>CycleGANs effectively bridge the gap between simulated and experimental STEM images.</li>
<li>FCNs trained on CycleGAN-processed images achieve high accuracy in defect identification.</li>
<li>Potential for real-time, automated microscopy data processing.</li>
</ul>
</section>
<section id="example-2-generation-of-highly-realistic-microstructural-images-of-alloys-from-limited-data-with-a-style-based-generative-adversarial-network" class="slide level2">
<h2>Example 2: Generation of Highly Realistic Microstructural Images of Alloys from Limited Data with a Style-Based Generative Adversarial Network</h2>
<p><strong>Authors:</strong> Guillaume Lambard, Kazuhiko Yamazaki, Masahiko Demura<br>
<strong>Published in:</strong> Scientific Reports (2023)<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1038/s41598-023-27574-8">10.1038/s41598-023-27574-8</a></p>
</section>
<section id="introduction-2" class="slide level2">
<h2>Introduction</h2>
<ul>
<li><strong>Microstructural Characterization</strong>: Essential for understanding material properties.</li>
<li><strong>Challenges</strong>: Limited availability of high-quality SEM images.</li>
<li><strong>Solution</strong>: Use StyleGAN2 with ADA to generate synthetic SEM images from a small dataset.</li>
</ul>
</section>
<section id="stylegan2-with-ada" class="slide level2">
<h2>StyleGAN2 with ADA</h2>
<ul>
<li><strong>StyleGAN2 Architecture</strong>:
<ul>
<li>Adaptive Discriminator Augmentation (ADA)</li>
<li>Effective in low data regimes</li>
</ul></li>
<li><strong>Training</strong>:
<ul>
<li>Dataset: 3000 SEM images of ferrite-martensite dual-phase steel</li>
<li>Image size: 512x512 pixels</li>
<li>Augmentations: Pixel blitting, geometrical transformations</li>
</ul></li>
</ul>
</section>
<section id="stylegan2-with-ada-1" class="slide level2">
<h2>StyleGAN2 with ADA</h2>
<ul>
<li><strong>Problem</strong>: GANs require large datasets to avoid discriminator overfitting.</li>
<li><strong>Solution</strong>: Adaptive Discriminator Augmentation (ADA) to stabilize training with limited data.</li>
<li><strong>Key Insight</strong>: ADA prevents overfitting without changing loss functions or network architectures.</li>
</ul>
</section>
<section id="adaptive-discriminator-augmentation-ada" class="slide level2">
<h2>Adaptive Discriminator Augmentation (ADA)</h2>
<ul>
<li><strong>Objective</strong>: Apply augmentations to prevent discriminator overfitting.</li>
<li><strong>Mechanism</strong>:
<ul>
<li>Stochastic augmentation of discriminator inputs.</li>
<li>Augmentations include geometric and color transformations.</li>
<li>Adaptive control based on overfitting heuristics.</li>
</ul></li>
<li><strong>Benefits</strong>:
<ul>
<li>Effective even with few thousand training images.</li>
<li>Maintains image quality and diversity.</li>
</ul></li>
</ul>
</section>
<section id="augmentations-and-training" class="slide level2">
<h2>Augmentations and Training</h2>
<ul>
<li><strong>Augmentation Strategies</strong>:
<ul>
<li>Pixel blitting and geometrical transformations improved FID by ~77%</li>
<li>Other augmentations like color transformations and additive noise were less effective.</li>
</ul></li>
<li><strong>Target Heuristic (rt)</strong>:
<ul>
<li>Optimal value: 0.5</li>
<li>Balances between reducing overfitting and maintaining diversity</li>
</ul></li>
</ul>
</section>
<section id="results-and-evaluation" class="slide level2">
<h2>Results and Evaluation</h2>
<ul>
<li><strong>Evaluation Metrics</strong>:
<ul>
<li>Fréchet Inception Distance (FID)</li>
<li>Recall Metric</li>
</ul></li>
</ul>
</section>
<section id="results-and-evaluation2" class="slide level2">
<h2>Results and Evaluation2</h2>
<ul>
<li><strong>Results</strong>:
<ul>
<li>Best FID: 6.59 with 3000 images</li>
<li>High-quality and diverse SEM images</li>
<li>Successful interpolation between microstructures</li>
</ul></li>
</ul>

<img data-src="../img2/alloy5.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Samples of non-curated SEM images generated with the StyleGAN2 with ADA</p></section>
<section id="interpolation-and-diversity" class="slide level2">
<h2>Interpolation and Diversity</h2>
<ul>
<li><strong>Latent Space Interpolation</strong>:
<ul>
<li>Smooth transitions between different microstructures</li>
<li>Demonstrates the potential for exploring new microstructural features</li>
</ul></li>
</ul>

<img data-src="../img2/alloy8.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Selection of 4 non-curated generated SEM images obtained thanks to a spherical linear interpolation</p></section>
<section id="interpolation-and-diversity-2" class="slide level2">
<h2>Interpolation and Diversity 2</h2>
<ul>
<li><strong>Generated Images</strong>:
<ul>
<li>High resemblance to real SEM images</li>
<li>Captured both coarse and fine microstructural details</li>
</ul></li>
</ul>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="eclipse_logo_small.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://github.com/ECLIPSE-Lab/WS24_DataScienceForEM">WS24_DataScienceForEM</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: false,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/ECLIPSE-Lab\.github\.io\/public_presentations\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>