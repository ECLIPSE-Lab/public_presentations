<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-927edb3cde42616945691bbf0360b549.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.30">

  <meta name="author" content="Philipp Pelz">
  <title>ECLIPSE Presentations – Data Science for Electron Microscopy  Lecture 6: Gaussian Processes 1</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #97947a;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #97947a;  padding-left: 4px; }
    div.sourceCode
      { color: #f8f8f2; background-color: #2b2b2b; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #f8f8f2; } /* Normal */
    code span.al { color: #dcc6e0; } /* Alert */
    code span.an { color: #d4d0ab; } /* Annotation */
    code span.at { color: #ffd700; } /* Attribute */
    code span.bn { color: #dcc6e0; } /* BaseN */
    code span.bu { color: #f5ab35; } /* BuiltIn */
    code span.cf { color: #ffa07a; } /* ControlFlow */
    code span.ch { color: #abe338; } /* Char */
    code span.cn { color: #ffa07a; } /* Constant */
    code span.co { color: #d4d0ab; } /* Comment */
    code span.cv { color: #d4d0ab; font-style: italic; } /* CommentVar */
    code span.do { color: #d4d0ab; font-style: italic; } /* Documentation */
    code span.dt { color: #dcc6e0; } /* DataType */
    code span.dv { color: #dcc6e0; } /* DecVal */
    code span.er { color: #dcc6e0; } /* Error */
    code span.ex { color: #ffd700; } /* Extension */
    code span.fl { color: #f5ab35; } /* Float */
    code span.fu { color: #ffd700; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #d4d0ab; } /* Information */
    code span.kw { color: #ffa07a; } /* Keyword */
    code span.op { color: #00e0e0; } /* Operator */
    code span.ot { color: #ffa07a; } /* Other */
    code span.pp { color: #dcc6e0; } /* Preprocessor */
    code span.sc { color: #00e0e0; } /* SpecialChar */
    code span.ss { color: #abe338; } /* SpecialString */
    code span.st { color: #abe338; } /* String */
    code span.va { color: #f5ab35; } /* Variable */
    code span.vs { color: #abe338; } /* VerbatimString */
    code span.wa { color: #d4d0ab; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-ce804447b3af982f2c55816970ecc9a3.css">
  <link rel="stylesheet" href="custom.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
    <h1 class="title"><p>Data Science for Electron Microscopy<br> Lecture 6: Gaussian Processes 1</p></h1>
    
  <div class="quarto-title-authors">
    <div class="quarto-title-author">
  <div class="quarto-title-author-name">
  Philipp Pelz 
  </div>
                <p class="quarto-title-affiliation">
                FAU Erlangen-Nürnberg
              </p>
          </div>
    </div>
  
    <div class="footer-logos1">
    <img src="logos/FAU.png" alt="FAU Logo" width="20%">
    <img src="logos/imn.png" alt="IMN Logo" width="20%">
    <img src="logos/cenem.png" alt="CENEM Logo" width="20%">
    <img src="logos/erc.jpg" alt="Elettra Logo" width="20%">
  </div>
  </section>
<section class="slide level2">

<!-- {{< include gaussian_processes.qmd  >}}    -->
</section>
<section id="introduction-to-gaussian-processes-1" class="slide level2">
<h2>Introduction to Gaussian Processes 1</h2>
<ul>
<li>Gaussian processes provide a mechanism for directly reasoning about the high-level properties of functions that could fit our data.</li>
</ul>
<div class="fragment">
<ul>
<li>may have a sense of whether these functions are quickly varying, periodic, involve conditional independencies, or translation invariance.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Gaussian processes: easily incorporate these properties into our model, by directly specifying a Gaussian distribution over the function values that could fit our data.</li>
</ul>
</div>
</section>
<section id="introduction-to-gaussian-processes-2" class="slide level2">
<h2>Introduction to Gaussian Processes 2</h2>
<ul>
<li><p>Suppose we observe the following dataset, of regression targets (outputs), <span class="math inline">\(y\)</span>, indexed by inputs, <span class="math inline">\(x\)</span>. <img data-src="img/gp-observed-data.svg" style="width:40.0%" alt="Observed data."></p></li>
<li><p>example: targets could be changes in carbon dioxide concentrations, inputs could be the times at which these targets have been recorded</p></li>
</ul>
<div class="fragment">
<ul>
<li>What are some features of the data? How quickly does it seem to varying? Do we have data points collected at regular intervals, or are there missing inputs? How would you imagine filling in the missing regions, or forecasting up until <span class="math inline">\(x=25\)</span>?</li>
</ul>
</div>
</section>
<section id="introduction-to-gaussian-processes-3" class="slide level2">
<h2>Introduction to Gaussian Processes 3</h2>
<ul>
<li>start by specifying a prior distribution over what types of functions we might believe to be reasonable.</li>
</ul>
<div class="fragment">
<ul>
<li>show several sample functions from a Gaussian process. Does this prior look reasonable? we are not looking for functions that fit our dataset, but instead for specifying reasonable high-level properties of the solutions, such as how quickly they vary with inputs. Note that we will see code for reproducing all of the plots in this notebook, in the next notebooks on priors and inference.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gp-sample-prior-functions.svg" style="width:30.0%"></p>
<figcaption>Sample prior functions that we may want to represent with our model.</figcaption>
</figure>
</div>
</div>
</section>
<section id="introduction-to-gaussian-processes-3.5" class="slide level2">
<h2>Introduction to Gaussian Processes 3.5</h2>

<img data-src="img/bayes_law.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Bayes theorem</p></section>
<section id="introduction-to-gaussian-processes-4" class="slide level2">
<h2>Introduction to Gaussian Processes 4</h2>
<p>Once we condition on data, we can use this prior to infer a posterior distribution over functions that could fit the data. Here we show sample posterior functions.</p>

<img data-src="img/gp-sample-posterior-functions.svg" class="r-stretch quarto-figure-center"><p class="caption">Sample posterior functions, once we have observed the data.</p><div class="fragment">
<ul>
<li>each of these functions are entirely consistent with our data, perfectly running through each observation.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In order to use these posterior samples to make predictions, we can average the values of every possible sample function from the posterior, to create the curve below, in thick blue.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Note that we do not actually have to take an infinite number of samples to compute this expectation; as we will see later, we can compute the expectation in closed form.</li>
</ul>
</div>
</section>
<section id="introduction-to-gaussian-processes-5" class="slide level2">
<h2>Introduction to Gaussian Processes 5</h2>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gp-posterior-samples.svg" style="width:10in"></p>
<figcaption>Posterior samples, alongside posterior mean, which can be used for point predictions, in blue.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><p>may also want a representation of uncertainty, so we know how confident we should be in our predictions.</p></li>
<li><p>Intuitively: more variability in the sample posterior functions –&gt; more uncertainty</p></li>
<li><p><em>epistemic uncertainty</em>, which is the <em>reducible uncertainty</em> associated with lack of information.</p></li>
<li><p>acquire more data –&gt; this type of uncertainty disappears, as there will be increasingly fewer solutions consistent with what we observe.</p></li>
<li><p>Like with the posterior mean, we can compute the posterior variance (the variability of these functions in the posterior) in closed form.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="introduction-to-gaussian-processes-6" class="slide level2">
<h2>Introduction to Gaussian Processes 6</h2>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gp-posterior-samples-95.svg" style="width:10in"></p>
<figcaption>Posterior samples, including 95% credible set.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>shade: two times the posterior standard deviation on either side of the mean, creating a <em>credible interval</em> that has a 95% probability of containing the true value of the function for any input <span class="math inline">\(x\)</span>.</li>
<li>plot looks somewhat cleaner if we remove the posterior samples, simply visualizing the data, posterior mean, and 95% credible set.</li>
<li>Notice how the uncertainty grows away from the data, a property of epistemic uncertainty.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="introduction-to-gaussian-processes-7" class="slide level2">
<h2>Introduction to Gaussian Processes 7</h2>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gp-point-predictions.svg" style="width:10in"></p>
<figcaption>Point predictions, and credible set.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><p>properties of the Gaussian process that we used to fit the data are strongly controlled by what’s called a <em>covariance function</em>, also known as a <em>kernel</em>.</p></li>
<li><p>covariance function we used is called the <em>RBF (Radial Basis Function) kernel</em>, which has the form <span class="math display">\[ k_{\text{RBF}}(x,x') = \mathrm{Cov}(f(x),f(x')) = a^2 \exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right) \]</span></p></li>
<li><p>The <em>hyperparameters</em> of this kernel are interpretable. The <em>amplitude</em> parameter <span class="math inline">\(a\)</span> controls the vertical scale over which the function is varying, and the <em>length-scale</em> parameter <span class="math inline">\(\ell\)</span> controls the rate of variation (the wiggliness) of the function.</p></li>
<li><p>Larger <span class="math inline">\(a\)</span> means larger function values, and larger <span class="math inline">\(\ell\)</span> means more slowly varying functions. Let’s see what happens to our sample prior and posterior functions as we vary <span class="math inline">\(a\)</span> and <span class="math inline">\(\ell\)</span>.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="introduction-to-gaussian-processes-8" class="slide level2">
<h2>Introduction to Gaussian Processes 8</h2>
<p><span class="math display">\[ k_{\text{RBF}}(x,x') = \mathrm{Cov}(f(x),f(x')) = a^2 \exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right) \]</span></p>
<ul>
<li><p>The <em>length-scale</em> has a particularly pronounced effect on the predictions and uncertainty of a GP. At <span class="math inline">\(||x-x'|| = \ell\)</span> , the covariance between a pair of function values is <span class="math inline">\(a^2\exp(-0.5)\)</span>.</p></li>
<li><p>At larger distances than <span class="math inline">\(\ell\)</span> , the values of the function values becomes nearly uncorrelated. This means that if we want to make a prediction at a point <span class="math inline">\(x_*\)</span>, then function values with inputs <span class="math inline">\(x\)</span> such that <span class="math inline">\(||x-x'||&gt;\ell\)</span> will not have a strong effect on our predictions.</p></li>
</ul>
</section>
<section id="introduction-to-gaussian-processes-9" class="slide level2">
<h2>Introduction to Gaussian Processes 9</h2>
<ul>
<li>how changing the lengthscale affects sample prior and posterior functions, and credible sets. The above fits use a length-scale of <span class="math inline">\(2\)</span>. Let’s now consider <span class="math inline">\(\ell = 0.1, 0.5, 2, 5, 10\)</span> .</li>
</ul>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><img data-src="img/gp-priorpoint1.svg" style="width:45.0%" alt="priorpoint1"> <img data-src="img/gp-postpoint1.svg" style="width:45.0%" alt="postpoint1"> <img data-src="img/gp-priorpoint5.svg" style="width:45.0%" alt="priorpoint5"> <img data-src="img/gp-postpoint5.svg" style="width:45.0%" alt="postpoint5"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>A length-scale of <span class="math inline">\(0.1\)</span> is very small relative to the range of the input domain we are considering, <span class="math inline">\(25\)</span>. For example, the values of the function at <span class="math inline">\(x=5\)</span> and <span class="math inline">\(x=10\)</span> will have essentially no correlation at such a length-scale.</li>
<li>On the other hand, for a length-scale of <span class="math inline">\(10\)</span>, the function values at these inputs will be highly correlated.</li>
<li>Note that the vertical scale changes in the following figures.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="introduction-to-gaussian-processes-10" class="slide level2">
<h2>Introduction to Gaussian Processes 10</h2>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><img data-src="img/gp-prior2.svg" style="width:40.0%" alt="prior2"> <img data-src="img/gp-post2.svg" style="width:40.0%" alt="post2"> <img data-src="img/gp-prior5.svg" style="width:40.0%" alt="prior5"> <img data-src="img/gp-post5.svg" style="width:40.0%" alt="post5"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>length-scale of <span class="math inline">\(0.1\)</span> is very small relative to the range of the input domain we are considering, <span class="math inline">\(25\)</span>.</li>
<li>For example, the values of the function at <span class="math inline">\(x=5\)</span> and <span class="math inline">\(x=10\)</span> will have essentially no correlation at such a length-scale.</li>
<li>On the other hand, for a length-scale of <span class="math inline">\(10\)</span>, the function values at these inputs will be highly correlated.</li>
<li>Note that the vertical scale changes in the following figures.</li>
</ul>
</div>
</div>
</div>
<ul>
<li>as the length-scale increases the ‘wiggliness’ of the functions decrease, and our uncertainty decreases.</li>
<li>If the length-scale is small, the uncertainty will quickly increase as we move away from the data, as the datapoints become less informative about the function values.</li>
</ul>
</section>
<section id="introduction-to-gaussian-processes-11" class="slide level2">
<h2>Introduction to Gaussian Processes 11</h2>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><img data-src="img/gp-priorap1.svg" style="width:40.0%" alt="priorap1"> <img data-src="img/gp-postapoint1.svg" style="width:40.0%" alt="postapoint1"> <img data-src="img/gp-priora2.svg" style="width:40.0%" alt="priora2"> <img data-src="img/gp-posta2.svg" style="width:40.0%" alt="posta2"> <img data-src="img/gp-priora8.svg" style="width:40.0%" alt="priora8"> <img data-src="img/gp-posta8.svg" style="width:40.0%" alt="posta8"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>now vary the amplitude parameter, holding the length-scale fixed at <span class="math inline">\(2\)</span>.</li>
<li>Note the vertical scale is held fixed for the prior samples, and varies for the posterior samples, so you can clearly see both the increasing scale of the function, and the fits to the data.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="introduction-to-gaussian-processes-12" class="slide level2">
<h2>Introduction to Gaussian Processes 12</h2>
<p><span class="math display">\[ k_{\text{RBF}}(x,x') = \mathrm{Cov}(f(x),f(x')) = a^2 \exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right) \]</span></p>
<div class="fragment">
<ul>
<li><p>amplitude parameter affects the scale of the function, but not the rate of variation. . . .</p></li>
<li><p>generalization performance of our procedure will depend on having reasonable values for these hyperparameters. . . .</p></li>
<li><p>Values of <span class="math inline">\(\ell=2\)</span> and <span class="math inline">\(a=1\)</span> appeared to provide reasonable fits, while some of the other values did not.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Fortunately, there is a robust and automatic way to specify these hyperparameters, using what is called the <em>marginal likelihood</em>, which we will return to in the notebook on inference.</li>
</ul>
</div>
</section>
<section id="interactive-visualization" class="slide level2">
<h2>Interactive Visualization</h2>
<iframe src="https://www.infinitecuriosity.org/vizgp/" style="width:100%; height:800px;" frameborder="0" scrolling="yes">
</iframe>
<script>
window.addEventListener('load', function() {
    var iframe = document.querySelector('iframe');
    var iframeDoc = iframe.contentWindow.document;
    
    // Hide everything except the body content
    var style = iframeDoc.createElement('style');
    style.textContent = `
        header, footer, nav, aside {
            display: none !important;
        }
        body {
            margin: 0;
            padding: 0;
        }
    `;
    iframeDoc.head.appendChild(style);
});
</script>
</section>
<section id="so-what-is-a-gp-really" class="slide level2">
<h2>So what is a GP, really?</h2>
<ul>
<li>GP: any collection of function values <span class="math inline">\(f(x_1),\dots,f(x_n)\)</span>, indexed by any collection of inputs <span class="math inline">\(x_1,\dots,x_n\)</span> has a joint multivariate Gaussian distribution.</li>
</ul>
<div class="fragment">
<ul>
<li><p>mean vector <span class="math inline">\(\mu\)</span> of this distribution is given by a <em>mean function</em>, which is typically taken to be a constant or zero.</p></li>
<li><p>covariance matrix of this distribution is given by the <em>kernel</em> evaluated at all pairs of the inputs <span class="math inline">\(x\)</span>.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math display">\[\begin{bmatrix}f(x) \\f(x_1) \\ \vdots \\ f(x_n) \end{bmatrix}\sim \mathcal{N}\left(\mu, \begin{bmatrix}k(x,x) &amp; k(x, x_1) &amp; \dots &amp; k(x,x_n) \\ k(x_1,x) &amp; k(x_1,x_1) &amp; \dots &amp; k(x_1,x_n) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ k(x_n, x) &amp; k(x_n, x_1) &amp; \dots &amp; k(x_n,x_n) \end{bmatrix}\right)\]</span> <code>(1)</code></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Equation <code>(1)</code> specifies a GP prior. We can compute the conditional distribution of <span class="math inline">\(f(x)\)</span> for any <span class="math inline">\(x\)</span> given <span class="math inline">\(f(x_1), \dots, f(x_n)\)</span>, the function values we have observed.</li>
<li>This conditional distribution is called the <em>posterior</em>, and it is what we use to make predictions.</li>
</ul>
</div>
</section>
<section id="introduction-to-gaussian-processes-14" class="slide level2">
<h2>Introduction to Gaussian Processes 14</h2>
<p>In particular,</p>
<p><span class="math display">\[f(x) | f(x_1), \dots, f(x_n) \sim \mathcal{N}(m,s^2)\]</span></p>
<p>where</p>
<p><span class="math display">\[m = k(x,x_{1:n}) k(x_{1:n},x_{1:n})^{-1} f(x_{1:n})\]</span></p>
<p><span class="math display">\[s^2 = k(x,x) - k(x,x_{1:n})k(x_{1:n},x_{1:n})^{-1}k(x,x_{1:n})\]</span></p>
<p>where <span class="math inline">\(k(x,x_{1:n})\)</span> is a <span class="math inline">\(1 \times n\)</span> vector formed by evaluating <span class="math inline">\(k(x,x_{i})\)</span> for <span class="math inline">\(i=1,\dots,n\)</span> and <span class="math inline">\(k(x_{1:n},x_{1:n})\)</span> is an <span class="math inline">\(n \times n\)</span> matrix formed by evaluating <span class="math inline">\(k(x_i,x_j)\)</span> for <span class="math inline">\(i,j = 1,\dots,n\)</span>. <span class="math inline">\(m\)</span> is what we can use as a point predictor for any <span class="math inline">\(x\)</span>, and <span class="math inline">\(s^2\)</span> is what we use for uncertainty:</p>
<ul>
<li><p>if we want to create an interval with a 95% probability that <span class="math inline">\(f(x)\)</span> is in the interval, we would use <span class="math inline">\(m \pm 2s\)</span>.</p></li>
<li><p>predictive means and uncertainties for all the above figures were created using these equations.</p></li>
<li><p>observed data points were given by <span class="math inline">\(f(x_1), \dots, f(x_n)\)</span> and chose a fine grained set of <span class="math inline">\(x\)</span> points to make predictions.</p></li>
</ul>
</section>
<section id="introduction-to-gaussian-processes-15" class="slide level2">
<h2>Introduction to Gaussian Processes 15</h2>
<ul>
<li>suppose we observe a single datapoint, <span class="math inline">\(f(x_1)\)</span>, and we want to determine the value of <span class="math inline">\(f(x)\)</span> at some <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(f(x)\)</span> described by Gaussian process –&gt; joint distribution over <span class="math inline">\((f(x), f(x_1))\)</span> is Gaussian:</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
f(x) \\
f(x_1) \\
\end{bmatrix}
\sim
\mathcal{N}\left(\mu,
\begin{bmatrix}
k(x,x) &amp; k(x, x_1) \\
k(x_1,x) &amp; k(x_1,x_1)
\end{bmatrix}
\right)
\]</span></p>
<ul>
<li>off-diagonal expression <span class="math inline">\(k(x,x_1) = k(x_1,x)\)</span> tells us how correlated the function values will be — how strongly determined <span class="math inline">\(f(x)\)</span> will be from <span class="math inline">\(f(x_1)\)</span>.</li>
<li>have seen already that if we use a large length-scale, relative to the distance between <span class="math inline">\(x\)</span> and <span class="math inline">\(x_1\)</span>, <span class="math inline">\(||x-x_1||\)</span>, then the function values will be highly correlated.</li>
<li>visualize the process of determining <span class="math inline">\(f(x)\)</span> from <span class="math inline">\(f(x_1)\)</span> both in the space of functions, and in the joint distribution over <span class="math inline">\(f(x_1), f(x)\)</span>.</li>
<li>initially consider an <span class="math inline">\(x\)</span> such that <span class="math inline">\(k(x,x_1) = 0.9\)</span>, and <span class="math inline">\(k(x,x)=1\)</span>, meaning that the value of <span class="math inline">\(f(x)\)</span> is moderately correlated with the value of <span class="math inline">\(f(x_1)\)</span>.</li>
<li>In the joint distribution, the contours of constant probability will be relatively narrow ellipses.</li>
</ul>
</section>
<section id="introduction-to-gaussian-processes-16" class="slide level2">
<h2>Introduction to Gaussian Processes 16</h2>
<ul>
<li>Suppose we observe <span class="math inline">\(f(x_1) = 1.2\)</span>. To condition on this value of <span class="math inline">\(f(x_1)\)</span>, we can draw a horizontal line at <span class="math inline">\(1.2\)</span> on our plot of the density, and see that the value of <span class="math inline">\(f(x)\)</span> is mostly constrained to <span class="math inline">\([0.64,1.52]\)</span>.</li>
<li>We have also drawn this plot in function space, showing the observed point <span class="math inline">\(f(x_1)\)</span> in orange, and 1 standard deviation of the Gaussian process predictive distribution for <span class="math inline">\(f(x)\)</span> in blue, about the mean value of <span class="math inline">\(1.08\)</span>.</li>
</ul>
<p><img data-src="https://user-images.githubusercontent.com/6753639/206867364-b4707db5-0c2e-4ae4-a412-8292bca4d08d.svg" style="width:35.0%" alt="Contours of constant probability of a bivariate Gaussian density over f(x_1) and f(x) with k(x,x_1) = 0.9."> <img data-src="https://user-images.githubusercontent.com/6753639/206867367-3815720c-93c8-4b4b-80e7-296db1d3553b.svg" style="width:35.0%" alt="Gaussian process predictive distribution in function space at f(x), with k(x,x_1) = 0.9."></p>
</section>
<section id="introduction-to-gaussian-processes-17" class="slide level2">
<h2>Introduction to Gaussian Processes 17</h2>
<ul>
<li>suppose we have a stronger correlation, <span class="math inline">\(k(x,x_1) = 0.95\)</span>.</li>
<li>the ellipses have narrowed further, and the value of <span class="math inline">\(f(x)\)</span> is even more strongly determined by <span class="math inline">\(f(x_1)\)</span>.</li>
<li>Drawing a horizontal line at <span class="math inline">\(1.2\)</span>, we see the contours for <span class="math inline">\(f(x)\)</span> support values mostly within <span class="math inline">\([0.83, 1.45]\)</span>.</li>
<li>show the plot in function space, with one standard deviation about the mean predictive value of <span class="math inline">\(1.14\)</span>.</li>
</ul>
<p><img data-src="https://user-images.githubusercontent.com/6753639/206867797-20e42783-31de-4c50-8103-e9441ba6d0a9.svg" style="width:35.0%" alt="Contours of constant probability of a bivariate Gaussian density over f(x_1) and f(x) with k(x,x_1) = 0.95."> <img data-src="https://user-images.githubusercontent.com/6753639/206867800-d9fc7add-649d-492c-8848-cab07c8fb83e.svg" style="width:35.0%" alt="Gaussian process predictive distribution in function space at f(x), with k(x,x_1) = 0.95."></p>
</section>
<section id="introduction-to-gaussian-processes-18" class="slide level2">
<h2>Introduction to Gaussian Processes 18</h2>
<p><img data-src="https://user-images.githubusercontent.com/6753639/206867797-20e42783-31de-4c50-8103-e9441ba6d0a9.svg" style="width:45.0%" alt="Contours of constant probability of a bivariate Gaussian density over f(x_1) and f(x) with k(x,x_1) = 0.95."> <img data-src="https://user-images.githubusercontent.com/6753639/206867800-d9fc7add-649d-492c-8848-cab07c8fb83e.svg" style="width:45.0%" alt="Gaussian process predictive distribution in function space at f(x), with k(x,x_1) = 0.95."></p>
<ul>
<li>posterior mean predictor of our Gaussian process is closer to <span class="math inline">\(1.2\)</span>, because there is a stronger correlation.</li>
<li>also uncertainty (the error bars) have somewhat decreased.</li>
<li>Despite strong correlation between function values, uncertainty still quite large, because we have only observed a single data point!</li>
</ul>
</section>
<section id="introduction-to-gaussian-processes-19" class="slide level2">
<h2>Introduction to Gaussian Processes 19</h2>
<ul>
<li><p>This procedure can give us a posterior on <span class="math inline">\(f(x)\)</span> for any <span class="math inline">\(x\)</span>, for any number of points we have observed.</p></li>
<li><p>Suppose we observe <span class="math inline">\(f(x_1), f(x_2)\)</span>.</p></li>
<li><p>visualize the posterior for <span class="math inline">\(f(x)\)</span> at a particular <span class="math inline">\(x=x'\)</span> in function space.</p></li>
<li><p>exact distribution for <span class="math inline">\(f(x)\)</span> is given by the above equations. <span class="math inline">\(f(x)\)</span> is Gaussian distributed, with mean</p></li>
</ul>
<p><span class="math display">\[m = k(x,x_{1:3}) k(x_{1:3},x_{1:3})^{-1} f(x_{1:3})\]</span></p>
<p>and variance</p>
<p><span class="math display">\[s^2 = k(x,x) - k(x,x_{1:3})k(x_{1:3},x_{1:3})^{-1}k(x,x_{1:3})\]</span></p>
<div class="fragment">
<ul>
<li><p>we have been considering <em>noise free</em> observations.</p></li>
<li><p>easy to include observation noise. If we assume that the data are generated from a latent noise free function <span class="math inline">\(f(x)\)</span> plus iid Gaussian noise <span class="math inline">\(\epsilon(x) \sim \mathcal{N}(0,\sigma^2)\)</span> with variance <span class="math inline">\(\sigma^2\)</span>, then our covariance function simply becomes <span class="math inline">\(k(x_i,x_j) \to k(x_i,x_j) + \delta_{ij}\sigma^2\)</span>, where <span class="math inline">\(\delta_{ij} = 1\)</span> if <span class="math inline">\(i=j\)</span> and <span class="math inline">\(0\)</span> otherwise.</p></li>
</ul>
</div>
</section>
<section id="summary-1" class="slide level2">
<h2>Summary 1</h2>
<ul>
<li>typical machine learning: we specify a function with some <strong>free parameters</strong> (such as a neural network and its weights), and we <strong>focus on estimating those parameters</strong>, which may not be interpretable.</li>
</ul>
<div class="fragment">
<ul>
<li>Gaussian process: <strong>reason about distributions over functions directly</strong>, which enables us to <strong>reason about the high-level properties of the solutions</strong>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>properties are controlled by a covariance function (kernel), which often has a few highly interpretable hyperparameters.</li>
<li>hyperparameters include the <strong>length-scale</strong>, which controls how rapidly (how wiggily) the functions are. Another hyperparameter is the <strong>amplitude</strong>, which controls the vertical scale over which our functions are varying.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>representing many different functions</strong> that can fit the data, and combining them all together into a predictive distribution, is a <strong>distinctive feature of Bayesian methods</strong>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>greater amount of variability between possible solutions far away from the data –&gt; uncertainty intuitively grows as we move from the data.</li>
</ul>
</div>
</section>
<section id="summary-2" class="slide level2">
<h2>Summary 2</h2>
<ul>
<li><strong>Gaussian process represents a distribution over functions by specifying a multivariate normal (Gaussian) distribution over all possible function values</strong>.</li>
</ul>
<div class="fragment">
<ul>
<li>possible to <strong>easily manipulate Gaussian distributions</strong> to find the distribution of one function value based on the values of any set of other values.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>observe a set of points</strong> –&gt; <strong>condition on these points</strong> and <strong>infer a distribution</strong> over what the value of the function might look like at any other input.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>How we model the correlations between these points is determined by the covariance function and is what defines the generalization properties of the Gaussian process.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>GPs easy to work with, have many applications, and help us understand and develop other model classes, like neural networks.</li>
</ul>
</div>
</section>
<section id="exercises" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>What is the difference between epistemic uncertainty versus observation uncertainty?</li>
<li>Besides rate of variation and amplitude, what other properties of functions might we want to consider, and what would be real-world examples of functions that have those properties?</li>
<li>The RBF covariance function we considered says that covariances (and correlations) between observations decrease with their distance in the input space (times, spatial locations, etc.). Is this a reasonable assumption? Why or why not?</li>
<li>Is a sum of two Gaussian variables Gaussian? Is a product of two Gaussian variables Gaussian? If (a,b) have a joint Gaussian distribution, is a|b (a given b) Gaussian? Is a Gaussian?</li>
<li>Repeat the exercise where we observe a data point at <span class="math inline">\(f(x_1) = 1.2\)</span>, but now suppose we additionally observe <span class="math inline">\(f(x_2) = 1.4\)</span>. Let <span class="math inline">\(k(x,x_1) = 0.9\)</span>, and <span class="math inline">\(k(x,x_2) = 0.8\)</span>. Will we be more or less certain about the value of <span class="math inline">\(f(x)\)</span>, than when we had only observed <span class="math inline">\(f(x_1)\)</span>? What is the mean and 95% credible set for our value of <span class="math inline">\(f(x)\)</span> now?</li>
<li>Do you think increasing our estimate of observation noise would increase or decrease our estimate of the length-scale of the ground truth function?</li>
<li>As we move away from the data, suppose the uncertainty in our predictive distribution increases to a point, then stops increasing. Why might that happen?</li>
</ol>
</section>
<section id="gaussian-process-priors" class="title-slide slide level1 center">
<h1>Gaussian Process Priors</h1>
<ul>
<li><p>Understanding GPs is important for reasoning about model construction and generalization, and for achieving state-of-the-art performance in a variety of applications, including active learning, and hyperparameter tuning in deep learning.</p></li>
<li><p>GPs are everywhere, and it is in our interests to know what they are and how we can use them.</p></li>
<li><p>this section: <strong>Gaussian process <em>priors</em></strong> over functions.</p></li>
</ul>
<div id="aa31afeb" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance_matrix</span>
<span id="cb1-3"><a></a><span class="im">import</span> d2l</span>
<span id="cb1-4"><a></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a></a></span>
<span id="cb1-6"><a></a>d2l.set_figsize()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>

<section id="definition-1" class="title-slide slide level1 center">
<h1>Definition 1</h1>
<ul>
<li><p>GP is defined as <strong><em>a collection of random variables, any finite number of which have a joint Gaussian distribution</em></strong>.</p></li>
<li><p>If a function <span class="math inline">\(f(x)\)</span> is a Gaussian process, with <em>mean function</em> <span class="math inline">\(m(x)\)</span> and <em>covariance function</em> or <em>kernel</em> <span class="math inline">\(k(x,x')\)</span>, <span class="math inline">\(f(x) \sim \mathcal{GP}(m, k)\)</span>,</p></li>
<li><p>–&gt; any collection of function values queried at any collection of input points <span class="math inline">\(x\)</span> (times, spatial locations, image pixels, etc.), has a joint multivariate Gaussian distribution with mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(K\)</span>: <span class="math inline">\(f(x_1),\dots,f(x_n) \sim \mathcal{N}(\mu, K)\)</span>, where <span class="math inline">\(\mu_i = E[f(x_i)] = m(x_i)\)</span> and <span class="math inline">\(K_{ij} = \mathrm{Cov}(f(x_i),f(x_j)) = k(x_i,x_j)\)</span>.</p></li>
</ul>
</section>

<section id="definition-2" class="title-slide slide level1 center">
<h1>Definition 2</h1>
<ul>
<li>Any function</li>
</ul>
<p><span class="math display">\[f(x) = w^{\top} \phi(x) = \langle w, \phi(x) \rangle,\]</span><code>(1)</code></p>
<p>with <span class="math inline">\(w\)</span> drawn from a Gaussian (normal) distribution, and <span class="math inline">\(\phi\)</span> being any vector of basis functions, for example <span class="math inline">\(\phi(x) = (1, x, x^2, ..., x^d)^{\top}\)</span>, is a Gaussian process.</p>
<ul>
<li>Moreover, any Gaussian process f(x) can be expressed in the form of equation <code>(1)</code>.</li>
</ul>
</section>

<section>
<section id="a-simple-gaussian-process-1" class="title-slide slide level1 center">
<h1>A Simple Gaussian Process 1</h1>
<ul>
<li><p>consider a few concrete examples</p></li>
<li><p>Suppose <span class="math inline">\(f(x) = w_0 + w_1 x\)</span>, and <span class="math inline">\(w_0, w_1 \sim \mathcal{N}(0,1)\)</span>, with <span class="math inline">\(w_0, w_1, x\)</span> all in one dimension.</p></li>
<li><p>can equivalently write this function as the inner product <span class="math inline">\(f(x) = (w_0, w_1)(1, x)^{\top}\)</span>. In <code>(1)</code> above, <span class="math inline">\(w = (w_0, w_1)^{\top}\)</span> and <span class="math inline">\(\phi(x) = (1,x)^{\top}\)</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>For any <span class="math inline">\(x\)</span>, <span class="math inline">\(f(x)\)</span> is a sum of two Gaussian random variables.</p></li>
<li><p>Gaussians are closed under addition –&gt; <span class="math inline">\(f(x)\)</span> is also a Gaussian random variable for any <span class="math inline">\(x\)</span>.</p></li>
<li><p>In fact, we can compute for any particular <span class="math inline">\(x\)</span> that <span class="math inline">\(f(x)\)</span> is <span class="math inline">\(\mathcal{N}(0,1+x^2)\)</span>.</p></li>
<li><p>Similarly, the joint distribution for any collection of function values, <span class="math inline">\((f(x_1),\dots,f(x_n))\)</span>, for any collection of inputs <span class="math inline">\(x_1,\dots,x_n\)</span>, is a multivariate Gaussian distribution. Therefore <span class="math inline">\(f(x)\)</span> is a Gaussian process.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(f(x)\)</span> is a <strong><em>random function</em></strong>, or a <strong><em>distribution over functions</em></strong>.</li>
<li>gain some insights into this distribution by repeatedly sampling values for <span class="math inline">\(w_0, w_1\)</span>, and visualizing the corresponding functions <span class="math inline">\(f(x)\)</span>, which are straight lines with slopes and different intercepts, as follows:</li>
</ul>
</div>
</section>
<section id="a-simple-gaussian-process-2" class="slide level2">
<h2>A Simple Gaussian Process 2</h2>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="kw">def</span> lin_func(x, n_sample):</span>
<span id="cb2-2"><a></a>    preds <span class="op">=</span> np.zeros((n_sample, x.shape[<span class="dv">0</span>]))</span>
<span id="cb2-3"><a></a>    <span class="cf">for</span> ii <span class="kw">in</span> <span class="bu">range</span>(n_sample):</span>
<span id="cb2-4"><a></a>        w <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb2-5"><a></a>        y <span class="op">=</span> w[<span class="dv">0</span>] <span class="op">+</span> w[<span class="dv">1</span>] <span class="op">*</span> x</span>
<span id="cb2-6"><a></a>        preds[ii, :] <span class="op">=</span> y</span>
<span id="cb2-7"><a></a>    <span class="cf">return</span> preds</span>
<span id="cb2-8"><a></a></span>
<span id="cb2-9"><a></a>x_points <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">50</span>)</span>
<span id="cb2-10"><a></a>outs <span class="op">=</span> lin_func(x_points, <span class="dv">10</span>)</span>
<span id="cb2-11"><a></a>lw_bd <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="op">*</span> np.sqrt((<span class="dv">1</span> <span class="op">+</span> x_points <span class="op">**</span> <span class="dv">2</span>))</span>
<span id="cb2-12"><a></a>up_bd <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt((<span class="dv">1</span> <span class="op">+</span> x_points <span class="op">**</span> <span class="dv">2</span>))</span>
<span id="cb2-13"><a></a></span>
<span id="cb2-14"><a></a>d2l.set_figsize((<span class="dv">12</span>,<span class="dv">5</span>))</span>
<span id="cb2-15"><a></a>d2l.plt.fill_between(x_points, lw_bd, up_bd, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb2-16"><a></a>d2l.plt.plot(x_points, np.zeros(<span class="bu">len</span>(x_points)), linewidth<span class="op">=</span><span class="dv">4</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb2-17"><a></a>d2l.plt.plot(x_points, outs.T)</span>
<span id="cb2-18"><a></a>d2l.plt.xlabel(<span class="st">"x"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb2-19"><a></a>d2l.plt.ylabel(<span class="st">"f(x)"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb2-20"><a></a>d2l.plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div id="02474545" class="cell quarto-layout-cell" data-execution_count="2" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="template_files/figure-revealjs/cell-3-output-1.svg"></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>If <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> are instead drawn from <span class="math inline">\(\mathcal{N}(0,\alpha^2)\)</span>, how do you imagine varying <span class="math inline">\(\alpha\)</span> affects the distribution over functions?</li>
</ul>
</div>
</div>
</div>
</section>
<section id="from-weight-space-to-function-space-1" class="slide level2">
<h2>From Weight Space to Function Space 1</h2>
<ul>
<li><p>we saw how a <strong>distribution over parameters in a modelinduces a distribution over functions</strong>.</p></li>
<li><p><strong>often have ideas about the functions we want to model</strong> — whether they’re smooth, periodic, quickly varying, etc. — <strong>relatively tedious to reason about the parameters</strong>, which are largely uninterpretable.</p></li>
<li><p>GPs provide an <strong>easy mechanism</strong> to <strong>reason <em>directly</em></strong> about functions.</p></li>
<li><p>Gaussian distribution is entirely defined by its first two moments, its mean and covariance matrix, a Gaussian process by extension is defined by its mean function and covariance function.</p></li>
</ul>
<p>In the above example, the mean function</p>
<p><span class="math display">\[m(x) = E[f(x)] = E[w_0 + w_1x] = E[w_0] + E[w_1]x = 0+0 = 0.\]</span></p>
<p>Similarly, the covariance function is</p>
<p><span class="math display">\[k(x,x') = \mathrm{Cov}(f(x),f(x')) = E[f(x)f(x')]-E[f(x)]E[f(x')] = \\ E[w_0^2 + w_0w_1x' + w_1w_0x + w_1^2xx'] = 1 + xx'.\]</span></p>
</section>
<section id="from-weight-space-to-function-space-2" class="slide level2">
<h2>From Weight Space to Function Space 2</h2>
<ul>
<li><p>distribution over functions can now be directly specified and sampled from, without needing to sample from the distribution over parameters.</p></li>
<li><p>For example, to draw from <span class="math inline">\(f(x)\)</span>, we can simply form our multivariate Gaussian distribution associated with any collection of <span class="math inline">\(x\)</span> we want to query, and sample from it directly.</p></li>
<li><p><strong>very advantageous</strong></p></li>
<li><p>same derivation for the simple straight line model above can be applied to find the mean and covariance function for <em>any</em> model of the form <span class="math inline">\(f(x) = w^{\top} \phi(x)\)</span>, with <span class="math inline">\(w \sim \mathcal{N}(u,S)\)</span>.</p></li>
<li><p>In this case, the mean function <span class="math inline">\(m(x) = u^{\top}\phi(x)\)</span>, and the covariance function <span class="math inline">\(k(x,x') = \phi(x)^{\top}S\phi(x')\)</span>. Since <span class="math inline">\(\phi(x)\)</span> can represent a vector of any non-linear basis functions, we are considering a very general model class, including models with an even an <em>infinite</em> number of parameters.</p></li>
</ul>
</section>
<section id="the-radial-basis-function-rbf-kernel-1" class="slide level2">
<h2>The Radial Basis Function (RBF) Kernel 1</h2>
<ul>
<li><em>radial basis function</em> (RBF) kernel is the most popular covariance function for Gaussian processes</li>
<li>kernel has the form <span class="math inline">\(k_{\text{RBF}}(x,x') = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right)\)</span>, where <span class="math inline">\(a\)</span> is an amplitude parameter, and <span class="math inline">\(\ell\)</span> is a <em>lengthscale</em> hyperparameter.</li>
</ul>
<p>Let’s derive this kernel starting from weight space. Consider the function</p>
<p><span class="math display">\[f(x) = \sum_{i=1}^J w_i \phi_i(x), w_i  \sim \mathcal{N}\left(0,\frac{\sigma^2}{J}\right), \phi_i(x) = \exp\left(-\frac{(x-c_i)^2}{2\ell^2 }\right).\]</span></p>
<p><span class="math inline">\(f(x)\)</span> is a sum of radial basis functions, with width <span class="math inline">\(\ell\)</span>, centred at the points <span class="math inline">\(c_i\)</span>, as shown in the following figure.</p>
<ul>
<li>We can recognize <span class="math inline">\(f(x)\)</span> as having the form <span class="math inline">\(w^{\top} \phi(x)\)</span>, where <span class="math inline">\(w = (w_1,\dots,w_J)^{\top}\)</span> and <span class="math inline">\(\phi(x)\)</span> is a vector containing each of the radial basis functions. The covariance function of this Gaussian process is then</li>
</ul>
<p><span class="math display">\[k(x,x') = \frac{\sigma^2}{J} \sum_{i=1}^{J} \phi_i(x)\phi_i(x').\]</span></p>
</section>
<section id="the-radial-basis-function-rbf-kernel-2" class="slide level2">
<h2>The Radial Basis Function (RBF) Kernel 2</h2>
<ul>
<li>what happens as we take the number of parameters (and basis functions) to infinity. Let <span class="math inline">\(c_J = \log J\)</span>, <span class="math inline">\(c_1 = -\log J\)</span>, and <span class="math inline">\(c_{i+1}-c_{i} = \Delta c = 2\frac{\log J}{J}\)</span>, and <span class="math inline">\(J \to \infty\)</span>. The covariance function becomes the Riemann sum:</li>
</ul>
<p><span class="math display">\[k(x,x') = \lim_{J \to \infty} \frac{\sigma^2}{J} \sum_{i=1}^{J} \phi_i(x)\phi_i(x') = \int_{c_0}^{c_\infty} \phi_c(x)\phi_c(x') dc.\]</span></p>
<p>By setting <span class="math inline">\(c_0 = -\infty\)</span> and <span class="math inline">\(c_\infty = \infty\)</span>, we spread the infinitely many basis functions across the whole real line, each a distance <span class="math inline">\(\Delta c \to 0\)</span> apart:</p>
<p><span class="math display">\[k(x,x') = \int_{-\infty}^{\infty} \exp(-\frac{(x-c)^2}{2\ell^2}) \exp(-\frac{(x'-c)^2}{2\ell^2 }) dc = \sqrt{\pi}\ell \sigma^2 \exp(-\frac{(x-x')^2}{2(\sqrt{2} \ell)^2}) \propto k_{\text{RBF}}(x,x').\]</span></p>
<ul>
<li><p>By moving into the function space representation, we have derived how to represent a model with an <strong><em>infinite</em> number of parameters, using a finite amount of computation</strong>.</p></li>
<li><p><strong>GP with an RBF kernel is a <em>universal approximator</em></strong>, capable of representing any continuous function to arbitrary precision.</p></li>
</ul>
</section>
<section id="the-radial-basis-function-rbf-kernel-3" class="slide level2">
<h2>The Radial Basis Function (RBF) Kernel 3</h2>
<ul>
<li><p>We can intuitively see why from the above derivation.</p></li>
<li><p>We can collapse each radial basis function to a point mass taking <span class="math inline">\(\ell \to 0\)</span>, and give each point mass any height we wish.</p></li>
<li><p><strong>GP with an RBF kernel is a model with an infinite number of parameters and much more flexibility than any finite neural network</strong></p></li>
<li><p><strong>all the fuss about <em>overparametrized</em> neural networks is misplaced?</strong></p></li>
<li><p><strong>GPs with RBF kernels do not overfit</strong>, and in fact provide especially compelling generalization performance on small datasets.</p></li>
<li><p>examples in <a href="https://dl.acm.org/doi/abs/10.1145/3446776">Zhang 2021</a>, such as the ability to fit images with random labels perfectly, but still generalize well on structured problems, (can be perfectly reproduced using Gaussian processes) <a href="https://arxiv.org/abs/2002.08791">Wilson 2020</a>.</p></li>
<li><p>Neural networks are not as distinct as we make them out to be.</p></li>
</ul>
</section>
<section id="the-radial-basis-function-rbf-kernel-4" class="slide level2">
<h2>The Radial Basis Function (RBF) Kernel 4</h2>
<ul>
<li><p><strong>build further intuition about GPs with RBF kernels, and hyperparameters such as <em>length-scale</em>, by sampling directly from the distribution over functions</strong>.</p></li>
<li><p>simple procedure:</p></li>
</ul>
<ol type="1">
<li>Choose the input <span class="math inline">\(x\)</span> points we want to query the GP: <span class="math inline">\(x_1,\dots,x_n\)</span>.</li>
<li>Evaluate <span class="math inline">\(m(x_i)\)</span>, <span class="math inline">\(i = 1,\dots,n\)</span>, and <span class="math inline">\(k(x_i,x_j)\)</span> for <span class="math inline">\(i,j = 1,\dots,n\)</span> to respectively form the mean vector and covariance matrix <span class="math inline">\(\mu\)</span> and <span class="math inline">\(K\)</span>, where <span class="math inline">\((f(x_1),\dots,f(x_n)) \sim \mathcal{N}(\mu, K)\)</span>.</li>
<li>Sample from this multivariate Gaussian distribution to obtain the sample function values.</li>
<li>Sample more times to visualize more sample functions queried at those points.</li>
</ol>
<p>We illustrate this process in the figure below.</p>
</section>
<section id="the-radial-basis-function-rbf-kernel-5" class="slide level2">
<h2>The Radial Basis Function (RBF) Kernel 5</h2>
<div id="f74be105" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="kw">def</span> rbfkernel(x1, x2, ls<span class="op">=</span><span class="fl">4.</span>):  <span class="co">#@save</span></span>
<span id="cb3-2"><a></a>    dist <span class="op">=</span> distance_matrix(np.expand_dims(x1, <span class="dv">1</span>), np.expand_dims(x2, <span class="dv">1</span>))</span>
<span id="cb3-3"><a></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>(<span class="fl">1.</span> <span class="op">/</span> ls <span class="op">/</span> <span class="dv">2</span>) <span class="op">*</span> (dist <span class="op">**</span> <span class="dv">2</span>))</span>
<span id="cb3-4"><a></a></span>
<span id="cb3-5"><a></a>x_points <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">50</span>)</span>
<span id="cb3-6"><a></a>meanvec <span class="op">=</span> np.zeros(<span class="bu">len</span>(x_points))</span>
<span id="cb3-7"><a></a>covmat <span class="op">=</span> rbfkernel(x_points,x_points, <span class="dv">1</span>)</span>
<span id="cb3-8"><a></a></span>
<span id="cb3-9"><a></a>prior_samples<span class="op">=</span> np.random.multivariate_normal(meanvec, covmat, size<span class="op">=</span><span class="dv">5</span>)<span class="op">;</span></span>
<span id="cb3-10"><a></a>d2l.plt.plot(x_points, prior_samples.T, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-11"><a></a>d2l.plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="template_files/figure-revealjs/cell-4-output-1.svg" class="r-stretch"></section>
<section id="the-neural-network-kernel-1" class="slide level2">
<h2>The Neural Network Kernel 1</h2>
<ul>
<li>Research on Gaussian processes in machine learning was triggered by research on neural networks.</li>
<li>We can derive the neural network kernel as follows.</li>
</ul>
<p>Consider a neural network function <span class="math inline">\(f(x)\)</span> with one hidden layer:</p>
<p><span class="math display">\[f(x) = b + \sum_{i=1}^{J} v_i h(x; u_i).\]</span></p>
<p><span class="math inline">\(b\)</span> is a bias, <span class="math inline">\(v_i\)</span> are the hidden to output weights, <span class="math inline">\(h\)</span> is any bounded hidden unit transfer function, <span class="math inline">\(u_i\)</span> are the input to hidden weights, and <span class="math inline">\(J\)</span> is the number of hidden units.</p>
<ul>
<li><p>Let <span class="math inline">\(b\)</span> and <span class="math inline">\(v_i\)</span> be independent with zero mean and variances <span class="math inline">\(\sigma_b^2\)</span> and <span class="math inline">\(\sigma_v^2/J\)</span>, respectively, and let the <span class="math inline">\(u_i\)</span> have independent identical distributions.</p></li>
<li><p>use the central limit theorem to show that any collection of function values <span class="math inline">\(f(x_1),\dots,f(x_n)\)</span> has a joint multivariate Gaussian distribution.</p></li>
</ul>
</section>
<section id="the-neural-network-kernel-2" class="slide level2">
<h2>The Neural Network Kernel 2</h2>
<p>The mean and covariance function of the corresponding Gaussian process are:</p>
<p><span class="math display">\[m(x) = E[f(x)] = 0\]</span></p>
<p><span class="math display">\[k(x,x') = \text{cov}[f(x),f(x')] = E[f(x)f(x')] = \sigma_b^2 + \frac{1}{J} \sum_{i=1}^{J} \sigma_v^2 E[h_i(x; u_i)h_i(x'; u_i)]\]</span></p>
<p>In some cases, we can essentially evaluate this covariance function in closed form. Let <span class="math inline">\(h(x; u) = \text{erf}(u_0 + \sum_{j=1}^{P} u_j x_j)\)</span>, where <span class="math inline">\(\text{erf}(z) = \frac{2}{\sqrt{\pi}} \int_{0}^{z} e^{-t^2} dt\)</span>, and <span class="math inline">\(u \sim \mathcal{N}(0,\Sigma)\)</span>. Then <span class="math inline">\(k(x,x') = \frac{2}{\pi} \text{sin}(\frac{2 \tilde{x}^{\top} \Sigma \tilde{x}'}{\sqrt{(1 + 2 \tilde{x}^{\top} \Sigma \tilde{x})(1 + 2 \tilde{x}'^{\top} \Sigma \tilde{x}')}})\)</span>.</p>
<ul>
<li>RBF kernel is <em>stationary</em>, meaning that it is <em>translation invariant</em>, and therefore can be written as a function of <span class="math inline">\(\tau = x-x'\)</span>.</li>
<li>Intuitively, stationarity means that the high-level properties of the function, such as rate of variation, do not change as we move in input space.</li>
<li>The neural network kernel, however, is <em>non-stationary</em>.</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li><p><strong>first step in performing Bayesian inference involves specifying a prior</strong></p></li>
<li><p>GPs can be used to specify a whole <strong>prior over functions</strong>.</p></li>
<li><p>Starting from a traditional “weight space” view of modelling, <strong>induce a prior over functions by starting with the functional form of a model, and introducing a distribution over its parameters</strong>.</p></li>
<li><p><strong>alternatively specify a prior distribution directly in function space</strong>, with properties controlled by a kernel.</p></li>
<li><p><strong>function-space approach has many advantages</strong>. We can build models that actually correspond to an infinite number of parameters, but use a finite amount of computation!</p></li>
<li><p>models have a <strong>great amount of flexibility</strong>, but <strong>also make strong assumptions</strong> about what types of functions are a priori likely, leading to relatively <strong>good generalization on small datasets</strong>.</p></li>
<li><p>assumptions of models in function space controlled by kernels: encode higher level properties of functions, such as smoothness and periodicity</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><p>Many kernels are <strong>stationary</strong>: they are <strong>translation invariant</strong>.</p></li>
<li><p>Functions drawn from GP with a stationary kernel have roughly the same high-level properties regardless of where we look in the input space.</p></li>
<li><p>GPs a <strong>relatively general model class</strong> including polynomials, Fourier series, and so on, <strong>as long as we have a Gaussian prior over the parameters</strong>.</p></li>
<li><p><strong>also include neural networks with an infinite number of parameters</strong>, even without Gaussian distributions over the parameters.</p></li>
</ul>
</section></section>
<section>
<section id="gaussian-process-inference" class="title-slide slide level1 center">
<h1>Gaussian Process Inference</h1>
<ul>
<li>Posterior inference &amp; predictions using GP priors</li>
<li>Focus on regression with <strong>closed-form inference</strong></li>
<li>Implementation from scratch</li>
<li>Introduction to <a href="https://gpytorch.ai/">GPyTorch</a> for SOTA GPs</li>
<li>Advanced topics (approximate inference) in next section</li>
</ul>
</section>
<section id="posterior-inference-for-regression-1" class="slide level2">
<h2>Posterior Inference for Regression 1</h2>
<ul>
<li><p><strong>Observation model</strong>: relates <span class="math inline">\(f(x)\)</span> to observations <span class="math inline">\(y(x)\)</span></p>
<ul>
<li><span class="math inline">\(x\)</span>: input (e.g., image pixels)</li>
<li><span class="math inline">\(y\)</span>: output (e.g., class label, temperature, concentration)</li>
</ul></li>
<li><p>Regression model: <span class="math display">\[y(x) = f(x) + \epsilon(x), \quad \epsilon(x) \sim \mathcal{N}(0,\sigma^2)\]</span></p></li>
<li><p>Notation:</p>
<ul>
<li><span class="math inline">\(\mathbf{y} = (y(x_1),\dots,y(x_n))^{\top}\)</span>: training observations</li>
<li><span class="math inline">\(\textbf{f} = (f(x_1),\dots,f(x_n))^{\top}\)</span>: latent function values</li>
<li><span class="math inline">\(X = \{x_1, \dots, x_n\}\)</span>: training inputs</li>
</ul></li>
</ul>
</section>
<section id="posterior-inference-for-regression-2" class="slide level2">
<h2>Posterior Inference for Regression 2</h2>
<ul>
<li>GP prior: <span class="math inline">\(f(x) \sim \mathcal{GP}(m,k)\)</span>
<ul>
<li>Mean vector: <span class="math inline">\(\mu_i = m(x_i)\)</span></li>
<li>Covariance matrix: <span class="math inline">\(K_{ij} = k(x_i,x_j)\)</span></li>
</ul></li>
<li>Standard choices:
<ul>
<li>RBF kernel: <span class="math inline">\(k(x_i,x_j) = a^2 \exp\left(-\frac{1}{2\ell^2}||x_i-x_j||^2\right)\)</span></li>
<li>Mean function: <span class="math inline">\(m(x)=0\)</span> (for simplicity)</li>
</ul></li>
<li>Goal: Predict at test inputs <span class="math inline">\(X_* = \{x_{*1},x_{*2},\dots,x_{*m}\}\)</span>
<ul>
<li>Find <span class="math inline">\(p(\mathbf{f}_* | \mathbf{y}, X)\)</span></li>
<li>Use Gaussian identities for joint distribution</li>
</ul></li>
</ul>
</section>
<section id="posterior-inference-for-regression-3" class="slide level2">
<h2>Posterior Inference for Regression 3</h2>
<ul>
<li>Training data: <span class="math inline">\(\mathbf{y} = \mathbf{f} + \mathbf{\epsilon}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{f} \sim \mathcal{N}(0,K(X,X))\)</span></li>
<li><span class="math inline">\(\mathbf{\epsilon} \sim \mathcal{N}(0,\sigma^2I)\)</span></li>
<li><span class="math inline">\(\mathbf{y} \sim \mathcal{N}(0, K(X,X) + \sigma^2I)\)</span></li>
</ul></li>
<li>Covariance structure:
<ul>
<li><span class="math inline">\(\mathrm{cov}(\mathbf{f}_*, \mathbf{y}) = K(X_*,X)\)</span></li>
<li>Joint distribution: <span class="math display">\[
\begin{bmatrix}
\mathbf{y} \\
\mathbf{f}_*
\end{bmatrix}
\sim
\mathcal{N}\left(0,
\begin{bmatrix}
K(X,X)+\sigma^2I &amp; K(X,X_*) \\
K(X_*,X) &amp; K(X_*,X_*)
\end{bmatrix}
\right)
\]</span></li>
</ul></li>
</ul>
</section>
<section id="posterior-inference-for-regression-4" class="slide level2">
<h2>Posterior Inference for Regression 4</h2>
<ul>
<li>Kernel parameters <span class="math inline">\(\theta\)</span> (e.g., <span class="math inline">\(a\)</span>, <span class="math inline">\(\ell\)</span> in RBF)</li>
<li>Use marginal likelihood <span class="math inline">\(p(\textbf{y} | \theta, X)\)</span> for learning</li>
<li>Marginal likelihood properties:
<ul>
<li>Balances model fit and complexity</li>
<li>Implements Occam’s razor</li>
<li>See <a href="https://books.google.de/books?id=AKuMj4PN_EMC">MacKay Ch. 28</a> and <a href="https://gaussianprocess.org/gpml/chapters/RW.pdf">Rasmussen and Williams Ch. 5</a></li>
</ul></li>
</ul>
</section>
<section id="equations-for-making-predictions-and-learning-kernel-hyperparameters-in-gp-regression" class="slide level2">
<h2>Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression</h2>
<ul>
<li><p>Two-step procedure:</p>
<ol type="1">
<li>Learn <span class="math inline">\(\hat{\theta}\)</span> via marginal likelihood maximization</li>
<li>Use predictive mean &amp; 2×std for 95% credible set</li>
</ol></li>
<li><p>Log marginal likelihood: <span class="math display">\[\log p(\textbf{y} | \theta, X) = -\frac{1}{2}\textbf{y}^{\top}[K_{\theta}(X,X) + \sigma^2I]^{-1}\textbf{y} - \frac{1}{2}\log|K_{\theta}(X,X)| + c\]</span></p></li>
<li><p>Predictive distribution: <span class="math display">\[p(y_* | x_*, \textbf{y}, \theta) = \mathcal{N}(a_*,v_*)\]</span> <span class="math display">\[a_* = k_{\theta}(x_*,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}(\textbf{y}-\mu) + \mu\]</span> <span class="math display">\[v_* = k_{\theta}(x_*,x_*) - K_{\theta}(x_*,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}k_{\theta}(X,x_*)\]</span></p></li>
</ul>
</section>
<section id="interpreting-equations-for-learning-and-predictions-1" class="slide level2">
<h2>Interpreting Equations for Learning and Predictions 1</h2>
<ul>
<li>Key advantages:
<ul>
<li><strong>Exact</strong> Bayesian inference in <strong>closed form</strong></li>
<li>No training beyond hyperparameter learning</li>
<li>Explicit predictive equations</li>
<li>Exceptional convenience &amp; versatility</li>
</ul></li>
<li>Predictive mean:
<ul>
<li>Linear combination of training targets</li>
<li>Weights determined by kernel</li>
<li>Kernel crucial for generalization</li>
</ul></li>
<li>Predictive variance:
<ul>
<li>Independent of target values</li>
<li>Grows with distance from training points</li>
<li>Implicitly depends on <span class="math inline">\(\theta\)</span> learned from data</li>
</ul></li>
</ul>
</section>
<section id="interpreting-equations-for-learning-and-predictions-2" class="slide level2">
<h2>Interpreting Equations for Learning and Predictions 2</h2>
<ul>
<li>Computational considerations:
<ul>
<li>Bottleneck: <span class="math inline">\(n \times n\)</span> matrix operations</li>
<li>Naive complexity: <span class="math inline">\(\mathcal{O}(n^3)\)</span> compute, <span class="math inline">\(\mathcal{O}(n^2)\)</span> storage</li>
<li>Historical limit: ~10,000 points</li>
<li>Modern scaling: millions of points possible</li>
</ul></li>
<li>Numerical stability:
<ul>
<li><span class="math inline">\(K(X,X)\)</span> often near-singular</li>
<li><span class="math inline">\(\sigma^2I\)</span> term improves conditioning</li>
<li>“Jitter” (<span class="math inline">\(\sim 10^{-6}\)</span>) for noise-free cases</li>
</ul></li>
</ul>
</section>
<section id="worked-example-gp-regression-from-scratch" class="slide level2">
<h2>Worked Example: GP Regression from Scratch</h2>
<h3 id="data-generation">1. Data Generation</h3>
<ul>
<li>True function: <span class="math inline">\(f(x) = \sin(x) + \frac{1}{2}\sin(4x)\)</span></li>
<li>Observations: <span class="math inline">\(y(x) = f(x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0,0.25^2)\)</span></li>
<li>Goal: Recover <span class="math inline">\(f(x)\)</span> from noisy observations</li>
</ul>

<img data-src="./img/output_gp-inference_714770_3_0.svg" style="width:100.0%" class="r-stretch"></section>
<section id="gp-prior-specification" class="slide level2">
<h2>2. GP Prior Specification</h2>
<ul>
<li>Mean function: <span class="math inline">\(m(x) = 0\)</span></li>
<li>Kernel: RBF <span class="math display">\[k(x_i,x_j) = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right)\]</span></li>
<li>Initial length-scale: 0.2</li>
</ul>
<div id="cell-setup1" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a></a><span class="im">import</span> d2l</span>
<span id="cb4-3"><a></a><span class="im">import</span> torch </span>
<span id="cb4-4"><a></a><span class="im">import</span> gpytorch</span>
<span id="cb4-5"><a></a><span class="im">import</span> math</span>
<span id="cb4-6"><a></a><span class="im">import</span> os </span>
<span id="cb4-7"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb4-8"><a></a><span class="im">import</span> torch</span>
<span id="cb4-9"><a></a><span class="im">from</span> scipy <span class="im">import</span> optimize</span>
<span id="cb4-10"><a></a> </span>
<span id="cb4-11"><a></a></span>
<span id="cb4-12"><a></a>d2l.set_figsize()</span>
<span id="cb4-13"><a></a><span class="kw">def</span> data_maker1(x, sig):</span>
<span id="cb4-14"><a></a>    <span class="cf">return</span> np.sin(x) <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x) <span class="op">+</span> np.random.randn(x.shape[<span class="dv">0</span>]) <span class="op">*</span> sig</span>
<span id="cb4-15"><a></a></span>
<span id="cb4-16"><a></a>sig <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb4-17"><a></a>train_x, test_x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">50</span>), np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">500</span>)</span>
<span id="cb4-18"><a></a>train_y, test_y <span class="op">=</span> data_maker1(train_x, sig<span class="op">=</span>sig), data_maker1(test_x, sig<span class="op">=</span><span class="fl">0.</span>)</span>
<span id="cb4-19"><a></a></span>
<span id="cb4-20"><a></a>d2l.plt.scatter(train_x, train_y)</span>
<span id="cb4-21"><a></a>d2l.plt.plot(test_x, test_y)</span>
<span id="cb4-22"><a></a>d2l.plt.xlabel(<span class="st">"x"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb4-23"><a></a>d2l.plt.ylabel(<span class="st">"Observations y"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb4-24"><a></a>d2l.plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="template_files/figure-revealjs/setup1-output-1.svg" id="setup1" class="r-stretch"></section>
<section class="slide level2">

<div id="6bd07550" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a>mean <span class="op">=</span> np.zeros(test_x.shape[<span class="dv">0</span>])</span>
<span id="cb5-2"><a></a>cov <span class="op">=</span> d2l.rbfkernel(test_x, test_x, ls<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Visualize prior:
<ul>
<li>Sample functions</li>
<li>95% credible set</li>
<li>Assess reasonableness</li>
</ul></li>
</ul>
<div id="948f9c8e" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>prior_samples <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>mean, cov<span class="op">=</span>cov, size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb6-2"><a></a>d2l.plt.plot(test_x, prior_samples.T, color<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb6-3"><a></a>d2l.plt.plot(test_x, mean, linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb6-4"><a></a>d2l.plt.fill_between(test_x, mean <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.diag(cov), mean <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.diag(cov), </span>
<span id="cb6-5"><a></a>                 alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb6-6"><a></a>d2l.plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="template_files/figure-revealjs/cell-7-output-1.svg" class="r-stretch"></section>
<section id="hyperparameter-learning" class="slide level2">
<h2>3. Hyperparameter Learning</h2>
<ul>
<li>Initial values:
<ul>
<li>Length-scale: 0.4</li>
<li>Noise std: 0.75</li>
</ul></li>
<li>Optimize via marginal likelihood: <span class="math display">\[\log p(y | X) = -\frac{1}{2}y^T(K + \sigma^2 I)^{-1}y - \frac{1}{2}\log |K + \sigma^2 I| - \frac{n}{2}\log 2\pi\]</span></li>
</ul>
<div id="6c4344f0" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>ell_est <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb7-2"><a></a>post_sig_est <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb7-3"><a></a></span>
<span id="cb7-4"><a></a><span class="kw">def</span> neg_MLL(pars):</span>
<span id="cb7-5"><a></a>    K <span class="op">=</span> d2l.rbfkernel(train_x, train_x, ls<span class="op">=</span>pars[<span class="dv">0</span>])</span>
<span id="cb7-6"><a></a>    kernel_term <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> train_y <span class="op">@</span> <span class="op">\</span></span>
<span id="cb7-7"><a></a>        np.linalg.inv(K <span class="op">+</span> pars[<span class="dv">1</span>] <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> np.eye(train_x.shape[<span class="dv">0</span>])) <span class="op">@</span> train_y</span>
<span id="cb7-8"><a></a>    logdet <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> np.log(np.linalg.det(K <span class="op">+</span> pars[<span class="dv">1</span>] <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb7-9"><a></a>                                         np.eye(train_x.shape[<span class="dv">0</span>])))</span>
<span id="cb7-10"><a></a>    const <span class="op">=</span> <span class="op">-</span>train_x.shape[<span class="dv">0</span>] <span class="op">/</span> <span class="fl">2.</span> <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb7-11"><a></a>    </span>
<span id="cb7-12"><a></a>    <span class="cf">return</span> <span class="op">-</span>(kernel_term <span class="op">+</span> logdet <span class="op">+</span> const)</span>
<span id="cb7-13"><a></a></span>
<span id="cb7-14"><a></a></span>
<span id="cb7-15"><a></a>learned_hypers <span class="op">=</span> optimize.minimize(neg_MLL, x0<span class="op">=</span>np.array([ell_est,post_sig_est]), </span>
<span id="cb7-16"><a></a>                                   bounds<span class="op">=</span>((<span class="fl">0.01</span>, <span class="fl">10.</span>), (<span class="fl">0.01</span>, <span class="fl">10.</span>)))</span>
<span id="cb7-17"><a></a>ell <span class="op">=</span> learned_hypers.x[<span class="dv">0</span>]</span>
<span id="cb7-18"><a></a>post_sig_est <span class="op">=</span> learned_hypers.x[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<ul>
<li>Learned parameters:
<ul>
<li>Length-scale: 0.299</li>
<li>Noise std: 0.24</li>
<li>Close to true noise → well-specified model</li>
</ul></li>
</ul>
</section>
<section id="posterior-inference" class="slide level2">
<h2>4. Posterior Inference</h2>
<ul>
<li>Predictive distribution:
<ul>
<li>Mean: <span class="math inline">\(\bar{f}_{*} = K(x, x_*)^T (K + \sigma^2 I)^{-1}y\)</span></li>
<li>Variance: <span class="math inline">\(V(f_{*}) = K(x_*, x_*) - K(x, x_*)^T (K + \sigma^2 I)^{-1}K(x, x_*)\)</span></li>
</ul></li>
</ul>
<div id="cb6a3141" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>K_x_xstar <span class="op">=</span> d2l.rbfkernel(train_x, test_x, ls<span class="op">=</span>ell)</span>
<span id="cb8-2"><a></a>K_x_x <span class="op">=</span> d2l.rbfkernel(train_x, train_x, ls<span class="op">=</span>ell)</span>
<span id="cb8-3"><a></a>K_xstar_xstar <span class="op">=</span> d2l.rbfkernel(test_x, test_x, ls<span class="op">=</span>ell)</span>
<span id="cb8-4"><a></a></span>
<span id="cb8-5"><a></a>post_mean <span class="op">=</span> K_x_xstar.T <span class="op">@</span> np.linalg.inv((K_x_x <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-6"><a></a>                post_sig_est <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> np.eye(train_x.shape[<span class="dv">0</span>]))) <span class="op">@</span> train_y</span>
<span id="cb8-7"><a></a>post_cov <span class="op">=</span> K_xstar_xstar <span class="op">-</span> K_x_xstar.T <span class="op">@</span> np.linalg.inv((K_x_x <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-8"><a></a>                post_sig_est <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> np.eye(train_x.shape[<span class="dv">0</span>]))) <span class="op">@</span> K_x_xstar</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="uncertainty-analysis" class="slide level2">
<h2>5. Uncertainty Analysis</h2>
<ul>
<li>Two types of uncertainty:
<ul>
<li><strong>Epistemic</strong> (reducible):
<ul>
<li>Uncertainty about true function</li>
<li>Captured by <code>np.diag(post_cov)</code></li>
<li>Grows away from data</li>
</ul></li>
<li><strong>Aleatoric</strong> (irreducible):
<ul>
<li>Observation noise</li>
<li>Captured by <code>post_sig_est**2</code></li>
</ul></li>
</ul></li>
<li>95% credible sets:
<ul>
<li><p>For true function:</p>
<div id="4b4389a5" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a>lw_bd <span class="op">=</span> post_mean <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(np.diag(post_cov))</span>
<span id="cb9-2"><a></a>up_bd <span class="op">=</span> post_mean <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(np.diag(post_cov))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
<li><p>For observations:</p>
<div id="47c63398" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>lw_bd_observed <span class="op">=</span> post_mean <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(np.diag(post_cov) <span class="op">+</span> post_sig_est <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb10-2"><a></a>up_bd_observed <span class="op">=</span> post_mean <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(np.diag(post_cov) <span class="op">+</span> post_sig_est <span class="op">**</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
</ul></li>
</ul>
</section>
<section id="posterior-samples" class="slide level2">
<h2>6. Posterior Samples</h2>
<ul>
<li>Visualize uncertainty:
<ul>
<li>20 posterior samples</li>
<li>Show function space consistent with data</li>
<li>Helps understand model fit</li>
</ul></li>
</ul>
<div id="92ad2836" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>post_samples <span class="op">=</span> np.random.multivariate_normal(post_mean, post_cov, size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb11-2"><a></a>d2l.plt.scatter(train_x, train_y)</span>
<span id="cb11-3"><a></a>d2l.plt.plot(test_x, test_y, linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb11-4"><a></a>d2l.plt.plot(test_x, post_mean, linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb11-5"><a></a>d2l.plt.plot(test_x, post_samples.T, color<span class="op">=</span><span class="st">'gray'</span>, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb11-6"><a></a>d2l.plt.fill_between(test_x, lw_bd, up_bd, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb11-7"><a></a>plt.legend([<span class="st">'Observed Data'</span>, <span class="st">'True Function'</span>, <span class="st">'Predictive Mean'</span>, <span class="st">'Posterior Samples'</span>])</span>
<span id="cb11-8"><a></a>d2l.plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="template_files/figure-revealjs/cell-12-output-1.svg" class="r-stretch"></section>
<section id="gpytorch-modern-gp-implementation" class="slide level2">
<h2>GPyTorch: Modern GP Implementation</h2>
<h3 class="smaller" id="why-gpytorch">Why GPyTorch?</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Advanced Features</strong> * Multiple kernel choices * Approximate inference * Neural network integration * Scalability (&gt;10k points) * Advanced methods (SKI/KISS-GP)</p>
</div><div class="column" style="width:50%;">
<p><strong>Implementation Benefits</strong> * No manual implementation * Efficient numerical routines * GPU acceleration * Modern PyTorch ecosystem</p>
</div></div>
<h3 class="smaller" id="model-definition">Model Definition</h3>
<ul>
<li><strong>Exact GP Implementation</strong>:
<ul>
<li>Zero mean function</li>
<li>RBF kernel</li>
<li>Gaussian likelihood</li>
</ul></li>
</ul>
<div id="bd6a6e7c" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a><span class="co"># Data preparation</span></span>
<span id="cb12-2"><a></a>train_x <span class="op">=</span> torch.tensor(train_x)</span>
<span id="cb12-3"><a></a>train_y <span class="op">=</span> torch.tensor(train_y)</span>
<span id="cb12-4"><a></a></span>
<span id="cb12-5"><a></a><span class="co"># Model definition</span></span>
<span id="cb12-6"><a></a><span class="kw">class</span> ExactGPModel(gpytorch.models.ExactGP):</span>
<span id="cb12-7"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, train_x, train_y, likelihood):</span>
<span id="cb12-8"><a></a>        <span class="bu">super</span>(ExactGPModel, <span class="va">self</span>).<span class="fu">__init__</span>(train_x, train_y, likelihood)</span>
<span id="cb12-9"><a></a>        <span class="va">self</span>.mean_module <span class="op">=</span> gpytorch.means.ZeroMean()</span>
<span id="cb12-10"><a></a>        <span class="va">self</span>.covar_module <span class="op">=</span> gpytorch.kernels.ScaleKernel(</span>
<span id="cb12-11"><a></a>            gpytorch.kernels.RBFKernel())</span>
<span id="cb12-12"><a></a>    </span>
<span id="cb12-13"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-14"><a></a>        mean_x <span class="op">=</span> <span class="va">self</span>.mean_module(x)</span>
<span id="cb12-15"><a></a>        covar_x <span class="op">=</span> <span class="va">self</span>.covar_module(x)</span>
<span id="cb12-16"><a></a>        <span class="cf">return</span> gpytorch.distributions.MultivariateNormal(mean_x, covar_x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<h3 class="smaller" id="training-setup">Training Setup</h3>
<ul>
<li><strong>Key Components</strong>:
<ul>
<li>Gaussian likelihood</li>
<li>Exact marginal log likelihood</li>
<li>Adam optimizer</li>
<li>Full-batch training</li>
</ul></li>
</ul>
<div id="b97b170e" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a><span class="co"># Initialize components</span></span>
<span id="cb13-2"><a></a>likelihood <span class="op">=</span> gpytorch.likelihoods.GaussianLikelihood()</span>
<span id="cb13-3"><a></a>model <span class="op">=</span> ExactGPModel(train_x, train_y, likelihood)</span>
<span id="cb13-4"><a></a></span>
<span id="cb13-5"><a></a><span class="co"># Training configuration</span></span>
<span id="cb13-6"><a></a>model.train()</span>
<span id="cb13-7"><a></a>likelihood.train()</span>
<span id="cb13-8"><a></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb13-9"><a></a>mll <span class="op">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<h3 class="smaller" id="training-process">Training Process</h3>
<ul>
<li><strong>Important Notes</strong>:
<ul>
<li>Full-batch optimization required</li>
<li>No mini-batches (marginal likelihood)</li>
<li>L-BFGS recommended for final optimization</li>
<li>Good optimization → good generalization</li>
</ul></li>
</ul>
<div id="851aacf2" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>training_iter <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb14-2"><a></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(training_iter):</span>
<span id="cb14-3"><a></a>    optimizer.zero_grad()</span>
<span id="cb14-4"><a></a>    output <span class="op">=</span> model(train_x)</span>
<span id="cb14-5"><a></a>    loss <span class="op">=</span> <span class="op">-</span>mll(output, train_y)</span>
<span id="cb14-6"><a></a>    loss.backward()</span>
<span id="cb14-7"><a></a>    </span>
<span id="cb14-8"><a></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-9"><a></a>        <span class="bu">print</span>(<span class="ss">f'Iter </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">:d}</span><span class="ss">/</span><span class="sc">{</span>training_iter<span class="sc">:d}</span><span class="ss"> - Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb14-10"><a></a>    </span>
<span id="cb14-11"><a></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iter 1/50 - Loss: 0.994
Iter 11/50 - Loss: 0.712
Iter 21/50 - Loss: 0.459
Iter 31/50 - Loss: 0.348
Iter 41/50 - Loss: 0.362</code></pre>
</div>
</div>
<h3 class="smaller" id="key-advantages">Key Advantages</h3>
<div class="columns">
<div class="column" style="width:33%;">
<p><strong>Implementation</strong> * Clean, modular code * Easy kernel switching * Automatic differentiation * GPU support</p>
</div><div class="column" style="width:33%;">
<p><strong>Performance</strong> * Efficient matrix operations * Modern optimization methods * Scalable to large datasets * State-of-the-art inference</p>
</div><div class="column" style="width:33%;">
<p><strong>Extensibility</strong> * Custom kernels * Custom likelihoods * Neural network integration * Advanced inference methods</p>
</div></div>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<div id="refs" role="list">

</div>
<script>
document.getElementById("marimo-frame").onload = function() {
    try {
        let iframeDoc = document.getElementById("marimo-frame").contentWindow.document;
        let marimoBadge = iframeDoc.querySelector("div.fixed.bottom-0.right-0.z-50");
        if (marimoBadge) {
            marimoBadge.style.display = "none";
            console.log("Marimo badge hidden successfully.");
        } else {
            console.log("Badge not found.");
        }
    } catch (error) {
        console.warn("Unable to modify iframe content due to CORS restrictions.");
    }
};
</script>
</section></section></div>

<div class="quarto-auto-generated-content" style="display: none;">
<p><img src="eclipse_logo_small.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy</p>
</div>
</div>

    </div>
  

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"right","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true,"width":"wide"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: false,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/ECLIPSE-Lab\.github\.io\/public_presentations\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>