 
# Gaussian Process Inference

* this section: show how to perform posterior inference and make predictions using the GP priors 

. . .

* start with regression, where we can perform **inference in _closed form_**. 

. . .

* start coding all the basic operations from scratch

* then introduce [GPyTorch](https://gpytorch.ai/) --> convenient SOTA GPs. 

. . .

* consider more advanced topics in depth in the next section

* settings where **approximate inference** is required --- classification, point processes, or any non-Gaussian likelihoods. 

## Posterior Inference for Regression 1

* **Observation model**: relates $f(x)$ to observations $y(x)$
  * $x$: input (e.g., image pixels)
  * $y$: output (e.g., class label, temperature, concentration)

* Regression model:
  $$y(x) = f(x) + \epsilon(x), \quad \epsilon(x) \sim \mathcal{N}(0,\sigma^2)$$

* Notation:
  * $\mathbf{y} = (y(x_1),\dots,y(x_n))^{\top}$: training observations
  * $\textbf{f} = (f(x_1),\dots,f(x_n))^{\top}$: latent function values
  * $X = \{x_1, \dots, x_n\}$: training inputs

## Posterior Inference for Regression 2

* GP prior: $f(x) \sim \mathcal{GP}(m,k)$
  * Mean vector: $\mu_i = m(x_i)$
  * Covariance matrix: $K_{ij} = k(x_i,x_j)$

* Standard choices:
  * RBF kernel: $k(x_i,x_j) = a^2 \exp\left(-\frac{1}{2\ell^2}||x_i-x_j||^2\right)$
  * Mean function: $m(x)=0$ (for simplicity)

* Goal: Predict at test inputs $X_* = \{x_{*1},x_{*2},\dots,x_{*m}\}$
  * Find $p(\mathbf{f}_* | \mathbf{y}, X)$
  * Use Gaussian identities for joint distribution

## Posterior Inference for Regression 3

* Training data: $\mathbf{y} = \mathbf{f} + \mathbf{\epsilon}$
  * $\mathbf{f} \sim \mathcal{N}(0,K(X,X))$
  * $\mathbf{\epsilon} \sim \mathcal{N}(0,\sigma^2I)$
  * $\mathbf{y} \sim \mathcal{N}(0, K(X,X) + \sigma^2I)$

* Covariance structure:
  * $\mathrm{cov}(\mathbf{f}_*, \mathbf{y}) = K(X_*,X)$
  * Joint distribution:
  $$
  \begin{bmatrix}
  \mathbf{y} \\
  \mathbf{f}_*
  \end{bmatrix}
  \sim
  \mathcal{N}\left(0, 
  \begin{bmatrix}
  K(X,X)+\sigma^2I & K(X,X_*) \\
  K(X_*,X) & K(X_*,X_*)
  \end{bmatrix}
  \right)
  $$

## Posterior Inference for Regression 4

* Kernel parameters $\theta$ (e.g., $a$, $\ell$ in RBF)
* Use marginal likelihood $p(\textbf{y} | \theta, X)$ for learning
* Marginal likelihood properties:
  * Balances model fit and complexity
  * Implements Occam's razor
  * See [MacKay Ch. 28](https://books.google.de/books?id=AKuMj4PN_EMC) and [Rasmussen and Williams Ch. 5](https://gaussianprocess.org/gpml/chapters/RW.pdf)

## Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression

* Two-step procedure:
  1. Learn $\hat{\theta}$ via marginal likelihood maximization
  2. Use predictive mean & 2×std for 95% credible set

* Log marginal likelihood:
  $$\log p(\textbf{y} | \theta, X) = -\frac{1}{2}\textbf{y}^{\top}[K_{\theta}(X,X) + \sigma^2I]^{-1}\textbf{y} - \frac{1}{2}\log|K_{\theta}(X,X)| + c$$

* Predictive distribution:
  $$p(y_* | x_*, \textbf{y}, \theta) = \mathcal{N}(a_*,v_*)$$
  $$a_* = k_{\theta}(x_*,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}(\textbf{y}-\mu) + \mu$$
  $$v_* = k_{\theta}(x_*,x_*) - K_{\theta}(x_*,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}k_{\theta}(X,x_*)$$

## Interpreting Equations for Learning and Predictions 1

* Key advantages:
  * **Exact** Bayesian inference in **closed form**
  * No training beyond hyperparameter learning
  * Explicit predictive equations
  * Exceptional convenience & versatility

* Predictive mean:
  * Linear combination of training targets
  * Weights determined by kernel
  * Kernel crucial for generalization

* Predictive variance:
  * Independent of target values
  * Grows with distance from training points
  * Implicitly depends on $\theta$ learned from data

## Interpreting Equations for Learning and Predictions 2

* Computational considerations:
  * Bottleneck: $n \times n$ matrix operations
  * Naive complexity: $\mathcal{O}(n^3)$ compute, $\mathcal{O}(n^2)$ storage
  * Historical limit: ~10,000 points
  * Modern scaling: millions of points possible

* Numerical stability:
  * $K(X,X)$ often near-singular
  * $\sigma^2I$ term improves conditioning
  * "Jitter" ($\sim 10^{-6}$) for noise-free cases

## Worked Example: GP Regression from Scratch

### 1. Data Generation
* True function: $f(x) = \sin(x) + \frac{1}{2}\sin(4x)$
* Observations: $y(x) = f(x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0,0.25^2)$
* Goal: Recover $f(x)$ from noisy observations

![](./img/output_gp-inference_714770_3_0.svg){width=100%}

## 2. GP Prior Specification
* Mean function: $m(x) = 0$
* Kernel: RBF
  $$k(x_i,x_j) = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right)$$
* Initial length-scale: 0.2


```{python}
#| label: setup1
#| code-fold: true
import numpy as np
import d2l
import torch 
import gpytorch
import math
import os 
import matplotlib.pyplot as plt 
import torch
from scipy import optimize
 

d2l.set_figsize()
def data_maker1(x, sig):
    return np.sin(x) + 0.5 * np.sin(4 * x) + np.random.randn(x.shape[0]) * sig

sig = 0.25
train_x, test_x = np.linspace(0, 5, 50), np.linspace(0, 5, 500)
train_y, test_y = data_maker1(train_x, sig=sig), data_maker1(test_x, sig=0.)

d2l.plt.scatter(train_x, train_y)
d2l.plt.plot(test_x, test_y)
d2l.plt.xlabel("x", fontsize=20)
d2l.plt.ylabel("Observations y", fontsize=20)
d2l.plt.show()
```

---


```{python}


mean = np.zeros(test_x.shape[0])
cov = d2l.rbfkernel(test_x, test_x, ls=0.2)
```

* Visualize prior:
  * Sample functions
  * 95% credible set
  * Assess reasonableness

```{python}
prior_samples = np.random.multivariate_normal(mean=mean, cov=cov, size=5)
d2l.plt.plot(test_x, prior_samples.T, color='black', alpha=0.5)
d2l.plt.plot(test_x, mean, linewidth=2.)
d2l.plt.fill_between(test_x, mean - 2 * np.diag(cov), mean + 2 * np.diag(cov), 
                 alpha=0.25)
d2l.plt.show()
```

## 3. Hyperparameter Learning
* Initial values:
  * Length-scale: 0.4
  * Noise std: 0.75
* Optimize via marginal likelihood:
  $$\log p(y | X) = -\frac{1}{2}y^T(K + \sigma^2 I)^{-1}y - \frac{1}{2}\log |K + \sigma^2 I| - \frac{n}{2}\log 2\pi$$

```{python}
ell_est = 0.4
post_sig_est = 0.5

def neg_MLL(pars):
    K = d2l.rbfkernel(train_x, train_x, ls=pars[0])
    kernel_term = -0.5 * train_y @ \
        np.linalg.inv(K + pars[1] ** 2 * np.eye(train_x.shape[0])) @ train_y
    logdet = -0.5 * np.log(np.linalg.det(K + pars[1] ** 2 * \
                                         np.eye(train_x.shape[0])))
    const = -train_x.shape[0] / 2. * np.log(2 * np.pi)
    
    return -(kernel_term + logdet + const)


learned_hypers = optimize.minimize(neg_MLL, x0=np.array([ell_est,post_sig_est]), 
                                   bounds=((0.01, 10.), (0.01, 10.)))
ell = learned_hypers.x[0]
post_sig_est = learned_hypers.x[1]
```

---


* Learned parameters:
  * Length-scale: 0.299
  * Noise std: 0.24
  * Close to true noise → well-specified model

## 4. Posterior Inference
* Predictive distribution:
  * Mean: $\bar{f}_{*} = K(x, x_*)^T (K + \sigma^2 I)^{-1}y$
  * Variance: $V(f_{*}) = K(x_*, x_*) - K(x, x_*)^T (K + \sigma^2 I)^{-1}K(x, x_*)$

```{python}
K_x_xstar = d2l.rbfkernel(train_x, test_x, ls=ell)
K_x_x = d2l.rbfkernel(train_x, train_x, ls=ell)
K_xstar_xstar = d2l.rbfkernel(test_x, test_x, ls=ell)

post_mean = K_x_xstar.T @ np.linalg.inv((K_x_x + \
                post_sig_est ** 2 * np.eye(train_x.shape[0]))) @ train_y
post_cov = K_xstar_xstar - K_x_xstar.T @ np.linalg.inv((K_x_x + \
                post_sig_est ** 2 * np.eye(train_x.shape[0]))) @ K_x_xstar
```

## 5. Uncertainty Analysis
* Two types of uncertainty:
  * **Epistemic** (reducible):
    * Uncertainty about true function
    * Captured by `np.diag(post_cov)`
    * Grows away from data
  * **Aleatoric** (irreducible):
    * Observation noise
    * Captured by `post_sig_est**2`

* 95% credible sets:
  * For true function:
    ```{python}
    lw_bd = post_mean - 2 * np.sqrt(np.diag(post_cov))
    up_bd = post_mean + 2 * np.sqrt(np.diag(post_cov))
    ```
  * For observations:
    ```{python}
    lw_bd_observed = post_mean - 2 * np.sqrt(np.diag(post_cov) + post_sig_est ** 2)
    up_bd_observed = post_mean + 2 * np.sqrt(np.diag(post_cov) + post_sig_est ** 2)
    ```

## 6. Posterior Samples
* Visualize uncertainty:
  * 20 posterior samples
  * Show function space consistent with data
  * Helps understand model fit

```{python}
post_samples = np.random.multivariate_normal(post_mean, post_cov, size=20)
d2l.plt.scatter(train_x, train_y)
d2l.plt.plot(test_x, test_y, linewidth=2.)
d2l.plt.plot(test_x, post_mean, linewidth=2.)
d2l.plt.plot(test_x, post_samples.T, color='gray', alpha=0.25)
d2l.plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25)
plt.legend(['Observed Data', 'True Function', 'Predictive Mean', 'Posterior Samples'])
d2l.plt.show()
```

## GPyTorch: Modern GP Implementation

### Why GPyTorch? {.smaller}

::: {.columns}
::: {.column width="50%"}
**Advanced Features**
* Multiple kernel choices
* Approximate inference
* Neural network integration
* Scalability (>10k points)
* Advanced methods (SKI/KISS-GP)
:::

::: {.column width="50%"}
**Implementation Benefits**
* No manual implementation
* Efficient numerical routines
* GPU acceleration
* Modern PyTorch ecosystem
:::
:::

### Model Definition {.smaller}

* **Exact GP Implementation**:
  * Zero mean function
  * RBF kernel
  * Gaussian likelihood

```{python}
# Data preparation
train_x = torch.tensor(train_x)
train_y = torch.tensor(train_y)

# Model definition
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ZeroMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel())
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)
```

### Training Setup {.smaller}

* **Key Components**:
  * Gaussian likelihood
  * Exact marginal log likelihood
  * Adam optimizer
  * Full-batch training

```{python}
# Initialize components
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x, train_y, likelihood)

# Training configuration
model.train()
likelihood.train()
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
```

### Training Process {.smaller}

* **Important Notes**:
  * Full-batch optimization required
  * No mini-batches (marginal likelihood)
  * L-BFGS recommended for final optimization
  * Good optimization → good generalization

```{python}
training_iter = 50
for i in range(training_iter):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output, train_y)
    loss.backward()
    
    if i % 10 == 0:
        print(f'Iter {i+1:d}/{training_iter:d} - Loss: {loss.item():.3f}')
    
    optimizer.step()
```

### Key Advantages {.smaller}

::: {.columns}
::: {.column width="33%"}
**Implementation**
* Clean, modular code
* Easy kernel switching
* Automatic differentiation
* GPU support
:::

::: {.column width="33%"}
**Performance**
* Efficient matrix operations
* Modern optimization methods
* Scalable to large datasets
* State-of-the-art inference
:::

::: {.column width="33%"}
**Extensibility**
* Custom kernels
* Custom likelihoods
* Neural network integration
* Advanced inference methods
:::
:::

