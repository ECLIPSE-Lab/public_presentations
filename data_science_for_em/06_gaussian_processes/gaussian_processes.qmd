## Introduction

- Even if you have spent some time reading about machine learning, chances are that you have never heard of Gaussian processes
- Gaussian processes are a powerful tool in the machine learning toolbox
- They allow us to make predictions about our data by incorporating prior knowledge
- Their most obvious area of application is fitting a function to the data (regression)

## Key Features of Gaussian Processes

- For a given set of training points, there are potentially infinitely many functions that fit the data
- Gaussian processes offer an elegant solution by assigning a probability to each of these functions
- The mean of this probability distribution represents the most probable characterization of the data
- Using a probabilistic approach allows us to incorporate the confidence of the prediction

## Mathematical Foundation

### Multivariate Gaussian Distributions

- The basic building block of Gaussian processes
- Defined by a mean vector $\mu$ and a covariance matrix $\Sigma$
- Each random variable is distributed normally
- Their joint distribution is also Gaussian

### Mathematical Definition

For a random vector $X$:

$$X = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma)$$

The covariance matrix $\Sigma$ is defined as:

$$\Sigma = \text{Cov}(X_i, X_j) = E \left[ (X_i - \mu_i)(X_j - \mu_j)^T \right]$$

## Properties of Multivariate Gaussians

- The mean vector $\mu$ describes the expected value of the distribution
- Each component describes the mean of the corresponding dimension
- $\Sigma$ models the variance along each dimension
- Determines how different random variables are correlated
- The covariance matrix is always symmetric and positive semi-definite

### Marginalization and Conditioning

- Gaussian distributions are closed under conditioning and marginalization
- This means the resulting distributions from these operations are also Gaussian
- Makes many problems in statistics and machine learning tractable
- These operations are the foundation for Gaussian processes

## Marginalization

For a joint distribution $P_{X,Y}$:

$$P_{X,Y} = \begin{bmatrix} X \\ Y \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma) = \mathcal{N} \left( \begin{bmatrix} \mu_X \\ \mu_Y \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} \, \Sigma_{XY} \\ \Sigma_{YX} \, \Sigma_{YY} \end{bmatrix} \right)$$

The marginalized distributions are:

$$\begin{aligned}
X &\sim \mathcal{N}(\mu_X, \Sigma_{XX}) \\
Y &\sim \mathcal{N}(\mu_Y, \Sigma_{YY})
\end{aligned}$$

### Conditioning

The conditional distributions are:

$$\begin{aligned}
X|Y &\sim \mathcal{N}(\:\mu_X + \Sigma_{XY}\Sigma_{YY}^{-1}(Y - \mu_Y),\: \Sigma_{XX}-\Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}\:) \\
Y|X &\sim \mathcal{N}(\:\mu_Y + \Sigma_{YX}\Sigma_{XX}^{-1}(X - \mu_X),\: \Sigma_{YY}-\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}\:) \\
\end{aligned}$$

## Gaussian Processes

- Move from continuous view to discrete representation
- Each test point is treated as a random variable
- The joint probability distribution spans the space of possible function values
- We use Bayesian inference to update our hypothesis as new information becomes available

## Kernels

- The covariance matrix $\Sigma$ is determined by its covariance function $k$ (kernel)
- The kernel receives two points $t,t' \in \mathbb{R}^n$ and returns a similarity measure
- The kernel determines the characteristics of the function we want to predict
- Common kernels include:
  - RBF (Radial Basis Function)
  - Periodic
  - Linear
  - Mat√©rn

## Kernel Combinations

- Kernels can be combined to create more specialized kernels
- Common combinations:
  - Addition: $k^*(t,t') = k_1(t,t') + k_2(t,t')$
  - Multiplication: $k^*(t,t') = k_1(t,t') \cdot k_2(t,t')$
- Allows modeling of complex patterns:
  - Global trends
  - Periodic behavior
  - Local variations

## Prior and Posterior Distributions

### Prior Distribution
- Before observing any training data
- Revolves around $\mu=0$
- Dimensionality matches number of test points
- Kernel determines the shape of possible functions

### Posterior Distribution
- After observing training data
- Updated through conditioning
- Incorporates measurement errors
- Provides mean prediction and confidence intervals

## Applications and Extensions

- Regression (main application)
- Classification
- Clustering
- Model-peeling
- Hypothesis testing
- Deep learning integration
- Learning specialized kernel functions

## Conclusion

- Gaussian processes offer a flexible framework for regression
- They combine probability theory with kernel methods
- Allow incorporation of domain knowledge through kernel choice
- Provide uncertainty estimates with predictions
- Can be extended to various machine learning tasks

## Further Reading

- Interactive visualizations:
  - [Interactive visualization of Gaussian processes](http://www.infinitecuriosity.org/vizgp/)
  - [Gaussian process regression demo](http://www.tmpl.fi/gp/)
- Tutorials:
  - [Gaussian Processes for Dummies](http://katbailey.github.io/post/gaussian-processes-for-dummies/)
  - [A Practical Guide to Gaussian Processes](https://tinyurl.com/guide2gp)
- Python notebooks:
  - [Fitting Gaussian Process Models in Python](https://blog.dominodatalab.com/fitting-gaussian-process-models-python/)
  - [Gaussian process lecture](http://nbviewer.jupyter.org/github/adamian/adamian.github.io/blob/master/talks/Brown2016.ipynb/)

## Interactive Visualization

<iframe src="https://www.infinitecuriosity.org/vizgp/" style="width:100%; height:800px;" frameborder="0" scrolling="yes">
</iframe>

<script>
window.addEventListener('load', function() {
    var iframe = document.querySelector('iframe');
    var iframeDoc = iframe.contentWindow.document;
    
    // Hide everything except the body content
    var style = iframeDoc.createElement('style');
    style.textContent = `
        header, footer, nav, aside {
            display: none !important;
        }
        body {
            margin: 0;
            padding: 0;
        }
    `;
    iframeDoc.head.appendChild(style);
});
</script>
 