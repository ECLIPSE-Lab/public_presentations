

# Introduction to Modern CNNs
## Key Points
- Tour of modern CNN architectures
- Simple concept: stack layers together
- Performance varies with architecture and hyperparameters
- Based on intuition, math insights, and experimentation
- Batch normalization and residual connections are key innovations


## Historical Architectures
### Key Milestones
- **AlexNet** (2012): First large-scale network to beat conventional methods
- **VGG** (2014): Introduced repeating block patterns
- **NiN** (2013): Convolved neural networks patch-wise
- **DenseNet** (2017): Generalized residual architecture


### Pre-CNN Classical Pipeline
#### Traditional Approach
1. Obtain dataset (e.g., Apple QuickTake 100, 1994)
2. Preprocess with hand-crafted features
3. Use standard feature extractors (SIFT, SURF)
4. Feed to classifier (linear model/kernel method)

```{python}
#| label: fig-pre-cnn
#| code-fold: true
import d2l
import torch
from torch import nn
```

## Representation Learning Evolution

::: {.columns}
::: {.column width="60%"}
- Pre-2012: Mechanical feature calculation
- Common features:
  - SIFT
  - SURF
  - HOG
  - Bags of visual words
:::

::: {.column width="40%"}

### Modern Approach
- Features learned automatically
- Hierarchical composition
- Multiple jointly learned layers
- Learnable parameters

:::
:::

## Feature Learning in CNNs


### Layer Progression
- Lowest layers: edges, colors, textures
- Analogous to animal visual system
- Automatic feature design
- Modern CNNs revolutionized approach


![Image filters learned by AlexNet's first layer](img/filters.png){width="600px"}
 

## VGG: Networks Using Blocks


### Key Innovation
- Evolution from individual neurons to layers to blocks
- Similar to VLSI design progression
- Pioneered repeated block structures
- Foundation for modern models


### VGG Block Structure

::: {.columns}
::: {.column width="60%"}
### Basic Building Block
- Convolutional layer with padding
- Nonlinearity (ReLU)
- Pooling layer

### Key Innovation
- Multiple 3×3 convolutions between pooling
- Two 3×3 = one 5×5 receptive field
- Three 3×3 ≈ one 7×7
- Deep and narrow networks perform better
:::

::: {.column width="40%"}
```{python}
def vgg_block(num_convs, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.LazyConv2d(out_channels, 
                                  kernel_size=3, 
                                  padding=1))
        layers.append(nn.ReLU())
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)
```
:::
:::

## VGG Network Architecture
::: {.columns}
::: {.column width=40%"}
### Two Main Parts
1. Convolutional and pooling layers
2. Fully connected layers (like AlexNet)

### Key Difference
- Convolutional layers grouped in blocks
- Nonlinear transformations
- Resolution reduction steps
:::

::: {.column width="60%"}
![From AlexNet to VGG](img/vgg.svg){width="40%" style="background: white"}
:::
:::

## VGG Implementation

```{python}
class VGG(d2l.Classifier):
    def __init__(self, arch, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        conv_blks = []
        for (num_convs, out_channels) in arch:
            conv_blks.append(vgg_block(num_convs, out_channels))
        self.net = nn.Sequential(
            *conv_blks, nn.Flatten(),
            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
            nn.LazyLinear(num_classes))
        self.net.apply(d2l.init_cnn)
```

### VGG Layer Summary

```{python}
VGG(arch=((1, 64), (1, 128), (2, 256), 
          (2, 512), (2, 512))).layer_summary(
    (1, 1, 224, 224))
```

## VGG Training


### Important Notes
- VGG-11 more demanding than AlexNet
- Smaller number of channels for Fashion-MNIST
- Similar training process to AlexNet
- Close match between validation and training loss


```{python}
model = VGG(arch=((1, 16), (1, 32), (2, 64), 
                  (2, 128), (2, 128)), lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
model.apply_init([next(iter(data.get_dataloader(True)))[0]], 
                d2l.init_cnn)
# trainer.fit(model, data)
```

## VGG Summary

 
### Key Contributions
- First truly modern CNN
- Introduced block-based design
- Preference for deep, narrow networks
- Family of similarly parametrized models
 

## Network in Network (NiN)


### Design Challenges
- Fully connected layers consume huge memory
- Adding nonlinearity can destroy spatial structure

### NiN Solution
- Use 1×1 convolutions for local nonlinearities
- Global average pooling instead of fully connected layers

 

## NiN Architecture

::: {.columns}
::: {.column width="60%"}
### Key Differences from VGG
- Applies fully connected layer at each pixel
- Uses 1×1 convolutions after initial convolution
- Eliminates need for large fully connected layers
:::

::: {.column width="40%"}
![Comparing VGG and NiN architectures](img/nin.svg){width="100%"   style="background: grey"}  
:::
:::

## NiN Block Implementation

```{python}
def nin_block(out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.LazyConv2d(out_channels, kernel_size, strides, padding), 
        nn.ReLU(),
        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),
        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())
```

## NiN Model


### Architecture Details
- Initial convolution sizes like AlexNet
- NiN block with output channels = number of classes
- Global average pooling layer
- Significantly fewer parameters


```{python}
class NiN(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nin_block(96, kernel_size=11, strides=4, padding=0),
            nn.MaxPool2d(3, stride=2),
            nin_block(256, kernel_size=5, strides=1, padding=2),
            nn.MaxPool2d(3, stride=2),
            nin_block(384, kernel_size=3, strides=1, padding=1),
            nn.MaxPool2d(3, stride=2),
            nn.Dropout(0.5),
            nin_block(num_classes, kernel_size=3, strides=1, padding=1),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten())
        self.net.apply(d2l.init_cnn)
```

## NiN Layer Summary

```{python}
NiN().layer_summary((1, 1, 224, 224))
```

### NiN Training

```{python}
model = NiN(lr=0.05)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
model.apply_init([next(iter(data.get_dataloader(True)))[0]], 
                d2l.init_cnn)
# trainer.fit(model, data)
```

## NiN Summary

 
### Key Advantages
- Dramatically fewer parameters
- No giant fully connected layers
- Global average pooling
- Simple averaging operation
- Translation invariance
- Nonlinearity across channels
 

## Batch Normalization


### Benefits
- Accelerates network convergence
- Enables training of very deep networks
- Provides inherent regularization
- Makes optimization landscape smoother


 
## Training Deep Networks

 
### Data Preprocessing
- Standardize input features
- Zero mean and unit variance
- Constrain function complexity
- Put parameters at similar scale
 

## Batch Normalization Layers

::: {.columns}
::: {.column width="60%"}
### Fully Connected Layers
- After affine transformation
- Before nonlinear activation
- Normalize across minibatch

### Convolutional Layers
- After convolution
- Before nonlinear activation
- Per-channel basis
- Across all locations
:::

::: {.column width="40%"}
```{python}
def conv_block(num_channels):
    return nn.Sequential(
        nn.LazyBatchNorm2d(), nn.ReLU(),
        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))
```
:::
:::

## Layer Normalization

 
### Key Features
- Applied to one observation at a time
- Offset and scaling factor are scalars
- Prevents divergence
- Scale independent
- Independent of minibatch size
 

## Batch Normalization During Prediction

 
### Important Notes
- Different behavior in training vs prediction
- Use entire dataset for statistics
- Fixed statistics at prediction time
- Similar to dropout behavior
 

## DenseNet


### Key Features
- Logical extension of ResNet
- Each layer connects to all preceding layers
- Concatenation instead of addition
- Preserves and reuses features


 
## DenseNet Architecture
 
::: {.columns}
::: {.column width="60%"}
### Key Components
- Dense blocks
- Transition layers
- Concatenation operation
- Feature reuse
:::

::: {.column width="40%"}
![DenseNet connections](img/densenet.svg){width="100%"  style="background: grey"}
:::
:::

### DenseNet Implementation

```{python}
class DenseBlock(nn.Module):
    def __init__(self, num_convs, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(num_channels))
        self.net = nn.Sequential(*layer)

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            X = torch.cat((X, Y), dim=1)
        return X
```

## Transition Layers

```{python}
def transition_block(num_channels):
    return nn.Sequential(
        nn.LazyBatchNorm2d(), nn.ReLU(),
        nn.LazyConv2d(num_channels, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2))
```

### DenseNet Model

```{python}
class DenseNet(d2l.Classifier):
    def b1(self):
        return nn.Sequential(
            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
            nn.LazyBatchNorm2d(), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

    def __init__(self, num_channels=64, growth_rate=32, 
                 arch=(4, 4, 4, 4), lr=0.1, num_classes=10):
        super(DenseNet, self).__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(self.b1())
        for i, num_convs in enumerate(arch):
            self.net.add_module(f'dense_blk{i+1}', 
                              DenseBlock(num_convs, growth_rate))
            num_channels += num_convs * growth_rate
            if i != len(arch) - 1:
                num_channels //= 2
                self.net.add_module(f'tran_blk{i+1}', 
                                  transition_block(num_channels))
        self.net.add_module('last', nn.Sequential(
            nn.LazyBatchNorm2d(), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.LazyLinear(num_classes)))
        self.net.apply(d2l.init_cnn)
```

### DenseNet Training

```{python}
model = DenseNet(lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
# trainer.fit(model, data)
```

## U-Net Architecture


### Key Features
- Originally for biomedical image segmentation
- Symmetric encoder-decoder structure
- Skip connections
- Works with limited training data
- Preserves spatial information


![U-Net architecture](img/u-net-architecture.png){width="40%"  style="background: grey"}

## U-Net Components

::: {.columns}
::: {.column width="50%"}
### Contracting Path
- Convolutional layers
- Max pooling
- Doubles feature channels
- Reduces spatial dimensions
:::

::: {.column width="50%"}
### Expansive Path
- Upsampling
- Feature concatenation
- Successive convolutions
- Recovers resolution
:::
:::

## U-Net Implementation

```{python}
class UNetBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x
```

### U-Net Applications

 
### Use Cases
- Medical image segmentation
- Object detection
- Industrial defect detection
- General segmentation tasks

### Advantages
- Works with limited data
- Precise localization
- End-to-end training
- Fast inference