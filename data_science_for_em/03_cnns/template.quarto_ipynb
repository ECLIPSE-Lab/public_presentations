{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: |\n",
        "  Data Science for Electron Microscopy<br>\n",
        "  Lecture 3: Convolutional Neural Networks\n",
        "bibliography: ref.bib\n",
        "csl: custom.csl\n",
        "author:\n",
        "  - name: Philipp Pelz\n",
        "    affiliation: \n",
        "      - FAU Erlangen-Nürnberg\n",
        "execute: \n",
        "  eval: true\n",
        "  echo: true\n",
        "format:\n",
        "    revealjs: \n",
        "        code-copy: true\n",
        "        # scroll-view:\n",
        "        #     activate: true\n",
        "        #     snap: mandatory\n",
        "        #     layout: full\n",
        "        width: 1920\n",
        "        height: 1080\n",
        "        menu:\n",
        "            side: right\n",
        "            width: wide\n",
        "        template-partials:\n",
        "            - title-slide.html\n",
        "        css: custom.css\n",
        "        theme: custom.scss\n",
        "        slide-number: c/t    \n",
        "        logo: \"eclipse_logo_small.png\" \n",
        "        highlight-style: a11y\n",
        "        incremental: false \n",
        "        background-transition: fade\n",
        "        footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "---\n",
        "\n",
        "\n",
        "## From Fully Connected Layers to Convolutions\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- MLPs impractical for high-dimensional perceptual data\n",
        "- One-megapixel image → $10^9$ parameters with 1000 hidden units\n",
        "- CNNs exploit rich image structure\n",
        "\n",
        "## Three Key CNN Design Principles\n",
        "\n",
        "### Core Principles\n",
        "1. **Translation Invariance**: Network responds similarly to patterns regardless of location\n",
        "2. **Locality**: Early layers focus on local regions\n",
        "3. **Hierarchy**: Deeper layers capture longer-range features\n",
        "\n",
        "## Invariance in Object Detection\n",
        "\n",
        "### Key Concept\n",
        "- Recognition should not depend on precise object location\n",
        "- Illustrated by \"Where's Waldo\" game\n",
        "- Waldo's appearance independent of location\n",
        "- Sweep image with detector for likelihood scores\n",
        "\n",
        "\n",
        "![An image of the \"Where's Waldo\" game.](./img/where-wally-walker-books.jpg)\n",
        "\n",
        "## CNN Design Desiderata\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "1. **Translation Invariance**\n",
        "   - Early layers respond similarly to same patch\n",
        "   - Regardless of location in image\n",
        "\n",
        "2. **Locality**\n",
        "   - Early layers focus on local regions\n",
        "   - Aggregate local representations later\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "3. **Hierarchy**\n",
        "   - Deeper layers capture longer-range features\n",
        "   - Similar to higher-level vision in nature\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Constraining the MLP\n",
        "\n",
        " \n",
        "### Mathematical Formulation\n",
        "- Input images $\\mathbf{X}$ and hidden representations $\\mathbf{H}$ as matrices\n",
        "- Fourth-order weight tensors $\\mathsf{W}$\n",
        "- With biases $\\mathbf{U}$:\n",
        "\n",
        "$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n",
        "\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}$$\n",
        "\n",
        "## Translation Invariance\n",
        "\n",
        " \n",
        "### Key Insight\n",
        "- Shift in input $\\mathbf{X}$ → shift in hidden representation $\\mathbf{H}$\n",
        "- $\\mathsf{V}$ and $\\mathbf{U}$ independent of $(i, j)$\n",
        "- Simplified definition:\n",
        "\n",
        "$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
        "\n",
        "- This is a *convolution*!\n",
        "\n",
        "## Locality\n",
        "\n",
        " \n",
        "### Implementation\n",
        "- Only look near location $(i, j)$\n",
        "- Set $[\\mathbf{V}]_{a, b} = 0$ outside range $|a|> \\Delta$ or $|b| > \\Delta$\n",
        "- Rewritten as:\n",
        "\n",
        "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
        "\n",
        "- Reduces parameters from $4 \\cdot 10^6$ to $4 \\Delta^2$\n",
        "\n",
        "## Convolutions in Mathematics\n",
        "\n",
        " \n",
        "### Definition\n",
        "- Convolution between functions $f, g: \\mathbb{R}^d \\to \\mathbb{R}$:\n",
        "\n",
        "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$$\n",
        "\n",
        "- For discrete objects (2D tensors):\n",
        "\n",
        "$$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$$\n",
        "\n",
        "## Channels in CNNs\n",
        " \n",
        " \n",
        "### Key Concepts\n",
        "- Images: 3 channels (RGB)\n",
        "- Third-order tensors: height × width × channel\n",
        "- Convolutional filter adapts: $[\\mathsf{V}]_{a,b,c}$\n",
        "- Hidden representations: third-order tensors $\\mathsf{H}$\n",
        "- Feature maps: spatialized learned features\n",
        "\n",
        "\n",
        "![Detect Waldo.](./img/waldo-mask.jpg){width=400px}\n",
        "\n",
        "## Multi-Channel Convolution\n",
        "\n",
        " \n",
        "### Complete Formulation\n",
        "- Input channels: $c_i$\n",
        "- Output channels: $c_o$\n",
        "- Kernel shape: $c_o\\times c_i\\times k_h\\times k_w$\n",
        "- Complete convolution:\n",
        "\n",
        "$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$\n",
        "\n",
        "where $d$ indexes output channels\n",
        "\n",
        "## Summary and Discussion\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- CNNs derived from first principles\n",
        "- Translation invariance: treat all patches similarly\n",
        "- Locality: use small neighborhoods\n",
        "- Channels: restore complexity lost to restrictions\n",
        "- Hyperspectral images: tens to hundreds of channels\n",
        ":::\n",
        "\n",
        "\n",
        "## Convolutions for Images"
      ],
      "id": "0307e09b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import d2l\n",
        "import torch\n",
        "from torch import nn"
      ],
      "id": "06870116",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Cross-Correlation Operation\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- Convolutional layers actually perform cross-correlation\n",
        "- Input tensor and kernel tensor combined\n",
        "- Window slides across input tensor\n",
        "- Elementwise multiplication and summation\n",
        "\n",
        "\n",
        "![Two-dimensional cross-correlation operation](./img/correlation.svg)\n",
        "\n",
        "## Cross-Correlation Implementation"
      ],
      "id": "5a8c5043"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corr2d(X, K):  #@save\n",
        "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
        "    h, w = K.shape\n",
        "    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))\n",
        "    return Y"
      ],
      "id": "a2c047dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Correlation Example"
      ],
      "id": "ac03b549"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a 256x256 sparse image with a few 1s\n",
        "X = torch.zeros((64, 64))\n",
        "X[11, 23] = 1.0\n",
        "X[23, 23] = 1.0\n",
        "X[56, 48] = 1.0\n",
        "X[19, 54] = 1.0\n",
        "\n",
        "# Create a 3x3 cross-shaped kernel\n",
        "K = torch.tensor([[0.0, 1.0, 0.0],\n",
        "                 [1.0, 1.0, 1.0],\n",
        "                 [0.0, 1.0, 0.0]])\n",
        "\n",
        "# Apply the correlation\n",
        "Y = corr2d(X, K)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot input image X\n",
        "ax1.imshow(X, cmap='gray')\n",
        "ax1.set_title('Input Image (X)')\n",
        "ax1.axis('on')\n",
        "\n",
        "# Plot kernel K\n",
        "ax2.imshow(K, cmap='gray')\n",
        "ax2.set_title('Kernel (K)')\n",
        "ax2.axis('on')\n",
        "\n",
        "# Plot output Y\n",
        "ax3.imshow(Y, cmap='gray')\n",
        "ax3.set_title('Output (Y)')\n",
        "ax3.axis('on')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "671e9ed7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convolutional Layers\n",
        "\n",
        " \n",
        "### Implementation\n",
        "- Cross-correlate input and kernel\n",
        "- Add scalar bias\n",
        "- Initialize kernels randomly\n",
        "- Parameters: kernel and scalar bias"
      ],
      "id": "dbd1222a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Conv2D(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return corr2d(x, self.weight) + self.bias"
      ],
      "id": "8936eaf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Edge Detection Example\n",
        "\n",
        " \n",
        "### Application\n",
        "- Detect object edges in images\n",
        "- Find pixel change locations\n",
        "- Use special kernel for edge detection"
      ],
      "id": "769a8c8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.ones((6, 8))\n",
        "X[:, 2:6] = 0\n",
        "X"
      ],
      "id": "4b6f3c2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "K = d2l.tensor([[1.0, -1.0]])\n",
        "Y = corr2d(X, K)\n",
        "Y"
      ],
      "id": "eb0ffbf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning a Kernel\n",
        "\n",
        " \n",
        "### Training Process\n",
        "- Learn kernel from input-output pairs\n",
        "- Use squared error loss\n",
        "- Update kernel via gradient descent"
      ],
      "id": "665367e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
        "X = X.reshape((1, 1, 6, 8))\n",
        "Y = Y.reshape((1, 1, 6, 7))\n",
        "lr = 3e-2\n",
        "\n",
        "for i in range(10):\n",
        "    Y_hat = conv2d(X)\n",
        "    l = (Y_hat - Y) ** 2\n",
        "    conv2d.zero_grad()\n",
        "    l.sum().backward()\n",
        "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "    if (i + 1) % 2 == 0:\n",
        "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
      ],
      "id": "a42d3fc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Correlation vs Convolution\n",
        "\n",
        " \n",
        "### Key Differences\n",
        "- Strict convolution: flip kernel horizontally and vertically\n",
        "- Cross-correlation: use original kernel\n",
        "- Outputs remain same due to learned kernels\n",
        "- Term \"convolution\" used for both operations\n",
        "\n",
        "\n",
        "## Feature Maps and Receptive Fields\n",
        "\n",
        " \n",
        "### Concepts\n",
        "- Feature map: learned spatial representations\n",
        "- Receptive field: elements affecting calculation\n",
        "- Can be larger than input size\n",
        "- Deeper networks for larger receptive fields\n",
        "\n",
        "\n",
        "![Figure from Field (1987): Coding with six different channels](./img/field-visual.png){width=600px}\n",
        "\n",
        "## Summary\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- Core computation: cross-correlation\n",
        "- Multiple channels: matrix-matrix operations\n",
        "- Highly local computation\n",
        "- Hardware optimization opportunities\n",
        "- Learnable filters replace feature engineering\n",
        "\n",
        "## Exercises\n",
        "\n",
        " \n",
        "### Practice Problems\n",
        "1. Diagonal edges and kernel effects\n",
        "2. Manual kernel design\n",
        "3. Gradient computation errors\n",
        "4. Cross-correlation as matrix multiplication\n",
        "5. Fast convolution algorithms\n",
        "6. Block-diagonal matrix multiplication\n",
        "\n",
        "## Padding and Stride\n",
        "### Motivation\n",
        "- Control output size\n",
        "- Prevent information loss\n",
        "- Handle large kernels\n",
        "- Reduce spatial resolution\n"
      ],
      "id": "500093cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "id": "4d18e720",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Padding\n",
        "\n",
        " \n",
        "### Key Concepts\n",
        "- Add extra pixels around boundary\n",
        "- Typically zero padding\n",
        "- Preserve spatial dimensions\n",
        "- Common with odd kernel sizes\n",
        "\n",
        "![Pixel utilization for different convolution sizes](./img/conv-reuse.svg)\n",
        "\n",
        "## Padding Implementation"
      ],
      "id": "36ec6b2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def comp_conv2d(conv2d, X):\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
        "X = torch.rand(size=(8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "4c9c6688",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stride\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- Move window more than one element\n",
        "- Skip intermediate locations\n",
        "- Useful for large kernels\n",
        "- Control output resolution\n",
        "\n",
        "\n",
        "![Cross-correlation with strides of 3 and 2](./img/conv-stride.svg){width=300px}\n",
        "\n",
        "## Stride Implementation"
      ],
      "id": "8919bc1d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
        "comp_conv2d(conv2d, X).shape\n",
        "\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "55e41911",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Discussion\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- Padding: control output dimensions\n",
        "- Stride: reduce resolution\n",
        "- Zero padding: computational benefits\n",
        "- Position information encoding\n",
        "- Alternative padding methods\n",
        " \n",
        "\n",
        "## Multiple Input and Output Channels\n",
        "\n",
        " \n",
        "### Key Concepts\n",
        "- RGB images: 3 channels\n",
        "- Input shape: $3\\times h\\times w$\n",
        "- Channel dimension: size 3\n",
        "- Multiple input/output channels\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "## Multiple Input Channels\n",
        "\n",
        " \n",
        "### Implementation\n",
        "- Kernel matches input channels\n",
        "- Shape: $c_i\\times k_h\\times k_w$\n",
        "- Cross-correlation per channel\n",
        "- Sum results\n"
      ],
      "id": "8d765643"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corr2d_multi_in(X, K):\n",
        "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\n",
        "\n",
        "X = d2l.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
        "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
        "K = d2l.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "corr2d_multi_in(X, K)"
      ],
      "id": "cc9fdd99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Output Channels\n",
        "\n",
        " \n",
        "### Implementation\n",
        "- Kernel tensor for each output channel\n",
        "- Shape: $c_o\\times c_i\\times k_h\\times k_w$\n",
        "- Concatenate on output channel dimension"
      ],
      "id": "280fdae5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corr2d_multi_in_out(X, K):\n",
        "    return d2l.stack([corr2d_multi_in(X, k) for k in K], 0)\n",
        "\n",
        "K = d2l.stack((K, K + 1, K + 2), 0)\n",
        "K.shape\n",
        "\n",
        "corr2d_multi_in_out(X, K)"
      ],
      "id": "759118cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## $1\\times 1$ Convolutional Layer\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- No spatial correlation\n",
        "- Channel dimension computation\n",
        "- Linear combination at each position\n",
        "- Fully connected layer per pixel\n",
        "\n",
        "\n",
        "![$1\\times 1$ convolution with 3 input and 2 output channels](./img/conv-1x1.svg)\n",
        "\n",
        "## $1\\times 1$ Convolution Implementation"
      ],
      "id": "5ed3559d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corr2d_multi_in_out_1x1(X, K):\n",
        "    c_i, h, w = X.shape\n",
        "    c_o = K.shape[0]\n",
        "    X = d2l.reshape(X, (c_i, h * w))\n",
        "    K = d2l.reshape(K, (c_o, c_i))\n",
        "    Y = d2l.matmul(K, X)\n",
        "    return d2l.reshape(Y, (c_o, h, w))\n",
        "\n",
        "X = d2l.normal(0, 1, (3, 3, 3))\n",
        "K = d2l.normal(0, 1, (2, 3, 1, 1))\n",
        "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
        "Y2 = corr2d_multi_in_out(X, K)\n",
        "# assert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) < 1e-6"
      ],
      "id": "625c0377",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- Channels combine MLP and CNN benefits\n",
        "- Trade-off: parameter reduction vs. model expressiveness\n",
        "- Computational cost: $\\mathcal{O}(h \\cdot w \\cdot k^2 \\cdot c_i \\cdot c_o)$\n",
        "- Example: 256×256 image, 5×5 kernel, 128 channels → 53B operations\n",
        "\n",
        " \n",
        "## Pooling\n",
        "\n",
        " \n",
        "### Motivation\n",
        "- Global questions about images\n",
        "- Gradual information aggregation\n",
        "- Translation invariance\n",
        "- Spatial downsampling\n",
        "\n",
        " \n",
        "\n",
        "## Maximum and Average Pooling\n",
        "\n",
        " \n",
        "### Key Concepts\n",
        "- Fixed-shape window\n",
        "- No parameters\n",
        "- Deterministic operations\n",
        "- Maximum or average value\n",
        "\n",
        "\n",
        "![Max-pooling with $2\\times 2$ window](./img/pooling.svg){width=300px}\n",
        "\n",
        "## Pooling Implementation"
      ],
      "id": "5a52bbba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pool2d(X, pool_size, mode='max'):\n",
        "    p_h, p_w = pool_size\n",
        "    Y = d2l.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            if mode == 'max':\n",
        "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
        "            elif mode == 'avg':\n",
        "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
        "    return Y\n",
        "\n",
        "X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "pool2d(X, (2, 2))\n",
        "pool2d(X, (2, 2), 'avg')"
      ],
      "id": "c60ea3f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Padding and Stride in Pooling\n",
        "\n",
        " \n",
        "### Implementation\n",
        "- Adjust output shape\n",
        "- Default: matching window and stride\n",
        "- Manual specification possible\n",
        "- Rectangular windows supported\n"
      ],
      "id": "2a9bc45e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 1, 4, 4))\n",
        "X\n",
        "\n",
        "pool2d = nn.MaxPool2d(3)\n",
        "pool2d(X)\n",
        "\n",
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "pool2d(X)\n",
        "\n",
        "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
        "pool2d(X)"
      ],
      "id": "06e7f96d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Channels in Pooling\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- Pool each channel separately\n",
        "- Maintain channel count\n",
        "- Independent operations\n"
      ],
      "id": "3efbec37"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.concat((X, X + 1), 1)\n",
        "X\n",
        "\n",
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "pool2d(X)"
      ],
      "id": "64efc9e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- Simple aggregation operation\n",
        "- Standard convolution semantics\n",
        "- Channel independence\n",
        "- Max-pooling preferred\n",
        "- $2 \\times 2$ window common\n",
        "- Alternative pooling methods\n",
        " \n",
        " \n",
        "\n",
        "## Summary\n",
        "\n",
        " \n",
        "### Key Points\n",
        "- Evolution from MLPs to CNNs\n",
        "- LeNet-5 remains relevant\n",
        "- Similar to modern architectures\n",
        "- Implementation ease\n",
        "- Democratized deep learning\n",
        "\n",
        "\n",
        "\n",
        "# Introduction to Modern CNNs\n",
        "## Key Points\n",
        "- Tour of modern CNN architectures\n",
        "- Simple concept: stack layers together\n",
        "- Performance varies with architecture and hyperparameters\n",
        "- Based on intuition, math insights, and experimentation\n",
        "- Batch normalization and residual connections are key innovations\n",
        "\n",
        "\n",
        "## Historical Architectures\n",
        "### Key Milestones\n",
        "- **AlexNet** (2012): First large-scale network to beat conventional methods\n",
        "- **VGG** (2014): Introduced repeating block patterns\n",
        "- **NiN** (2013): Convolved neural networks patch-wise\n",
        "- **DenseNet** (2017): Generalized residual architecture\n",
        "\n",
        "\n",
        "### Pre-CNN Classical Pipeline\n",
        "#### Traditional Approach\n",
        "1. Obtain dataset (e.g., Apple QuickTake 100, 1994)\n",
        "2. Preprocess with hand-crafted features\n",
        "3. Use standard feature extractors (SIFT, SURF)\n",
        "4. Feed to classifier (linear model/kernel method)"
      ],
      "id": "2e5c5a8a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-pre-cnn\n",
        "#| code-fold: true\n",
        "import d2l\n",
        "import torch\n",
        "from torch import nn"
      ],
      "id": "fig-pre-cnn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Representation Learning Evolution\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "- Pre-2012: Mechanical feature calculation\n",
        "- Common features:\n",
        "  - SIFT\n",
        "  - SURF\n",
        "  - HOG\n",
        "  - Bags of visual words\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "\n",
        "### Modern Approach\n",
        "- Features learned automatically\n",
        "- Hierarchical composition\n",
        "- Multiple jointly learned layers\n",
        "- Learnable parameters\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Feature Learning in CNNs\n",
        "\n",
        "\n",
        "### Layer Progression\n",
        "- Lowest layers: edges, colors, textures\n",
        "- Analogous to animal visual system\n",
        "- Automatic feature design\n",
        "- Modern CNNs revolutionized approach\n",
        "\n",
        "\n",
        "![Image filters learned by AlexNet's first layer](img/filters.png){width=\"600px\"}\n",
        " \n",
        "\n",
        "## VGG: Networks Using Blocks\n",
        "\n",
        "\n",
        "### Key Innovation\n",
        "- Evolution from individual neurons to layers to blocks\n",
        "- Similar to VLSI design progression\n",
        "- Pioneered repeated block structures\n",
        "- Foundation for modern models\n",
        "\n",
        "\n",
        "### VGG Block Structure\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "### Basic Building Block\n",
        "- Convolutional layer with padding\n",
        "- Nonlinearity (ReLU)\n",
        "- Pooling layer\n",
        "\n",
        "### Key Innovation\n",
        "- Multiple 3×3 convolutions between pooling\n",
        "- Two 3×3 = one 5×5 receptive field\n",
        "- Three 3×3 ≈ one 7×7\n",
        "- Deep and narrow networks perform better\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}"
      ],
      "id": "a04a8282"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def vgg_block(num_convs, out_channels):\n",
        "    layers = []\n",
        "    for _ in range(num_convs):\n",
        "        layers.append(nn.LazyConv2d(out_channels, \n",
        "                                  kernel_size=3, \n",
        "                                  padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "    return nn.Sequential(*layers)"
      ],
      "id": "447c4068",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        ":::\n",
        "\n",
        "## VGG Network Architecture\n",
        "::: {.columns}\n",
        "::: {.column width=40%\"}\n",
        "### Two Main Parts\n",
        "1. Convolutional and pooling layers\n",
        "2. Fully connected layers (like AlexNet)\n",
        "\n",
        "### Key Difference\n",
        "- Convolutional layers grouped in blocks\n",
        "- Nonlinear transformations\n",
        "- Resolution reduction steps\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "![From AlexNet to VGG](img/vgg.svg){width=\"40%\" style=\"background: white\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## VGG Implementation"
      ],
      "id": "033a9c6b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class VGG(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        conv_blks = []\n",
        "        for (num_convs, out_channels) in arch:\n",
        "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
        "        self.net = nn.Sequential(\n",
        "            *conv_blks, nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "id": "5950c6ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VGG Layer Summary"
      ],
      "id": "0302c3f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "VGG(arch=((1, 64), (1, 128), (2, 256), \n",
        "          (2, 512), (2, 512))).layer_summary(\n",
        "    (1, 1, 224, 224))"
      ],
      "id": "236e8421",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG Training\n",
        "\n",
        "\n",
        "### Important Notes\n",
        "- VGG-11 more demanding than AlexNet\n",
        "- Smaller number of channels for Fashion-MNIST\n",
        "- Similar training process to AlexNet\n",
        "- Close match between validation and training loss\n"
      ],
      "id": "27c404bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = VGG(arch=((1, 16), (1, 32), (2, 64), \n",
        "                  (2, 128), (2, 128)), lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], \n",
        "                d2l.init_cnn)\n",
        "# trainer.fit(model, data)"
      ],
      "id": "17ad21bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG Summary\n",
        "\n",
        " \n",
        "### Key Contributions\n",
        "- First truly modern CNN\n",
        "- Introduced block-based design\n",
        "- Preference for deep, narrow networks\n",
        "- Family of similarly parametrized models\n",
        " \n",
        "\n",
        "## Network in Network (NiN)\n",
        "\n",
        "\n",
        "### Design Challenges\n",
        "- Fully connected layers consume huge memory\n",
        "- Adding nonlinearity can destroy spatial structure\n",
        "\n",
        "### NiN Solution\n",
        "- Use 1×1 convolutions for local nonlinearities\n",
        "- Global average pooling instead of fully connected layers\n",
        "\n",
        " \n",
        "\n",
        "## NiN Architecture\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "### Key Differences from VGG\n",
        "- Applies fully connected layer at each pixel\n",
        "- Uses 1×1 convolutions after initial convolution\n",
        "- Eliminates need for large fully connected layers\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![Comparing VGG and NiN architectures](img/nin.svg){width=\"100%\"   style=\"background: grey\"}  \n",
        ":::\n",
        ":::\n",
        "\n",
        "## NiN Block Implementation"
      ],
      "id": "24e1e5f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def nin_block(out_channels, kernel_size, strides, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyConv2d(out_channels, kernel_size, strides, padding), \n",
        "        nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())"
      ],
      "id": "a3ebe4c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NiN Model\n",
        "\n",
        "\n",
        "### Architecture Details\n",
        "- Initial convolution sizes like AlexNet\n",
        "- NiN block with output channels = number of classes\n",
        "- Global average pooling layer\n",
        "- Significantly fewer parameters\n"
      ],
      "id": "ae90a8e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NiN(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nin_block(96, kernel_size=11, strides=4, padding=0),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nin_block(256, kernel_size=5, strides=1, padding=2),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nin_block(384, kernel_size=3, strides=1, padding=1),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.Dropout(0.5),\n",
        "            nin_block(num_classes, kernel_size=3, strides=1, padding=1),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten())\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "id": "0added53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NiN Layer Summary"
      ],
      "id": "03669c2b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "NiN().layer_summary((1, 1, 224, 224))"
      ],
      "id": "e41b1925",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NiN Training"
      ],
      "id": "3f602602"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = NiN(lr=0.05)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], \n",
        "                d2l.init_cnn)\n",
        "# trainer.fit(model, data)"
      ],
      "id": "8154b50f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NiN Summary\n",
        "\n",
        " \n",
        "### Key Advantages\n",
        "- Dramatically fewer parameters\n",
        "- No giant fully connected layers\n",
        "- Global average pooling\n",
        "- Simple averaging operation\n",
        "- Translation invariance\n",
        "- Nonlinearity across channels\n",
        " \n",
        "\n",
        "## Batch Normalization\n",
        "\n",
        "\n",
        "### Benefits\n",
        "- Accelerates network convergence\n",
        "- Enables training of very deep networks\n",
        "- Provides inherent regularization\n",
        "- Makes optimization landscape smoother\n",
        "\n",
        "\n",
        " \n",
        "## Training Deep Networks\n",
        "\n",
        " \n",
        "### Data Preprocessing\n",
        "- Standardize input features\n",
        "- Zero mean and unit variance\n",
        "- Constrain function complexity\n",
        "- Put parameters at similar scale\n",
        " \n",
        "\n",
        "## Batch Normalization Layers\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "### Fully Connected Layers\n",
        "- After affine transformation\n",
        "- Before nonlinear activation\n",
        "- Normalize across minibatch\n",
        "\n",
        "### Convolutional Layers\n",
        "- After convolution\n",
        "- Before nonlinear activation\n",
        "- Per-channel basis\n",
        "- Across all locations\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}"
      ],
      "id": "d0c5c91f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def conv_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))"
      ],
      "id": "950aa6d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        ":::\n",
        "\n",
        "## Layer Normalization\n",
        "\n",
        " \n",
        "### Key Features\n",
        "- Applied to one observation at a time\n",
        "- Offset and scaling factor are scalars\n",
        "- Prevents divergence\n",
        "- Scale independent\n",
        "- Independent of minibatch size\n",
        " \n",
        "\n",
        "## Batch Normalization During Prediction\n",
        "\n",
        " \n",
        "### Important Notes\n",
        "- Different behavior in training vs prediction\n",
        "- Use entire dataset for statistics\n",
        "- Fixed statistics at prediction time\n",
        "- Similar to dropout behavior\n",
        " \n",
        "\n",
        "## DenseNet\n",
        "\n",
        "\n",
        "### Key Features\n",
        "- Logical extension of ResNet\n",
        "- Each layer connects to all preceding layers\n",
        "- Concatenation instead of addition\n",
        "- Preserves and reuses features\n",
        "\n",
        "\n",
        " \n",
        "## DenseNet Architecture\n",
        " \n",
        "::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "### Key Components\n",
        "- Dense blocks\n",
        "- Transition layers\n",
        "- Concatenation operation\n",
        "- Feature reuse\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![DenseNet connections](img/densenet.svg){width=\"100%\"  style=\"background: grey\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "### DenseNet Implementation"
      ],
      "id": "281d00bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, num_convs, num_channels):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layer = []\n",
        "        for i in range(num_convs):\n",
        "            layer.append(conv_block(num_channels))\n",
        "        self.net = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for blk in self.net:\n",
        "            Y = blk(X)\n",
        "            X = torch.cat((X, Y), dim=1)\n",
        "        return X"
      ],
      "id": "bcc16dc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transition Layers"
      ],
      "id": "154ceae3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def transition_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2))"
      ],
      "id": "31e2370b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DenseNet Model"
      ],
      "id": "8d07e346"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class DenseNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def __init__(self, num_channels=64, growth_rate=32, \n",
        "                 arch=(4, 4, 4, 4), lr=0.1, num_classes=10):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.b1())\n",
        "        for i, num_convs in enumerate(arch):\n",
        "            self.net.add_module(f'dense_blk{i+1}', \n",
        "                              DenseBlock(num_convs, growth_rate))\n",
        "            num_channels += num_convs * growth_rate\n",
        "            if i != len(arch) - 1:\n",
        "                num_channels //= 2\n",
        "                self.net.add_module(f'tran_blk{i+1}', \n",
        "                                  transition_block(num_channels))\n",
        "        self.net.add_module('last', nn.Sequential(\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "id": "a532015f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DenseNet Training"
      ],
      "id": "325b6538"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = DenseNet(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "# trainer.fit(model, data)"
      ],
      "id": "4da548cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## U-Net Architecture\n",
        "\n",
        "\n",
        "### Key Features\n",
        "- Originally for biomedical image segmentation\n",
        "- Symmetric encoder-decoder structure\n",
        "- Skip connections\n",
        "- Works with limited training data\n",
        "- Preserves spatial information\n",
        "\n",
        "\n",
        "![U-Net architecture](img/u-net-architecture.png){width=\"40%\"  style=\"background: grey\"}\n",
        "\n",
        "## U-Net Components\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "### Contracting Path\n",
        "- Convolutional layers\n",
        "- Max pooling\n",
        "- Doubles feature channels\n",
        "- Reduces spatial dimensions\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "### Expansive Path\n",
        "- Upsampling\n",
        "- Feature concatenation\n",
        "- Successive convolutions\n",
        "- Recovers resolution\n",
        ":::\n",
        ":::\n",
        "\n",
        "## U-Net Implementation"
      ],
      "id": "35d11c2b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        return x"
      ],
      "id": "878fd5cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### U-Net Applications\n",
        "\n",
        " \n",
        "### Use Cases\n",
        "- Medical image segmentation\n",
        "- Object detection\n",
        "- Industrial defect detection\n",
        "- General segmentation tasks\n",
        "\n",
        "### Advantages\n",
        "- Works with limited data\n",
        "- Precise localization\n",
        "- End-to-end training\n",
        "- Fast inference\n",
        "\n",
        "\n",
        "\n",
        "## References\n",
        "::: {#refs}\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<script>\n",
        "document.getElementById(\"marimo-frame\").onload = function() {\n",
        "    try {\n",
        "        let iframeDoc = document.getElementById(\"marimo-frame\").contentWindow.document;\n",
        "        let marimoBadge = iframeDoc.querySelector(\"div.fixed.bottom-0.right-0.z-50\");\n",
        "        if (marimoBadge) {\n",
        "            marimoBadge.style.display = \"none\";\n",
        "            console.log(\"Marimo badge hidden successfully.\");\n",
        "        } else {\n",
        "            console.log(\"Badge not found.\");\n",
        "        }\n",
        "    } catch (error) {\n",
        "        console.warn(\"Unable to modify iframe content due to CORS restrictions.\");\n",
        "    }\n",
        "};\n",
        "</script>\n",
        "</div>"
      ],
      "id": "87b65390"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}