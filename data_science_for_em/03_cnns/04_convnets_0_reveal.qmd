---
title: |
  Data Science for Electron Microscopy<br>
  Convolutional Neural Networks 0
bibliography: ref.bib
csl: custom.csl
author:
  - name: Philipp Pelz
    affiliation: 
      - FAU Erlangen-Nürnberg
execute: 
  eval: true
  echo: true
format:
    revealjs: 
        code-fold: true
        code-tools: true
        highlight-style: github
        incremental: true
        width: 1920
        height: 1080
        menu:
            side: right
            width: wide
        template-partials:
            - title-slide.html
        css: custom.css
        theme: custom.scss
        slide-number: c/t    
        logo: "eclipse_logo_small.png" 
        background-transition: fade
        footer: "Philipp Pelz - IMN Retreat 2025 - Roadmap for Scale-Bridging 3D Electron Ptychography"
---

## From Fully Connected Layers to Convolutions

::: {.callout-note}
## Key Points
- MLPs impractical for high-dimensional perceptual data
- One-megapixel image → $10^9$ parameters with 1000 hidden units
- CNNs exploit rich image structure
:::

## Three Key CNN Design Principles

::: {.callout-important}
## Core Principles
1. **Translation Invariance**: Network responds similarly to patterns regardless of location
2. **Locality**: Early layers focus on local regions
3. **Hierarchy**: Deeper layers capture longer-range features
:::

## Invariance in Object Detection

::: {.callout-tip}
## Key Concept
- Recognition should not depend on precise object location
- Illustrated by "Where's Waldo" game
- Waldo's appearance independent of location
- Sweep image with detector for likelihood scores
:::

![An image of the "Where's Waldo" game.](../img/where-wally-walker-books.jpg)

## CNN Design Desiderata

::: {.columns}
::: {.column width="60%"}
1. **Translation Invariance**
   - Early layers respond similarly to same patch
   - Regardless of location in image

2. **Locality**
   - Early layers focus on local regions
   - Aggregate local representations later
:::

::: {.column width="40%"}
3. **Hierarchy**
   - Deeper layers capture longer-range features
   - Similar to higher-level vision in nature
:::
:::

## Constraining the MLP

::: {.callout-note}
## Mathematical Formulation
- Input images $\mathbf{X}$ and hidden representations $\mathbf{H}$ as matrices
- Fourth-order weight tensors $\mathsf{W}$
- With biases $\mathbf{U}$:

$$\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +
\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}$$
:::

## Translation Invariance

::: {.callout-important}
## Key Insight
- Shift in input $\mathbf{X}$ → shift in hidden representation $\mathbf{H}$
- $\mathsf{V}$ and $\mathbf{U}$ independent of $(i, j)$
- Simplified definition:

$$[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$

- This is a *convolution*!
:::

## Locality

::: {.callout-tip}
## Implementation
- Only look near location $(i, j)$
- Set $[\mathbf{V}]_{a, b} = 0$ outside range $|a|> \Delta$ or $|b| > \Delta$
- Rewritten as:

$$[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$

- Reduces parameters from $4 \cdot 10^6$ to $4 \Delta^2$
:::

## Convolutions in Mathematics

::: {.callout-note}
## Definition
- Convolution between functions $f, g: \mathbb{R}^d \to \mathbb{R}$:

$$(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.$$

- For discrete objects (2D tensors):

$$(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).$$
:::

## Channels in CNNs

::: {.callout-important}
## Key Concepts
- Images: 3 channels (RGB)
- Third-order tensors: height × width × channel
- Convolutional filter adapts: $[\mathsf{V}]_{a,b,c}$
- Hidden representations: third-order tensors $\mathsf{H}$
- Feature maps: spatialized learned features
:::

![Detect Waldo.](../img/waldo-mask.jpg){width=400px}

## Multi-Channel Convolution

::: {.callout-tip}
## Complete Formulation
- Input channels: $c_i$
- Output channels: $c_o$
- Kernel shape: $c_o\times c_i\times k_h\times k_w$
- Complete convolution:

$[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c},$

where $d$ indexes output channels
:::

## Summary and Discussion

::: {.callout-note}
## Key Points
- CNNs derived from first principles
- Translation invariance: treat all patches similarly
- Locality: use small neighborhoods
- Channels: restore complexity lost to restrictions
- Hyperspectral images: tens to hundreds of channels
:::

## Exercises

::: {.callout-warning}
## Practice Problems
1. Kernel size $\Delta = 0$ → MLP for each channel
2. Audio data and locality
3. When translation invariance fails
4. CNNs for text data
5. Object at image boundary
6. Convolution symmetry proof
:::

## Convolutions for Images

```{python}
from d2l import torch as d2l
import torch
from torch import nn
```

## The Cross-Correlation Operation

::: {.callout-important}
## Key Points
- Convolutional layers actually perform cross-correlation
- Input tensor and kernel tensor combined
- Window slides across input tensor
- Elementwise multiplication and summation
:::

![Two-dimensional cross-correlation operation](../img/correlation.svg)

## Cross-Correlation Implementation

```{python}
def corr2d(X, K):  #@save
    """Compute 2D cross-correlation."""
    h, w = K.shape
    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))
    return Y
```

## Cross-Correlation Example

```{python}
# Create a 256x256 sparse image with a few 1s
X = torch.zeros((64, 64))
X[11, 23] = 1.0
X[23, 23] = 1.0
X[56, 48] = 1.0
X[19, 54] = 1.0

# Create a 3x3 cross-shaped kernel
K = torch.tensor([[0.0, 1.0, 0.0],
                 [1.0, 1.0, 1.0],
                 [0.0, 1.0, 0.0]])

# Apply the correlation
Y = corr2d(X, K)

import matplotlib.pyplot as plt

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))

# Plot input image X
ax1.imshow(X, cmap='gray')
ax1.set_title('Input Image (X)')
ax1.axis('on')

# Plot kernel K
ax2.imshow(K, cmap='gray')
ax2.set_title('Kernel (K)')
ax2.axis('on')

# Plot output Y
ax3.imshow(Y, cmap='gray')
ax3.set_title('Output (Y)')
ax3.axis('on')

plt.tight_layout()
plt.show()
```

## Convolutional Layers

::: {.callout-note}
## Implementation
- Cross-correlate input and kernel
- Add scalar bias
- Initialize kernels randomly
- Parameters: kernel and scalar bias
:::

```{python}
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

## Edge Detection Example

::: {.callout-important}
## Application
- Detect object edges in images
- Find pixel change locations
- Use special kernel for edge detection
:::

```{python}
X = d2l.ones((6, 8))
X[:, 2:6] = 0
X
```

```{python}
K = d2l.tensor([[1.0, -1.0]])
Y = corr2d(X, K)
Y
```

## Learning a Kernel

::: {.callout-tip}
## Training Process
- Learn kernel from input-output pairs
- Use squared error loss
- Update kernel via gradient descent
:::

```{python}
conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i + 1}, loss {l.sum():.3f}')
```

## Cross-Correlation vs Convolution

::: {.callout-note}
## Key Differences
- Strict convolution: flip kernel horizontally and vertically
- Cross-correlation: use original kernel
- Outputs remain same due to learned kernels
- Term "convolution" used for both operations
:::

## Feature Maps and Receptive Fields

::: {.callout-important}
## Concepts
- Feature map: learned spatial representations
- Receptive field: elements affecting calculation
- Can be larger than input size
- Deeper networks for larger receptive fields
:::

![Figure from Field (1987): Coding with six different channels](../img/field-visual.png){width=600px}

## Summary

::: {.callout-tip}
## Key Points
- Core computation: cross-correlation
- Multiple channels: matrix-matrix operations
- Highly local computation
- Hardware optimization opportunities
- Learnable filters replace feature engineering
:::

## Exercises

::: {.callout-warning}
## Practice Problems
1. Diagonal edges and kernel effects
2. Manual kernel design
3. Gradient computation errors
4. Cross-correlation as matrix multiplication
5. Fast convolution algorithms
6. Block-diagonal matrix multiplication
:::

## Padding and Stride

::: {.callout-note}
## Motivation
- Control output size
- Prevent information loss
- Handle large kernels
- Reduce spatial resolution
:::

```{python}
import torch
from torch import nn
```

## Padding

::: {.callout-important}
## Key Concepts
- Add extra pixels around boundary
- Typically zero padding
- Preserve spatial dimensions
- Common with odd kernel sizes
:::

![Pixel utilization for different convolution sizes](../img/conv-reuse.svg)

## Padding Implementation

```{python}
def comp_conv2d(conv2d, X):
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    return Y.reshape(Y.shape[2:])

conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
```

## Stride

::: {.callout-tip}
## Key Points
- Move window more than one element
- Skip intermediate locations
- Useful for large kernels
- Control output resolution
:::

![Cross-correlation with strides of 3 and 2](../img/conv-stride.svg){width=300px}

## Stride Implementation

```{python}
conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)
comp_conv2d(conv2d, X).shape

conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
```

## Summary and Discussion

::: {.callout-note}
## Key Points
- Padding: control output dimensions
- Stride: reduce resolution
- Zero padding: computational benefits
- Position information encoding
- Alternative padding methods
:::

## Exercises

::: {.callout-warning}
## Practice Problems
1. Output shape calculation
2. Audio signal stride
3. Mirror padding implementation
4. Computational benefits of stride > 1
5. Statistical benefits of stride > 1
6. Stride of 1/2 implementation
:::

## Multiple Input and Output Channels

::: {.callout-important}
## Key Concepts
- RGB images: 3 channels
- Input shape: $3\times h\times w$
- Channel dimension: size 3
- Multiple input/output channels
:::

```{python}
from d2l import torch as d2l
import torch
```

## Multiple Input Channels

::: {.callout-note}
## Implementation
- Kernel matches input channels
- Shape: $c_i\times k_h\times k_w$
- Cross-correlation per channel
- Sum results
:::

```{python}
def corr2d_multi_in(X, K):
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))

X = d2l.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])
K = d2l.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])

corr2d_multi_in(X, K)
```

## Multiple Output Channels

::: {.callout-tip}
## Implementation
- Kernel tensor for each output channel
- Shape: $c_o\times c_i\times k_h\times k_w$
- Concatenate on output channel dimension
:::

```{python}
def corr2d_multi_in_out(X, K):
    return d2l.stack([corr2d_multi_in(X, k) for k in K], 0)

K = d2l.stack((K, K + 1, K + 2), 0)
K.shape

corr2d_multi_in_out(X, K)
```

## $1\times 1$ Convolutional Layer

::: {.callout-important}
## Key Points
- No spatial correlation
- Channel dimension computation
- Linear combination at each position
- Fully connected layer per pixel
:::

![$1\times 1$ convolution with 3 input and 2 output channels](../img/conv-1x1.svg)

## $1\times 1$ Convolution Implementation

```{python}
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = d2l.reshape(X, (c_i, h * w))
    K = d2l.reshape(K, (c_o, c_i))
    Y = d2l.matmul(K, X)
    return d2l.reshape(Y, (c_o, h, w))

X = d2l.normal(0, 1, (3, 3, 3))
K = d2l.normal(0, 1, (2, 3, 1, 1))
Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
assert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) < 1e-6
```

## Discussion

::: {.callout-note}
## Key Points
- Channels combine MLP and CNN benefits
- Trade-off: parameter reduction vs. model expressiveness
- Computational cost: $\mathcal{O}(h \cdot w \cdot k^2 \cdot c_i \cdot c_o)$
- Example: 256×256 image, 5×5 kernel, 128 channels → 53B operations
:::

## Exercises

::: {.callout-warning}
## Practice Problems
1. Convolution kernel composition
2. Computational and memory costs
3. Channel doubling effects
4. Variable comparison
5. Convolution as matrix multiplication
6. Fast convolution algorithms
7. Block-diagonal matrix multiplication
:::

## Pooling

::: {.callout-important}
## Motivation
- Global questions about images
- Gradual information aggregation
- Translation invariance
- Spatial downsampling
:::

```{python}
from d2l import torch as d2l
import torch
from torch import nn
```

## Maximum and Average Pooling

::: {.callout-note}
## Key Concepts
- Fixed-shape window
- No parameters
- Deterministic operations
- Maximum or average value
:::

![Max-pooling with $2\times 2$ window](../img/pooling.svg){width=300px}

## Pooling Implementation

```{python}
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = d2l.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y

X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
pool2d(X, (2, 2))
pool2d(X, (2, 2), 'avg')
```

## Padding and Stride in Pooling

::: {.callout-tip}
## Implementation
- Adjust output shape
- Default: matching window and stride
- Manual specification possible
- Rectangular windows supported
:::

```{python}
X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 1, 4, 4))
X

pool2d = nn.MaxPool2d(3)
pool2d(X)

pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)

pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))
pool2d(X)
```

## Multiple Channels in Pooling

::: {.callout-important}
## Key Points
- Pool each channel separately
- Maintain channel count
- Independent operations
:::

```{python}
X = d2l.concat((X, X + 1), 1)
X

pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```

## Summary

::: {.callout-note}
## Key Points
- Simple aggregation operation
- Standard convolution semantics
- Channel independence
- Max-pooling preferred
- $2 \times 2$ window common
- Alternative pooling methods
:::

## Exercises

::: {.callout-warning}
## Practice Problems
1. Average pooling via convolution
2. Max-pooling impossibility proof
3. Max-pooling with ReLU
4. Computational cost analysis
5. Max vs average pooling
6. Minimum pooling alternatives
7. Softmax pooling
:::

## LeNet

::: {.callout-important}
## Historical Context
- First published CNNs
- Yann LeCun, AT&T Bell Labs
- Handwritten digit recognition
- < 1% error rate
- ATM digit processing
:::

```{python}
from d2l import torch as d2l
import torch
from torch import nn
```

## LeNet Architecture

::: {.callout-note}
## Components
1. Convolutional encoder
   - Two convolutional layers
2. Dense block
   - Three fully connected layers
:::

![LeNet data flow](../img/lenet.svg){center=True width=600px background-color=white}

## LeNet Implementation

```{python}
def init_cnn(module):
    if type(module) == nn.Linear or type(module) == nn.Conv2d:
        nn.init.xavier_uniform_(module.weight)

class LeNet(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),
            nn.AvgPool2d(kernel_size=2, stride=2),
            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),
            nn.AvgPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.LazyLinear(120), nn.Sigmoid(),
            nn.LazyLinear(84), nn.Sigmoid(),
            nn.LazyLinear(num_classes))
```

## Layer Summary

```{python}
@d2l.add_to_class(d2l.Classifier)
def layer_summary(self, X_shape):
    X = d2l.randn(*X_shape)
    for layer in self.net:
        X = layer(X)
        print(layer.__class__.__name__, 'output shape:\t', X.shape)
        
model = LeNet()
model.layer_summary((1, 1, 28, 28))
```

## Training LeNet

```{python}
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128)
model = LeNet(lr=0.1)
model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)
trainer.fit(model, data)
```

## Summary

::: {.callout-tip}
## Key Points
- Evolution from MLPs to CNNs
- LeNet-5 remains relevant
- Similar to modern architectures
- Implementation ease
- Democratized deep learning
:::

## References
::: {#refs}
::: 