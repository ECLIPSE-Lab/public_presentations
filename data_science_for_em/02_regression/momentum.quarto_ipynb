{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ---\n",
        "title: \"Momentum in Optimization\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Momentum in Optimization\n",
        "\n",
        "- Momentum is a key optimization technique in deep learning\n",
        "- Addresses challenges in stochastic gradient descent:\n",
        "  - Learning rate sensitivity\n",
        "  - Convergence issues\n",
        "  - Noise handling\n",
        "- Particularly effective for ill-conditioned problems\n",
        "\n",
        "## Basics of Momentum\n",
        "\n",
        "### Leaky Averages\n",
        "\n",
        "- Minibatch SGD averages gradients to reduce variance\n",
        "- Momentum extends this concept using \"leaky averages\":\n",
        "  - Accumulates past gradients\n",
        "  - Weights recent gradients more heavily\n",
        "  - Formula: $\\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}$\n",
        "  - $\\beta \\in (0, 1)$ controls the \"memory\" of past gradients\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Key Benefits\n",
        "\n",
        "- Accelerates convergence\n",
        "- Particularly effective for:\n",
        "  - Ill-conditioned problems\n",
        "  - Narrow canyons in optimization landscape\n",
        "- Provides more stable descent directions\n",
        "- Works well with both:\n",
        "  - Noise-free convex problems\n",
        "  - Stochastic gradient descent\n",
        "\n",
        "## Visualizing the Problem\n",
        "\n",
        "Let's examine an ill-conditioned problem:\n"
      ],
      "id": "422f488c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import torch\n",
        "\n",
        "eta = 0.4\n",
        "def f_2d(x1, x2):\n",
        "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
        "def gd_2d(x1, x2, s1, s2):\n",
        "    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n",
        "\n",
        "d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Challenge\n",
        "\n",
        "- Function $f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2$ is very flat in $x_1$ direction\n",
        "- Gradient in $x_2$ direction:\n",
        "  - Much higher\n",
        "  - Changes more rapidly\n",
        "- Trade-off in learning rate:\n",
        "  - Small rate: Slow convergence in $x_1$\n",
        "  - Large rate: Divergence in $x_2$\n",
        "\n",
        "## Momentum Method\n",
        "\n",
        "### Update Equations\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{v}_t &\\leftarrow \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}, \\\\\n",
        "\\mathbf{x}_t &\\leftarrow \\mathbf{x}_{t-1} - \\eta_t \\mathbf{v}_t.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "### Implementation\n"
      ],
      "id": "6851b282"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: momentum-implementation\n",
        "def momentum_2d(x1, x2, v1, v2):\n",
        "    v1 = beta * v1 + 0.2 * x1\n",
        "    v2 = beta * v2 + 4 * x2\n",
        "    return x1 - eta * v1, x2 - eta * v2, v1, v2\n",
        "\n",
        "eta, beta = 0.6, 0.5\n",
        "d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"
      ],
      "id": "momentum-implementation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Effect of Momentum Parameter\n",
        "\n",
        "- $\\beta = 0.5$: Good convergence\n",
        "- $\\beta = 0.25$: Barely converges but better than no momentum\n",
        "- $\\beta = 0$: Reduces to regular gradient descent\n"
      ],
      "id": "d3f300c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: momentum-beta\n",
        "eta, beta = 0.6, 0.25\n",
        "d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"
      ],
      "id": "momentum-beta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Effective Sample Weight\n",
        "\n",
        "- Sum of weights: $\\sum_{\\tau=0}^\\infty \\beta^\\tau = \\frac{1}{1-\\beta}$\n",
        "- Step size effectively becomes $\\frac{\\eta}{1-\\beta}$\n",
        "- Better behaved descent direction\n"
      ],
      "id": "51f3d150"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: effective-weight\n",
        "d2l.set_figsize()\n",
        "betas = [0.95, 0.9, 0.6, 0]\n",
        "for beta in betas:\n",
        "    x = d2l.numpy(d2l.arange(40))\n",
        "    d2l.plt.plot(x, beta ** x, label=f'beta = {beta:.2f}')\n",
        "d2l.plt.xlabel('time')\n",
        "d2l.plt.legend();"
      ],
      "id": "effective-weight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Implementation\n",
        "\n",
        "### From Scratch\n"
      ],
      "id": "340bd267"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: momentum-scratch\n",
        "def init_momentum_states(feature_dim):\n",
        "    v_w = d2l.zeros((feature_dim, 1))\n",
        "    v_b = d2l.zeros(1)\n",
        "    return (v_w, v_b)\n",
        "\n",
        "def sgd_momentum(params, states, hyperparams):\n",
        "    for p, v in zip(params, states):\n",
        "        with torch.no_grad():\n",
        "            v[:] = hyperparams['momentum'] * v + p.grad\n",
        "            p[:] -= hyperparams['lr'] * v\n",
        "        p.grad.data.zero_()"
      ],
      "id": "momentum-scratch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training with Different Parameters\n"
      ],
      "id": "feda8b15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: momentum-training\n",
        "def train_momentum(lr, momentum, num_epochs=2):\n",
        "    d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),\n",
        "                   {'lr': lr, 'momentum': momentum}, data_iter,\n",
        "                   feature_dim, num_epochs)\n",
        "\n",
        "data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n",
        "train_momentum(0.02, 0.5)"
      ],
      "id": "momentum-training",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Theoretical Analysis\n",
        "\n",
        "### Quadratic Convex Functions\n",
        "\n",
        "- General form: $h(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x} + \\mathbf{x}^\\top \\mathbf{c} + b$\n",
        "- For positive definite $\\mathbf{Q}$:\n",
        "  - Minimizer at $\\mathbf{x}^* = -\\mathbf{Q}^{-1} \\mathbf{c}$\n",
        "  - Minimum value: $b - \\frac{1}{2} \\mathbf{c}^\\top \\mathbf{Q}^{-1} \\mathbf{c}$\n",
        "- Gradient: $\\partial_{\\mathbf{x}} h(\\mathbf{x}) = \\mathbf{Q} (\\mathbf{x} - \\mathbf{Q}^{-1} \\mathbf{c})$\n",
        "\n",
        "---\n",
        "\n",
        "### Convergence Analysis\n",
        "\n",
        "- For scalar function $f(x) = \\frac{\\lambda}{2} x^2$:\n",
        "  - Gradient descent: $x_{t+1} = (1 - \\eta \\lambda) x_t$\n",
        "  - Convergence when $|1 - \\eta \\lambda| < 1$\n",
        "  - Exponential convergence rate\n"
      ],
      "id": "751bf671"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: convergence-analysis\n",
        "lambdas = [0.1, 1, 10, 19]\n",
        "eta = 0.1\n",
        "d2l.set_figsize((6, 4))\n",
        "for lam in lambdas:\n",
        "    t = d2l.numpy(d2l.arange(20))\n",
        "    d2l.plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')\n",
        "d2l.plt.xlabel('time')\n",
        "d2l.plt.legend();"
      ],
      "id": "convergence-analysis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Momentum replaces gradients with leaky averages\n",
        "- Key benefits:\n",
        "  - Accelerates convergence\n",
        "  - Works for both noise-free and noisy gradients\n",
        "  - Prevents optimization stalling\n",
        "  - Effective sample size: $\\frac{1}{1-\\beta}$\n",
        "- Implementation requires:\n",
        "  - Additional state vector (velocity)\n",
        "  - Careful parameter tuning\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with different momentum and learning rate combinations\n",
        "2. Analyze gradient descent and momentum for quadratic problems with multiple eigenvalues\n",
        "3. Derive minimum value and minimizer for $h(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x} + \\mathbf{x}^\\top \\mathbf{c} + b$\n",
        "4. Investigate behavior with stochastic gradient descent and minibatch variants "
      ],
      "id": "4844dfd5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}