---
title: |
  Data Science for Electron Microscopy<br>
  Lecture 2: Optimization, Regression, Sensor Fusion
bibliography: ref.bib
# csl: custom.csl
author:
  - name: Prof. Dr. Philipp Pelz
    affiliation: 
      - FAU Erlangen-Nürnberg
      - Institute of Micro- and Nanostructure Research

execute: 
  eval: true
  echo: true
format:
    revealjs: 
        # scroll-view:
        #     activate: true
        #     snap: mandatory
        #     layout: full
        width: 1920
        height: 1080
        menu:
            side: right
            width: wide
        template-partials:
            - title-slide.html
        css: custom.css
        theme: custom.scss
        slide-number: c/t    
        logo: "eclipse_logo_small.png" 
        highlight-style: a11y
        incremental: false 
        background-transition: fade
        footer: "©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy"
---



<!-- ## Outline

::: {.outline-container}

::: {.outline-box .fragment}
### Formalities
![](02_imaging.png)
:::

::: {.outline-box .fragment}
### Introduction <br>to<br> Electron<br> Microscopy<br> Data
![](02_imaging.png)
:::

::: {.outline-box .fragment}
### Basic Pytorch<br> Knowledge
![](02_imaging.png)
:::

::: {.outline-box .fragment}
### .
![](02_imaging.png) 
:::-->


## Optimization and Deep Learning

- Loss functions quantify model performance
  - Also called objective functions
  - Most optimization algorithms minimize (not maximize)
  - To maximize: just flip the sign

## Linear Regression

- Predicts numerical values
  - House prices
  - Hospital stay length
  - Retail demand
- Key terminology:
  - Training dataset: collection of examples
  - Features: input variables (e.g., area, age)
  - Labels: target values to predict (e.g., price)

## Model Basics

- Assumes linear relationship between features and target
- Key components:
  - Weights ($w$): influence of each feature
  - Bias ($b$): base value when features are zero
- Mathematical form:
  $$\hat{y} = \mathbf{w}^\top \mathbf{x} + b$$

## Loss Function

- Measures prediction quality
- Squared error for each example:
  $$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} (\hat{y}^{(i)} - y^{(i)})^2$$
- Total loss: average over all examples
- Quadratic form:
  - Penalizes large errors heavily
  - Can be sensitive to outliers

## Training Methods

1. Analytic Solution
   - Direct formula when possible
   - $\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}$
   - Only works for simple models

2. Minibatch Stochastic Gradient Descent
   - More practical for complex models
   - Process:
     - Sample random minibatch
     - Compute gradient
     - Update parameters
   - Key hyperparameters:
     - Learning rate ($\eta$)
     - Minibatch size (typically 32-256)

## Vectorization

- Critical for efficient computation
- Example: Vector addition
  ```python
  # Slow: for-loop
  for i in range(n):
      c[i] = a[i] + b[i]
  
  # Fast: vectorized
  d = a + b
  ```
- Benefits:
  - Order-of-magnitude speedups
  - Reduced code complexity
  - Better portability

## Statistical Motivation

- Normal distribution connection:
  $$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right)$$
- Maximum likelihood interpretation:
  - Assumes Gaussian noise
  - Minimizing squared error = maximizing likelihood
  - Provides theoretical foundation

## Linear Regression as Neural Network

- Single-layer neural network
- Structure:
  - Input layer: features ($x_1, ..., x_d$)
  - Output layer: single neuron
  - Direct connections (no hidden layers)
- Foundation for more complex networks

## Summary

- Linear regression: foundation of machine learning
- Key components:
  - Parametric form
  - Differentiable objective
  - Optimization methods
  - Evaluation metrics
- Building block for more complex models

## Exercises

1. Assume that we have some data $x_1, \ldots, x_n \in \mathbb{R}$. Our goal is to find a constant $b$ such that $\sum_i (x_i - b)^2$ is minimized.
    1. Find an analytic solution for the optimal value of $b$.
    1. How does this problem and its solution relate to the normal distribution?
    1. What if we change the loss from $\sum_i (x_i - b)^2$ to $\sum_i |x_i-b|$? Can you find the optimal solution for $b$?
1. Prove that the affine functions that can be expressed by $\mathbf{x}^\top \mathbf{w} + b$ are equivalent to linear functions on $(\mathbf{x}, 1)$.
1. Assume that you want to find quadratic functions of $\mathbf{x}$, i.e., $f(\mathbf{x}) = b + \sum_i w_i x_i + \sum_{j \leq i} w_{ij} x_{i} x_{j}$. How would you formulate this in a deep network?

--- 


2. Recall that one of the conditions for the linear regression problem to be solvable was that the design matrix $\mathbf{X}^\top \mathbf{X}$ has full rank.
    1. What happens if this is not the case?
    2. How could you fix it? What happens if you add a small amount of coordinate-wise independent Gaussian noise to all entries of $\mathbf{X}$?
    3. What is the expected value of the design matrix $\mathbf{X}^\top \mathbf{X}$ in this case?
    4. What happens with stochastic gradient descent when $\mathbf{X}^\top \mathbf{X}$ does not have full rank?
3. Assume that the noise model governing the additive noise $\epsilon$ is the exponential distribution. That is, $p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$.
    1. Write out the negative log-likelihood of the data under the model $-\log P(\mathbf y \mid \mathbf X)$.
    2. Can you find a closed form solution?
    3. Suggest a minibatch stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint: what happens near the stationary point as we keep on updating the parameters)? Can you fix this?
4. Assume that we want to design a neural network with two layers by composing two linear layers. That is, the output of the first layer becomes the input of the second layer. Why would such a naive composition not work?

--- 


5. What happens if you want to use regression for realistic price estimation of houses or stock prices?
    1. Show that the additive Gaussian noise assumption is not appropriate. Hint: can we have negative prices? What about fluctuations?
    2. Why would regression to the logarithm of the price be much better, i.e., $y = \log \textrm{price}$?
    3. What do you need to worry about when dealing with pennystock, i.e., stock with very low prices? Hint: can you trade at all possible prices? Why is this a bigger problem for cheap stock? For more information review the celebrated Black--Scholes model for option pricing :cite:`Black.Scholes.1973`.
6. Suppose we want to use regression to estimate the *number* of apples sold in a grocery store.
    1. What are the problems with a Gaussian additive noise model? Hint: you are selling apples, not oil.
    2. The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) captures distributions over counts. It is given by $p(k \mid \lambda) = \lambda^k e^{-\lambda}/k!$. Here $\lambda$ is the rate function and $k$ is the number of events you see. Prove that $\lambda$ is the expected value of counts $k$.
    3. Design a loss function associated with the Poisson distribution.
    4. Design a loss function for estimating $\log \lambda$ instead.

## Goal of Optimization

- Different goals in optimization vs deep learning:
  - Optimization: Minimize objective function
  - Deep learning: Find suitable model with finite data
- Key concepts:
  - Empirical risk: Average loss on training data
  - Risk: Expected loss on entire population

```{python}
%matplotlib inline
import numpy as np
from mpl_toolkits import mplot3d
import torch
import d2l
```

---

- To illustrate different goals:
  - Empirical risk: Average loss on training dataset
  - Risk: Expected loss on entire population
- Two functions:
  - Risk function `f`
  - Empirical risk function `g` (less smooth due to finite data)

```{python}
def f(x):
    return x * torch.cos(np.pi * x)

def g(x):
    return f(x) + 0.2 * torch.cos(5 * np.pi * x)
```

---

- Visualization shows:
  - Minimum of empirical risk ≠ minimum of risk
  - Training data minimum may differ from true minimum

```{python}
def annotate(text, xy, xytext):  #@save
    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
                           arrowprops=dict(arrowstyle='->'))

x = d2l.arange(0.5, 1.5, 0.01)
d2l.set_figsize((4.5, 2.5))
d2l.plot(x, [f(x), g(x)], 'x', 'risk')
annotate('min of\nempirical risk', (1.0, -1.2), (0.5, -1.1))
annotate('min of risk', (1.1, -1.05), (0.95, -0.5))
```

## Optimization Challenges in Deep Learning

- Focus on optimization algorithm performance
- Most objective functions:
  - Are complicated
  - Lack analytical solutions
  - Require numerical optimization
- Three major challenges:
  1. Local minima
  2. Saddle points
  3. Vanishing gradients

## Local Minima

- Definition:
  - $f(x)$ is local minimum if smaller than nearby points
  - Global minimum: smallest value over entire domain
- Example function:
  $$f(x) = x \cdot \textrm{cos}(\pi x) \textrm{ for } -1.0 \leq x \leq 2.0$$

```{python}
x = torch.arange(-1.0, 2.0, 0.01)
d2l.plot(x, [f(x), ], 'x', 'f(x)')
annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))
annotate('global minimum', (1.1, -0.95), (0.6, 0.8))
```

---

- In deep learning:
  - Models often have many local optima
  - Gradient approaches zero near local minimum
  - Minibatch SGD can help escape local minima
    - Natural gradient variation provides "noise"
    - Can dislodge parameters from local minima

## Saddle Points

- Characteristics:
  - All gradients vanish
  - Neither global nor local minimum
- Example: $f(x) = x^3$
  - First and second derivatives vanish at $x=0$
  - Optimization can stall here

```{python}
x = d2l.arange(-2.0, 2.0, 0.01)
d2l.plot(x, [x**3], 'x', 'f(x)')
annotate('saddle point', (0, -0.2), (-0.52, -5.0))
```

---

::: {.columns}

::: {.column width="40%"}
- Higher dimensions:
  - More complex (e.g., $f(x,y) = x^2 - y^2$)
  - More likely to encounter saddle points
  - Hessian matrix eigenvalues determine type:
    - All positive: local minimum
    - All negative: local maximum
    - Mixed signs: saddle point
:::

::: {.column width="60%"}

```{python}
x, y = d2l.meshgrid(
    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))
z = x**2 - y**2

ax = d2l.plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})
ax.plot([0], [0], [0], 'rx')
ticks = [-1, 0, 1]
d2l.plt.xticks(ticks)
d2l.plt.yticks(ticks)
ax.set_zticks(ticks)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y');
```

:::
:::
## Vanishing Gradients

- Most insidious optimization problem
- Example: $f(x) = \tanh(x)$
  - At $x = 4$: gradient ≈ 0.0013
  - Optimization stalls
- Historical context:
  - Major challenge before ReLU activation
  - Made deep learning training difficult

```{python}
x = d2l.arange(-2.0, 5.0, 0.01)
d2l.plot(x, [d2l.tanh(x)], 'x', 'f(x)')
annotate('vanishing gradient', (4, 1), (2, 0.0))
```

## Summary

- Key takeaways:
  - Training error minimization ≠ best generalization
  - Many local minima exist
  - Saddle points are common in non-convex problems
  - Vanishing gradients can stall optimization
- Good news:
  - Robust algorithms exist
  - Perfect solutions not always necessary
  - Local optima can be useful
  - Many practical solutions available


## Exercises

1. Consider a simple MLP with a single hidden layer of, say, $d$ dimensions in the hidden layer and a single output. Show that for any local minimum there are at least $d!$ equivalent solutions that behave identically.
1. Assume that we have a symmetric random matrix $\mathbf{M}$ where the entries
   $M_{ij} = M_{ji}$ are each drawn from some probability distribution
   $p_{ij}$. Furthermore assume that $p_{ij}(x) = p_{ij}(-x)$, i.e., that the
   distribution is symmetric (see e.g., `Wigner.1958` for details).
    1. Prove that the distribution over eigenvalues is also symmetric. That is, for any eigenvector $\mathbf{v}$ the probability that the associated eigenvalue $\lambda$ satisfies $P(\lambda > 0) = P(\lambda < 0)$.
    1. Why does the above *not* imply $P(\lambda > 0) = 0.5$?
1. What other challenges involved in deep learning optimization can you think of?
1. Assume that you want to balance a (real) ball on a (real) saddle.
    1. Why is this hard?
    1. Can you exploit this effect also for optimization algorithms?

## Application: Sensor Fusion



## References
::: {#refs}
:::










































<script>
document.getElementById("marimo-frame").onload = function() {
    try {
        let iframeDoc = document.getElementById("marimo-frame").contentWindow.document;
        let marimoBadge = iframeDoc.querySelector("div.fixed.bottom-0.right-0.z-50");
        if (marimoBadge) {
            marimoBadge.style.display = "none";
            console.log("Marimo badge hidden successfully.");
        } else {
            console.log("Badge not found.");
        }
    } catch (error) {
        console.warn("Unable to modify iframe content due to CORS restrictions.");
    }
};
</script>
</div>