<!-- ---
title: "Optimization and Deep Learning"
format: 
  revealjs:
    theme: custom.scss
    css: custom.css
    width: 1920
    height: 1080
    menu:
      side: right
      width: wide
    template-partials:
      - title-slide.html
    slide-number: c/t
    logo: "eclipse_logo_small.png"
    highlight-style: a11y
    incremental: false
    background-transition: fade
    footer: "©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy"
execute:
  eval: true
  echo: true
--- -->

# Optimization and Deep Learning

- Optimization and deep learning are closely related
- Deep learning typically involves:
  - Defining a loss function
  - Using optimization to minimize the loss
- Note: Most optimization algorithms minimize by convention
  - To maximize: simply flip the sign of the objective

## Goals of Optimization vs Deep Learning

::: {.columns}

::: {.column width="50%"}
### Optimization
- Primary goal: Minimize objective function
- Focus on training error
- Direct mathematical approach
:::

::: {.column width="50%"}
### Deep Learning
- Primary goal: Find suitable model
- Focus on generalization error
- Must handle finite data
- Must prevent overfitting
:::

:::

## Visualizing the Difference

Let's examine empirical risk vs. risk:

```{python}
#| label: setup3
#| code-fold: true
%matplotlib inline
import d2l
import numpy as np
from mpl_toolkits import mplot3d
import torch

def f(x):
    return x * d2l.cos(np.pi * x)

def g(x):
    return f(x) + 0.2 * d2l.cos(5 * np.pi * x)

def annotate(text, xy, xytext):
    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
                           arrowprops=dict(arrowstyle='->'))

x = d2l.arange(0.5, 1.5, 0.01)
d2l.set_figsize((4.5, 2.5))
d2l.plot(x, [f(x), g(x)], 'x', 'risk')
annotate('min of\nempirical risk', (1.0, -1.2), (0.5, -1.1))
annotate('min of risk', (1.1, -1.05), (0.95, -0.5))
```

## Key Challenges in Deep Learning Optimization

1. Local Minima
2. Saddle Points
3. Vanishing Gradients

## Local Minima

- Definition: Point where function value is smaller than nearby points
- Global minimum: Smallest value over entire domain
- Example function: $f(x) = x \cdot \textrm{cos}(\pi x)$

```{python}
#| label: local-minima1
#| code-fold: true
x = d2l.arange(-1.0, 2.0, 0.01)
d2l.plot(x, [f(x), ], 'x', 'f(x)')
annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))
annotate('global minimum', (1.1, -0.95), (0.6, 0.8))
```

## Impact of Local Minima

- Deep learning models often have many local optima
- Gradient approaches zero near local minimum
- Minibatch SGD can help escape local minima
  - Natural gradient variation provides "noise"
  - Can dislodge parameters from local minima

## Saddle Points

- Characteristics:
  - All gradients vanish
  - Neither global nor local minimum
- Example: $f(x) = x^3$
  - First and second derivatives vanish at $x=0$
  - Optimization can stall here

```{python}
#| label: saddle-points
#| code-fold: true
x = d2l.arange(-2.0, 2.0, 0.01)
d2l.plot(x, [x**3], 'x', 'f(x)')
annotate('saddle point', (0, -0.2), (-0.52, -5.0))
```

## Higher-Dimensional Saddle Points

- More complex in higher dimensions
- Example: $f(x,y) = x^2 - y^2$
- Has saddle point at $(0,0)$
  - Maximum with respect to $y$
  - Minimum with respect to $x$

```{python}
#| label: 3d-saddle
#| code-fold: true
x, y = d2l.meshgrid(
    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))
z = x**2 - y**2

ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})
ax.plot([0], [0], [0], 'rx')
ticks = [-1, 0, 1]
d2l.plt.xticks(ticks)
d2l.plt.yticks(ticks)
ax.set_zticks(ticks)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
```

## Hessian Matrix Analysis

For a $k$-dimensional input vector:

- All positive eigenvalues → Local minimum
- All negative eigenvalues → Local maximum
- Mixed signs → Saddle point

## Vanishing Gradients

- Most insidious optimization problem
- Example: $f(x) = \tanh(x)$
  - At $x = 4$: gradient ≈ 0.0013
  - Optimization stalls
- Historical context:
  - Major challenge before ReLU activation
  - Made deep learning training difficult

```{python}
#| label: vanishing-gradients
#| code-fold: true
x = d2l.arange(-2.0, 5.0, 0.01)
d2l.plot(x, [d2l.tanh(x)], 'x', 'f(x)')
annotate('vanishing gradient', (4, 1), (2, 0.0))
```

## Summary

- Key takeaways:
  - Training error minimization ≠ best generalization
  - Many local minima exist
  - Saddle points are common in non-convex problems
  - Vanishing gradients can stall optimization
- Good news:
  - Robust algorithms exist
  - Perfect solutions not always necessary
  - Local optima can be useful
  - Many practical solutions available

## Exercises

1. Consider a simple MLP with a single hidden layer of $d$ dimensions:
   - Show that for any local minimum there are at least $d!$ equivalent solutions
   - Why does this happen?

2. For a symmetric random matrix $\mathbf{M}$:
   - Prove that eigenvalue distribution is symmetric
   - Why doesn't this imply $P(\lambda > 0) = 0.5$?

3. Additional challenges in deep learning optimization?

4. Balancing a ball on a saddle:
   - Why is this hard?
   - How might this relate to optimization algorithms? 