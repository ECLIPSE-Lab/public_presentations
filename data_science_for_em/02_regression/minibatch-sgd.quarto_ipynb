{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ---\n",
        "title: \"\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Minibatch Stochastic Gradient Descent\n",
        "\n",
        "- Two extremes in gradient-based learning:\n",
        "  - Full dataset (gradient descent)\n",
        "  - Single examples (stochastic gradient descent)\n",
        "- Each approach has drawbacks:\n",
        "  - Gradient descent: Not data efficient for similar data\n",
        "  - SGD: Not computationally efficient (poor vectorization)\n",
        "- Minibatch SGD offers a middle ground\n",
        "\n",
        "## Vectorization and Caches\n",
        "\n",
        "### Hardware Considerations\n",
        "\n",
        "- Multiple GPUs and servers require larger minibatches\n",
        "  - 8 GPUs × 16 servers = minimum batch size of 128\n",
        "- Single GPU/CPU considerations:\n",
        "  - Multiple memory types (registers, L1/L2/L3 cache)\n",
        "  - Different bandwidth constraints\n",
        "  - Memory access patterns matter\n",
        "\n",
        "### Performance Metrics\n",
        "\n",
        "- Modern CPU capabilities:\n",
        "  - 2GHz CPU with 16 cores and AVX-512\n",
        "  - Can process up to 10¹² bytes/second\n",
        "- GPU capabilities:\n",
        "  - 100× better than CPU\n",
        "- Memory bandwidth limitations:\n",
        "  - Midrange server: ~100 GB/s\n",
        "  - Memory access width: 64-384 bit\n",
        "\n",
        "## Matrix Multiplication Strategies\n",
        "\n",
        "### Different Approaches\n",
        "\n",
        "1. Element-wise computation\n",
        "2. Column-wise computation\n",
        "3. Full matrix multiplication\n",
        "4. Block-wise computation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Performance Comparison\n"
      ],
      "id": "e5acc88f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: matrix-mult\n",
        " \n",
        "import d2l\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return torch.tensor(self.times).cumsum().tolist()\n",
        "\n",
        "# Initialize matrices\n",
        "A = torch.zeros(256, 256)\n",
        "B = torch.randn(256, 256)\n",
        "C = torch.randn(256, 256)\n",
        "timer = Timer()"
      ],
      "id": "matrix-mult",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Element-wise Computation\n"
      ],
      "id": "a02a334f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: element-wise\n",
        "# Compute A = BC one element at a time\n",
        "timer.start()\n",
        "for i in range(256):\n",
        "    for j in range(256):\n",
        "        A[i, j] = torch.dot(B[i, :], C[:, j])\n",
        "timer.stop()"
      ],
      "id": "element-wise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Column-wise Computation\n"
      ],
      "id": "cc7a37fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: column-wise\n",
        "# Compute A = BC one column at a time\n",
        "timer.start()\n",
        "for j in range(256):\n",
        "    A[:, j] = torch.mv(B, C[:, j])\n",
        "timer.stop()"
      ],
      "id": "column-wise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Full Matrix Multiplication\n"
      ],
      "id": "78ea5ab2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: full-matrix\n",
        "# Compute A = BC in one go\n",
        "timer.start()\n",
        "A = torch.mm(B, C)\n",
        "timer.stop()\n",
        "\n",
        "gigaflops = [0.03 / i for i in timer.times]\n",
        "print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n",
        "      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')"
      ],
      "id": "full-matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minibatch Processing\n",
        "\n",
        "### Why Use Minibatches?\n",
        "\n",
        "- Computational efficiency\n",
        "- Statistical properties:\n",
        "  - Maintains gradient expectation\n",
        "  - Reduces variance by factor of $b^{-\\frac{1}{2}}$\n",
        "  - $b$ = batch size\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Batch Size Trade-offs\n",
        "\n",
        "- Too small:\n",
        "  - Poor computational efficiency\n",
        "  - High variance\n",
        "- Too large:\n",
        "  - Diminishing returns in variance reduction\n",
        "  - Memory constraints\n",
        "- Optimal: Balance between:\n",
        "  - Computational efficiency\n",
        "  - Statistical efficiency\n",
        "  - Available memory\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Implementation\n",
        "\n",
        "### Data Loading\n"
      ],
      "id": "847c50a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data-loading\n",
        "#@save\n",
        "d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n",
        "                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n",
        "\n",
        "#@save\n",
        "def get_data_ch11(batch_size=10, n=1500):\n",
        "    data = np.genfromtxt(d2l.download('airfoil'),\n",
        "                         dtype=np.float32, delimiter='\\t')\n",
        "    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))\n",
        "    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\n",
        "                               batch_size, is_train=True)\n",
        "    return data_iter, data.shape[1]-1"
      ],
      "id": "data-loading",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Function\n"
      ],
      "id": "c3eb9463"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: training-fn\n",
        "#@save\n",
        "def sgd(params, states, hyperparams):\n",
        "    for p in params:\n",
        "        p.data.sub_(hyperparams['lr'] * p.grad)\n",
        "        p.grad.data.zero_()"
      ],
      "id": "training-fn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Training Function\n"
      ],
      "id": "5d794d0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: training-fn1\n",
        "\n",
        "def train_ch11(trainer_fn, states, hyperparams, data_iter,\n",
        "               feature_dim, num_epochs=2):\n",
        "    # Initialization\n",
        "    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\n",
        "                     requires_grad=True)\n",
        "    b = torch.zeros((1), requires_grad=True)\n",
        "    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
        "    # Train\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
        "    n, timer = 0, d2l.Timer()\n",
        "    for _ in range(num_epochs):\n",
        "        for X, y in data_iter:\n",
        "            l = loss(net(X), y).mean()\n",
        "            l.backward()\n",
        "            trainer_fn([w, b], states, hyperparams)\n",
        "            n += X.shape[0]\n",
        "            if n % 200 == 0:\n",
        "                timer.stop()\n",
        "                animator.add(n/X.shape[0]/len(data_iter),\n",
        "                             (d2l.evaluate_loss(net, data_iter, loss),))\n",
        "                timer.start()\n",
        "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n",
        "    return timer.cumsum(), animator.Y[0]"
      ],
      "id": "training-fn1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison\n",
        "\n",
        "### Different Batch Sizes\n",
        "::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n"
      ],
      "id": "e7bc9067"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: batch-comparison1\n",
        "def train_sgd(lr, batch_size, num_epochs=2):\n",
        "    data_iter, feature_dim = get_data_ch11(batch_size)\n",
        "    return train_ch11(\n",
        "        sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)\n",
        "\n",
        "# Compare different approaches\n",
        "gd_res = train_sgd(1, 1500, 10)  # Full batch\n",
        "sgd_res = train_sgd(0.005, 1)    # Single example\n",
        "d2l.set_figsize([6, 3])\n",
        "d2l.plot(*list(map(list, zip(gd_res, sgd_res))),\n",
        "         'time (sec)', 'loss', xlim=[1e-2, 10],\n",
        "         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\n",
        "d2l.plt.gca().set_xscale('log')"
      ],
      "id": "batch-comparison1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n"
      ],
      "id": "1059152c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: batch-comparison2\n",
        "mini1_res = train_sgd(.4, 100)   # Medium batch\n",
        "mini2_res = train_sgd(.05, 10)   # Small batch\n",
        "\n",
        "# Plot results\n",
        "d2l.set_figsize([6, 3])\n",
        "d2l.plot(*list(map(list, zip( mini1_res, mini2_res))),\n",
        "         'time (sec)', 'loss', xlim=[1e-2, 10],\n",
        "         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\n",
        "d2l.plt.gca().set_xscale('log')"
      ],
      "id": "batch-comparison2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        ":::\n",
        "## Summary\n",
        "\n",
        "- Vectorization benefits:\n",
        "  - Reduced framework overhead\n",
        "  - Better memory locality\n",
        "  - Improved caching\n",
        "- Minibatch SGD advantages:\n",
        "  - Computational efficiency\n",
        "  - Statistical efficiency\n",
        "  - Memory efficiency\n",
        "- Key considerations:\n",
        "  - Batch size selection\n",
        "  - Learning rate decay\n",
        "  - Hardware constraints\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with different batch sizes and learning rates\n",
        "2. Implement learning rate decay\n",
        "3. Compare with replacement sampling\n",
        "4. Analyze behavior with duplicated data "
      ],
      "id": "d2883535"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}