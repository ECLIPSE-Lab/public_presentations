{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: |\n",
        "  Data Science for Electron Microscopy<br>\n",
        "  Lecture 2: Optimization, Regression, Sensor Fusion\n",
        "bibliography: ref.bib\n",
        "# csl: custom.csl\n",
        "author:\n",
        "  - name: Prof. Dr. Philipp Pelz\n",
        "    affiliation: \n",
        "      - FAU Erlangen-Nürnberg\n",
        "      - Institute of Micro- and Nanostructure Research\n",
        "\n",
        "execute: \n",
        "  eval: true \n",
        "  echo: true\n",
        "# lightbox: true\n",
        "format: \n",
        "    revealjs: \n",
        "        code-copy: true\n",
        "        # scroll-view:\n",
        "        #     activate: true\n",
        "        #     snap: mandatory\n",
        "        #     layout: full\n",
        "        width: 1920\n",
        "        height: 1080\n",
        "        menu:\n",
        "            side: right\n",
        "            width: wide\n",
        "        template-partials:\n",
        "            - title-slide.html\n",
        "        css: custom.css\n",
        "        theme: custom.scss\n",
        "        slide-number: c/t    \n",
        "        logo: \"eclipse_logo_small.png\" \n",
        "        highlight-style: a11y\n",
        "        incremental: false \n",
        "        background-transition: fade\n",
        "        footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "---\n",
        "\n",
        "\n",
        "      \n",
        " \n",
        "<!-- ## Outline\n",
        "\n",
        "::: {.outline-container}\n",
        "\n",
        "::: {.outline-box .fragment}\n",
        "### Formalities\n",
        "![](02_imaging.png) \n",
        ":::\n",
        "\n",
        "::: {.outline-box .fragment}\n",
        "### Introduction <br>to<br> Electron<br> Microscopy<br> Data\n",
        "![](02_imaging.png)\n",
        ":::\n",
        "\n",
        "::: {.outline-box .fragment}\n",
        "### Basic Pytorch<br> Knowledge\n",
        "![](02_imaging.png)\n",
        ":::\n",
        "\n",
        "::: {.outline-box .fragment}\n",
        "### .\n",
        "![](02_imaging.png)  \n",
        ":::-->\n",
        "  \n",
        "\n",
        "<!-- ---\n",
        "title: \"Optimization and Deep Learning\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Optimization and Deep Learning\n",
        "\n",
        "- Optimization and deep learning are closely related\n",
        "- Deep learning typically involves:\n",
        "  - Defining a loss function\n",
        "  - Using optimization to minimize the loss\n",
        "- Note: Most optimization algorithms minimize by convention\n",
        "  - To maximize: simply flip the sign of the objective\n",
        "\n",
        "## Goals of Optimization vs Deep Learning\n",
        "\n",
        "::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "### Optimization\n",
        "- Primary goal: Minimize objective function\n",
        "- Focus on training error\n",
        "- Direct mathematical approach\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "### Deep Learning\n",
        "- Primary goal: Find suitable model\n",
        "- Focus on generalization error\n",
        "- Must handle finite data\n",
        "- Must prevent overfitting\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Visualizing the Difference\n",
        "\n",
        "Let's examine empirical risk vs. risk:"
      ],
      "id": "a4dc41ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup3\n",
        "#| code-fold: true\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import numpy as np\n",
        "from mpl_toolkits import mplot3d\n",
        "import torch\n",
        "\n",
        "def f(x):\n",
        "    return x * d2l.cos(np.pi * x)\n",
        "\n",
        "def g(x):\n",
        "    return f(x) + 0.2 * d2l.cos(5 * np.pi * x)\n",
        "\n",
        "def annotate(text, xy, xytext):\n",
        "    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,\n",
        "                           arrowprops=dict(arrowstyle='->'))\n",
        "\n",
        "x = d2l.arange(0.5, 1.5, 0.01)\n",
        "d2l.set_figsize((4.5, 2.5))\n",
        "d2l.plot(x, [f(x), g(x)], 'x', 'risk')\n",
        "annotate('min of\\nempirical risk', (1.0, -1.2), (0.5, -1.1))\n",
        "annotate('min of risk', (1.1, -1.05), (0.95, -0.5))"
      ],
      "id": "setup3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Challenges in Deep Learning Optimization\n",
        "\n",
        "1. Local Minima\n",
        "2. Saddle Points\n",
        "3. Vanishing Gradients\n",
        "\n",
        "## Local Minima\n",
        "\n",
        "- Definition: Point where function value is smaller than nearby points\n",
        "- Global minimum: Smallest value over entire domain\n",
        "- Example function: $f(x) = x \\cdot \\textrm{cos}(\\pi x)$"
      ],
      "id": "2fb7727b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: local-minima1\n",
        "#| code-fold: true\n",
        "x = d2l.arange(-1.0, 2.0, 0.01)\n",
        "d2l.plot(x, [f(x), ], 'x', 'f(x)')\n",
        "annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))\n",
        "annotate('global minimum', (1.1, -0.95), (0.6, 0.8))"
      ],
      "id": "local-minima1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Impact of Local Minima\n",
        "\n",
        "- Deep learning models often have many local optima\n",
        "- Gradient approaches zero near local minimum\n",
        "- Minibatch SGD can help escape local minima\n",
        "  - Natural gradient variation provides \"noise\"\n",
        "  - Can dislodge parameters from local minima\n",
        "\n",
        "## Saddle Points\n",
        "\n",
        "- Characteristics:\n",
        "  - All gradients vanish\n",
        "  - Neither global nor local minimum\n",
        "- Example: $f(x) = x^3$\n",
        "  - First and second derivatives vanish at $x=0$\n",
        "  - Optimization can stall here"
      ],
      "id": "d4fdbca0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: saddle-points\n",
        "#| code-fold: true\n",
        "x = d2l.arange(-2.0, 2.0, 0.01)\n",
        "d2l.plot(x, [x**3], 'x', 'f(x)')\n",
        "annotate('saddle point', (0, -0.2), (-0.52, -5.0))"
      ],
      "id": "saddle-points",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Higher-Dimensional Saddle Points\n",
        "\n",
        "- More complex in higher dimensions\n",
        "- Example: $f(x,y) = x^2 - y^2$\n",
        "- Has saddle point at $(0,0)$\n",
        "  - Maximum with respect to $y$\n",
        "  - Minimum with respect to $x$"
      ],
      "id": "15011b4b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 3d-saddle\n",
        "#| code-fold: true\n",
        "x, y = d2l.meshgrid(\n",
        "    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))\n",
        "z = x**2 - y**2\n",
        "\n",
        "ax = d2l.plt.figure().add_subplot(111, projection='3d')\n",
        "ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})\n",
        "ax.plot([0], [0], [0], 'rx')\n",
        "ticks = [-1, 0, 1]\n",
        "d2l.plt.xticks(ticks)\n",
        "d2l.plt.yticks(ticks)\n",
        "ax.set_zticks(ticks)\n",
        "d2l.plt.xlabel('x')\n",
        "d2l.plt.ylabel('y')"
      ],
      "id": "d-saddle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hessian Matrix Analysis\n",
        "\n",
        "For a $k$-dimensional input vector:\n",
        "\n",
        "- All positive eigenvalues → Local minimum\n",
        "- All negative eigenvalues → Local maximum\n",
        "- Mixed signs → Saddle point\n",
        "\n",
        "## Vanishing Gradients\n",
        "\n",
        "- Most insidious optimization problem\n",
        "- Example: $f(x) = \\tanh(x)$\n",
        "  - At $x = 4$: gradient ≈ 0.0013\n",
        "  - Optimization stalls\n",
        "- Historical context:\n",
        "  - Major challenge before ReLU activation\n",
        "  - Made deep learning training difficult"
      ],
      "id": "ce603c4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: vanishing-gradients\n",
        "#| code-fold: true\n",
        "x = d2l.arange(-2.0, 5.0, 0.01)\n",
        "d2l.plot(x, [d2l.tanh(x)], 'x', 'f(x)')\n",
        "annotate('vanishing gradient', (4, 1), (2, 0.0))"
      ],
      "id": "vanishing-gradients",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Key takeaways:\n",
        "  - Training error minimization ≠ best generalization\n",
        "  - Many local minima exist\n",
        "  - Saddle points are common in non-convex problems\n",
        "  - Vanishing gradients can stall optimization\n",
        "- Good news:\n",
        "  - Robust algorithms exist\n",
        "  - Perfect solutions not always necessary\n",
        "  - Local optima can be useful\n",
        "  - Many practical solutions available\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Consider a simple MLP with a single hidden layer of $d$ dimensions:\n",
        "   - Show that for any local minimum there are at least $d!$ equivalent solutions\n",
        "   - Why does this happen?\n",
        "\n",
        "2. For a symmetric random matrix $\\mathbf{M}$:\n",
        "   - Prove that eigenvalue distribution is symmetric\n",
        "   - Why doesn't this imply $P(\\lambda > 0) = 0.5$?\n",
        "\n",
        "3. Additional challenges in deep learning optimization?\n",
        "\n",
        "4. Balancing a ball on a saddle:\n",
        "   - Why is this hard?\n",
        "   - How might this relate to optimization algorithms? \n",
        "\n",
        "\n",
        "<!-- ---\n",
        "title: \"Convexity\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Convexity\n",
        "\n",
        "- Convexity is crucial for optimization algorithm design\n",
        "- Benefits:\n",
        "  - Easier algorithm analysis and testing\n",
        "  - Better understanding of deep learning optimization\n",
        "  - Properties near local minima often resemble convex functions\n",
        "- Even nonconvex problems can benefit from convex analysis"
      ],
      "id": "2ed65a09"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup1\n",
        "#| code-fold: true\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import numpy as np\n",
        "from mpl_toolkits import mplot3d\n",
        "import torch"
      ],
      "id": "setup1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definitions\n",
        "\n",
        "### Convex Sets\n",
        "\n",
        "- A set $\\mathcal{X}$ is convex if:\n",
        "  - For any $a, b \\in \\mathcal{X}$\n",
        "  - Line segment connecting $a$ and $b$ is in $\\mathcal{X}$\n",
        "- Mathematical definition:\n",
        "  $$\\lambda a + (1-\\lambda) b \\in \\mathcal{X} \\textrm{ whenever } a, b \\in \\mathcal{X}$$\n",
        "  for all $\\lambda \\in [0, 1]$\n",
        "\n",
        "### Visual Examples\n",
        "\n",
        "![The first set is nonconvex and the other two are convex.](./img/pacman.svg)\n",
        "\n",
        "--- \n",
        "\n",
        "### Set Operations\n",
        "\n",
        "- Intersections of convex sets are convex\n",
        "- Unions of convex sets need not be convex\n",
        "- Example: $\\mathbb{R}^d$ is convex\n",
        "- Bounded sets (e.g., balls) are often convex\n",
        "\n",
        "![The intersection between two convex sets is convex.](./img/convex-intersect.svg)\n",
        "\n",
        "![The union of two convex sets need not be convex.](./img/nonconvex.svg)\n",
        "\n",
        "## Convex Functions\n",
        "\n",
        "### Definition\n",
        "\n",
        "- Function $f: \\mathcal{X} \\to \\mathbb{R}$ is convex if:\n",
        "  - For all $x, x' \\in \\mathcal{X}$\n",
        "  - For all $\\lambda \\in [0, 1]$\n",
        "  - Satisfies: $\\lambda f(x) + (1-\\lambda) f(x') \\geq f(\\lambda x + (1-\\lambda) x')$\n",
        "\n",
        "### Examples"
      ],
      "id": "3992e2b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: function-examples\n",
        "#| code-fold: true\n",
        "f = lambda x: 0.5 * x**2  # Convex\n",
        "g = lambda x: d2l.cos(np.pi * x)  # Nonconvex\n",
        "h = lambda x: d2l.exp(0.5 * x)  # Convex\n",
        "\n",
        "x, segment = d2l.arange(-2, 2, 0.01), d2l.tensor([-1.5, 1])\n",
        "d2l.use_svg_display()\n",
        "_, axes = d2l.plt.subplots(1, 3, figsize=(9, 3))\n",
        "for ax, func in zip(axes, [f, g, h]):\n",
        "    d2l.plot([x, segment], [func(x), func(segment)], axes=ax)"
      ],
      "id": "function-examples",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Jensen's Inequality\n",
        "\n",
        "### Definition\n",
        "\n",
        "- Generalization of convexity\n",
        "- For convex function $f$:\n",
        "  $$\\sum_i \\alpha_i f(x_i) \\geq f\\left(\\sum_i \\alpha_i x_i\\right)$$\n",
        "  $$E_X[f(X)] \\geq f\\left(E_X[X]\\right)$$\n",
        "- Where $\\alpha_i \\geq 0$ and $\\sum_i \\alpha_i = 1$\n",
        "\n",
        "### Applications\n",
        "\n",
        "- Bounding complex expressions\n",
        "- Log-likelihood of partially observed variables\n",
        "- Variational methods\n",
        "- Clustering algorithms\n",
        "\n",
        "## Properties\n",
        "\n",
        "### Local vs Global Minima\n",
        "\n",
        "- Local minima of convex functions are global minima\n",
        "- Proof by contradiction\n",
        "- Example: $f(x) = (x-1)^2$"
      ],
      "id": "45612d03"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: local-global-minima\n",
        "#| code-fold: true\n",
        "f = lambda x: (x - 1) ** 2\n",
        "d2l.set_figsize((8,8))\n",
        "d2l.plot([x, segment], [f(x), f(segment)], 'x', 'f(x)')"
      ],
      "id": "local-global-minima",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "### Below Sets\n",
        "\n",
        "- Given convex function $f$ on convex set $\\mathcal{X}$\n",
        "- Below set $\\mathcal{S}_b = \\{x | x \\in \\mathcal{X} \\textrm{ and } f(x) \\leq b\\}$ is convex\n",
        "- Proof uses definition of convexity\n",
        "\n",
        "### Second Derivatives\n",
        "\n",
        "- For twice-differentiable $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
        "- Convex if and only if Hessian is positive semidefinite\n",
        "- One-dimensional case: $f'' \\geq 0$\n",
        "- Multidimensional case: $\\nabla^2f \\succeq 0$\n",
        "\n",
        "## Constraints\n",
        "\n",
        "### Constrained Optimization\n",
        "\n",
        "- Form: \n",
        "  $$\\begin{aligned} \\mathop{\\textrm{minimize~}}_{\\mathbf{x}} & f(\\mathbf{x}) \\\\\n",
        "    \\textrm{ subject to } & c_i(\\mathbf{x}) \\leq 0 \\textrm{ for all } i \\in \\{1, \\ldots, n\\}\\end{aligned}$$\n",
        "- Examples:\n",
        "  - Unit ball constraint: $c_1(\\mathbf{x}) = \\|\\mathbf{x}\\|_2 - 1$\n",
        "  - Half-space constraint: $c_2(\\mathbf{x}) = \\mathbf{v}^\\top \\mathbf{x} + b$\n",
        "\n",
        "### Lagrangian\n",
        "\n",
        "- Combines objective and constraints\n",
        "- Form: $L(\\mathbf{x}, \\alpha_1, \\ldots, \\alpha_n) = f(\\mathbf{x}) + \\sum_{i=1}^n \\alpha_i c_i(\\mathbf{x})$\n",
        "- Lagrange multipliers $\\alpha_i \\geq 0$\n",
        "- Saddle point optimization\n",
        "\n",
        "---\n",
        "\n",
        "### Penalties\n",
        "\n",
        "- Alternative to exact constraint satisfaction\n",
        "- Add $\\alpha_i c_i(\\mathbf{x})$ to objective\n",
        "- Example: weight decay\n",
        "- More robust than exact satisfaction\n",
        "- Works well for nonconvex problems\n",
        "\n",
        "---\n",
        "\n",
        "### Projections\n",
        "\n",
        "- Projection on convex set $\\mathcal{X}$:\n",
        "  $$\\textrm{Proj}_\\mathcal{X}(\\mathbf{x}) = \\mathop{\\mathrm{argmin}}_{\\mathbf{x}' \\in \\mathcal{X}} \\|\\mathbf{x} - \\mathbf{x}'\\|$$\n",
        "- Example: gradient clipping\n",
        "- Applications:\n",
        "  - Sparse weight vectors\n",
        "  - $\\ell_1$ ball projections\n",
        "\n",
        "![Convex Projections.](./img/projections.svg)\n",
        "\n",
        "## Summary\n",
        "\n",
        "- Key properties:\n",
        "  - Intersections of convex sets are convex\n",
        "  - Jensen's inequality for expectations\n",
        "  - Hessian positive semidefinite for convex functions\n",
        "  - Local minima are global minima\n",
        "- Constraint handling:\n",
        "  - Lagrangian approach\n",
        "  - Penalty methods\n",
        "  - Projections\n",
        "- Applications in deep learning:\n",
        "  - Algorithm motivation\n",
        "  - Understanding optimization\n",
        "  - Gradient descent analysis\n",
        "\n",
        "\n",
        "\n",
        "<!-- ---\n",
        "title: \"Gradient Descent\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- Gradient descent is fundamental to understanding optimization\n",
        "- Key concepts apply to more advanced algorithms\n",
        "- Important considerations:\n",
        "  - Learning rate selection\n",
        "  - Divergence issues\n",
        "  - Preconditioning techniques\n",
        "\n",
        "## One-Dimensional Gradient Descent\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "- For continuously differentiable $f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
        "- Taylor expansion:\n",
        "  $$f(x + \\epsilon) = f(x) + \\epsilon f'(x) + \\mathcal{O}(\\epsilon^2)$$\n",
        "- Moving in negative gradient direction:\n",
        "  - Choose $\\epsilon = -\\eta f'(x)$\n",
        "  - Fixed step size $\\eta > 0$\n",
        "  - Results in: $f(x - \\eta f'(x)) \\lessapprox f(x)$\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation"
      ],
      "id": "df294c00"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup2\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def f(x):  # Objective function\n",
        "    return x ** 2\n",
        "\n",
        "def f_grad(x):  # Gradient (derivative) of the objective function\n",
        "    return 2 * x"
      ],
      "id": "setup2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Gradient Descent"
      ],
      "id": "ac41c319"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: gd-implementation\n",
        "def gd(eta, f_grad):\n",
        "    x = 10.0\n",
        "    results = [x]\n",
        "    for i in range(10):\n",
        "        x -= eta * f_grad(x)\n",
        "        results.append(float(x))\n",
        "    print(f'epoch 10, x: {x:f}')\n",
        "    return results\n",
        "\n",
        "results = gd(0.2, f_grad)"
      ],
      "id": "gd-implementation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Visualization"
      ],
      "id": "f3b78f37"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: gd-visualization\n",
        "def show_trace(results, f):\n",
        "    n = max(abs(min(results)), abs(max(results)))\n",
        "    f_line = d2l.arange(-n, n, 0.01)\n",
        "    d2l.set_figsize()\n",
        "    d2l.plot([f_line, results], [[f(x) for x in f_line], [\n",
        "        f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
        "\n",
        "show_trace(results, f)"
      ],
      "id": "gd-visualization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Rate Effects\n",
        "\n",
        "### Too Small Learning Rate\n",
        "\n",
        "- Slow convergence\n",
        "- More iterations needed\n",
        "- Example with $\\eta = 0.05$:"
      ],
      "id": "2f93f8ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: small-lr\n",
        "show_trace(gd(0.05, f_grad), f)"
      ],
      "id": "small-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Too Large Learning Rate\n",
        "\n",
        "- Solution oscillates\n",
        "- May diverge\n",
        "- Example with $\\eta = 1.1$:"
      ],
      "id": "950077cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: large-lr\n",
        "show_trace(gd(1.1, f_grad), f)"
      ],
      "id": "large-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Local Minima\n",
        "\n",
        "- Nonconvex functions have multiple minima\n",
        "- Example: $f(x) = x \\cdot \\cos(cx)$\n",
        "- High learning rates can lead to poor local minima"
      ],
      "id": "698b790c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: local-minima\n",
        "c = d2l.tensor(0.15 * np.pi)\n",
        "\n",
        "def f(x):  # Objective function\n",
        "    return x * d2l.cos(c * x)\n",
        "\n",
        "def f_grad(x):  # Gradient of the objective function\n",
        "    return d2l.cos(c * x) - c * x * d2l.sin(c * x)\n",
        "\n",
        "show_trace(gd(2, f_grad), f)"
      ],
      "id": "local-minima",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multivariate Gradient Descent\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "- For $f: \\mathbb{R}^d \\to \\mathbb{R}$\n",
        "- Gradient vector: $\\nabla f(\\mathbf{x}) = [\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}]^\\top$\n",
        "- Taylor expansion:\n",
        "  $$f(\\mathbf{x} + \\boldsymbol{\\epsilon}) = f(\\mathbf{x}) + \\mathbf{\\boldsymbol{\\epsilon}}^\\top \\nabla f(\\mathbf{x}) + \\mathcal{O}(\\|\\boldsymbol{\\epsilon}\\|^2)$$\n",
        "- Update rule: $\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f(\\mathbf{x})$\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation"
      ],
      "id": "4db4bfb7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "code-block-height": 800
      },
      "source": [
        "#| label: multivariate-gd\n",
        "def train_2d(trainer, steps=20, f_grad=None):\n",
        "    \"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n",
        "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
        "    results = [(x1, x2)]\n",
        "    for i in range(steps):\n",
        "        if f_grad:\n",
        "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n",
        "        else:\n",
        "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n",
        "        results.append((x1, x2))\n",
        "    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n",
        "    return results\n",
        "\n",
        "def show_trace_2d(f, results):\n",
        "    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n",
        "    d2l.set_figsize()\n",
        "    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
        "    x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),\n",
        "                          d2l.arange(-3.0, 1.0, 0.1), indexing='ij')\n",
        "    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
        "    d2l.plt.xlabel('x1')\n",
        "    d2l.plt.ylabel('x2')"
      ],
      "id": "multivariate-gd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Example: Quadratic Function"
      ],
      "id": "f5283a51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: quadratic-example\n",
        "def f_2d(x1, x2):  # Objective function\n",
        "    return x1 ** 2 + 2 * x2 ** 2\n",
        "\n",
        "def f_2d_grad(x1, x2):  # Gradient of the objective function\n",
        "    return (2 * x1, 4 * x2)\n",
        "\n",
        "def gd_2d(x1, x2, s1, s2, f_grad):\n",
        "    g1, g2 = f_grad(x1, x2)\n",
        "    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n",
        "\n",
        "eta = 0.1\n",
        "show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))"
      ],
      "id": "quadratic-example",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Methods\n",
        "\n",
        "### Newton's Method\n",
        "\n",
        "- Uses second-order information\n",
        "- Taylor expansion with Hessian:\n",
        "  $$f(\\mathbf{x} + \\boldsymbol{\\epsilon}) = f(\\mathbf{x}) + \\boldsymbol{\\epsilon}^\\top \\nabla f(\\mathbf{x}) + \\frac{1}{2} \\boldsymbol{\\epsilon}^\\top \\nabla^2 f(\\mathbf{x}) \\boldsymbol{\\epsilon} + \\mathcal{O}(\\|\\boldsymbol{\\epsilon}\\|^3)$$\n",
        "- Update rule: $\\boldsymbol{\\epsilon} = -\\mathbf{H}^{-1} \\nabla f(\\mathbf{x})$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Implementation"
      ],
      "id": "28729f7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: newton-method\n",
        "c = d2l.tensor(0.5)\n",
        "\n",
        "def f(x):  # Objective function\n",
        "    return d2l.cosh(c * x)\n",
        "\n",
        "def f_grad(x):  # Gradient of the objective function\n",
        "    return c * d2l.sinh(c * x)\n",
        "\n",
        "def f_hess(x):  # Hessian of the objective function\n",
        "    return c**2 * d2l.cosh(c * x)\n",
        "\n",
        "def newton(eta=1):\n",
        "    x = 10.0\n",
        "    results = [x]\n",
        "    for i in range(10):\n",
        "        x -= eta * f_grad(x) / f_hess(x)\n",
        "        results.append(float(x))\n",
        "    print('epoch 10, x:', x)\n",
        "    return results\n",
        "\n",
        "show_trace(newton(), f)"
      ],
      "id": "newton-method",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Nonconvex Example"
      ],
      "id": "7fd60f92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: nonconvex-newton\n",
        "c = d2l.tensor(0.15 * np.pi)\n",
        "\n",
        "def f(x):  # Objective function\n",
        "    return x * d2l.cos(c * x)\n",
        "\n",
        "def f_grad(x):  # Gradient of the objective function\n",
        "    return d2l.cos(c * x) - c * x * d2l.sin(c * x)\n",
        "\n",
        "def f_hess(x):  # Hessian of the objective function\n",
        "    return - 2 * c * d2l.sin(c * x) - x * c**2 * d2l.cos(c * x)\n",
        "\n",
        "show_trace(newton(0.5), f)"
      ],
      "id": "nonconvex-newton",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preconditioning\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- Avoid full Hessian computation\n",
        "- Use diagonal entries only\n",
        "- Update rule: $\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\textrm{diag}(\\mathbf{H})^{-1} \\nabla f(\\mathbf{x})$\n",
        "- Benefits:\n",
        "  - Different learning rates per variable\n",
        "  - Handles scale mismatches\n",
        "  - More efficient than full Newton's method\n",
        "\n",
        "## Summary\n",
        "\n",
        "- Learning rate selection is crucial\n",
        "- Local minima can trap gradient descent\n",
        "- High dimensions require careful learning rate adjustment\n",
        "- Preconditioning helps with scale issues\n",
        "- Newton's method:\n",
        "  - Fast convergence for convex problems\n",
        "  - Requires careful handling for nonconvex problems\n",
        "  - Computationally expensive for large problems\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with different learning rates and objective functions\n",
        "2. Implement line search for convex optimization\n",
        "3. Design a slow-converging 2D objective function\n",
        "4. Implement lightweight Newton's method with preconditioning\n",
        "5. Test algorithms on rotated coordinate systems \n",
        "\n",
        "\n",
        "<!-- ---\n",
        "title: \"Stochastic Gradient Descent\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "\n",
        "- Previously used SGD without detailed explanation\n",
        "- Now diving deeper into its principles\n",
        "- Building on gradient descent fundamentals\n",
        "- Understanding why and how it works\n"
      ],
      "id": "ad4c980a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup4\n",
        "#| code-fold: true\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import math\n",
        "import torch"
      ],
      "id": "setup4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradient Updates\n",
        "\n",
        "### Objective Function\n",
        "\n",
        "- Training dataset with $n$ examples\n",
        "- Loss function $f_i(\\mathbf{x})$ for example $i$\n",
        "- Overall objective:\n",
        "  $$f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\mathbf{x})$$\n",
        "- Full gradient:\n",
        "  $$\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x})$$\n",
        "\n",
        "---\n",
        "\n",
        "### Computational Cost\n",
        "\n",
        "- Gradient descent: $\\mathcal{O}(n)$ per iteration\n",
        "- SGD: $\\mathcal{O}(1)$ per iteration\n",
        "- Update rule:\n",
        "  $$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x})$$\n",
        "- Unbiased estimate:\n",
        "  $$\\mathbb{E}_i \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x})$$\n",
        "\n",
        "### Implementation"
      ],
      "id": "cf41d28f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: objective-fn\n",
        "def f(x1, x2):  # Objective function\n",
        "    return x1 ** 2 + 2 * x2 ** 2\n",
        "\n",
        "def f_grad(x1, x2):  # Gradient of the objective function\n",
        "    return 2 * x1, 4 * x2"
      ],
      "id": "objective-fn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ],
      "id": "818bb730"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: sgd-implementation\n",
        "def sgd(x1, x2, s1, s2, f_grad):\n",
        "    g1, g2 = f_grad(x1, x2)\n",
        "    # Simulate noisy gradient\n",
        "    g1 += torch.normal(0.0, 1, (1,)).item()\n",
        "    g2 += torch.normal(0.0, 1, (1,)).item()\n",
        "    eta_t = eta * lr()\n",
        "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)"
      ],
      "id": "sgd-implementation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: constant-lr\n",
        "def constant_lr():\n",
        "    return 1\n",
        "\n",
        "eta = 0.1\n",
        "lr = constant_lr  # Constant learning rate\n",
        "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
      ],
      "id": "constant-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Learning Rate\n",
        "\n",
        "### Learning Rate Strategies\n",
        "\n",
        "- Piecewise constant: $\\eta(t) = \\eta_i \\textrm{ if } t_i \\leq t \\leq t_{i+1}$\n",
        "- Exponential decay: $\\eta(t) = \\eta_0 \\cdot e^{-\\lambda t}$\n",
        "- Polynomial decay: $\\eta(t) = \\eta_0 \\cdot (\\beta t + 1)^{-\\alpha}$\n",
        "\n",
        "### Exponential Decay Implementation"
      ],
      "id": "3bbf0c29"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: exponential-lr\n",
        "def exponential_lr():\n",
        "    global t\n",
        "    t += 1\n",
        "    return math.exp(-0.1 * t)\n",
        "\n",
        "t = 1\n",
        "lr = exponential_lr\n",
        "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))"
      ],
      "id": "exponential-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Polynomial Decay Implementation"
      ],
      "id": "8fdf1c4b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: polynomial-lr\n",
        "def polynomial_lr():\n",
        "    global t\n",
        "    t += 1\n",
        "    return (1 + 0.1 * t) ** (-0.5)\n",
        "\n",
        "t = 1\n",
        "lr = polynomial_lr\n",
        "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
      ],
      "id": "polynomial-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradients and Finite Samples\n",
        "\n",
        "### Sampling Strategies\n",
        "\n",
        "- With replacement:\n",
        "  - Probability of choosing element: $1 - e^{-1} \\approx 0.63$\n",
        "  - Increased variance\n",
        "  - Decreased data efficiency\n",
        "- Without replacement:\n",
        "  - Better variance properties\n",
        "  - More efficient data usage\n",
        "  - Default choice in practice\n",
        "\n",
        "## Summary\n",
        "\n",
        "- Key points:\n",
        "  - SGD reduces computational cost to $\\mathcal{O}(1)$\n",
        "  - Learning rate scheduling is crucial\n",
        "  - Convergence guarantees for convex problems\n",
        "  - Sampling without replacement preferred\n",
        "- Practical considerations:\n",
        "  - Dynamic learning rates\n",
        "  - Trade-offs in sampling strategies\n",
        "  - Nonconvex optimization challenges\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with learning rate schedules\n",
        "2. Analyze noise in gradient updates\n",
        "3. Compare sampling strategies\n",
        "4. Investigate gradient coordinate scaling\n",
        "5. Study local minima in nonconvex functions \n",
        "\n",
        "\n",
        "<!-- ---\n",
        "title: \"\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Minibatch Stochastic Gradient Descent\n",
        "\n",
        "- Two extremes in gradient-based learning:\n",
        "  - Full dataset (gradient descent)\n",
        "  - Single examples (stochastic gradient descent)\n",
        "- Each approach has drawbacks:\n",
        "  - Gradient descent: Not data efficient for similar data\n",
        "  - SGD: Not computationally efficient (poor vectorization)\n",
        "- Minibatch SGD offers a middle ground\n",
        "\n",
        "## Vectorization and Caches\n",
        "\n",
        "### Hardware Considerations\n",
        "\n",
        "- Multiple GPUs and servers require larger minibatches\n",
        "  - 8 GPUs × 16 servers = minimum batch size of 128\n",
        "- Single GPU/CPU considerations:\n",
        "  - Multiple memory types (registers, L1/L2/L3 cache)\n",
        "  - Different bandwidth constraints\n",
        "  - Memory access patterns matter\n",
        "\n",
        "### Performance Metrics\n",
        "\n",
        "- Modern CPU capabilities:\n",
        "  - 2GHz CPU with 16 cores and AVX-512\n",
        "  - Can process up to 10¹² bytes/second\n",
        "- GPU capabilities:\n",
        "  - 100× better than CPU\n",
        "- Memory bandwidth limitations:\n",
        "  - Midrange server: ~100 GB/s\n",
        "  - Memory access width: 64-384 bit\n",
        "\n",
        "## Matrix Multiplication Strategies\n",
        "\n",
        "### Different Approaches\n",
        "\n",
        "1. Element-wise computation\n",
        "2. Column-wise computation\n",
        "3. Full matrix multiplication\n",
        "4. Block-wise computation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Performance Comparison"
      ],
      "id": "0287b754"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: matrix-mult\n",
        " \n",
        "import d2l\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return torch.tensor(self.times).cumsum().tolist()\n",
        "\n",
        "# Initialize matrices\n",
        "A = torch.zeros(256, 256)\n",
        "B = torch.randn(256, 256)\n",
        "C = torch.randn(256, 256)\n",
        "timer = Timer()"
      ],
      "id": "matrix-mult",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Element-wise Computation"
      ],
      "id": "0bfb7d69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: element-wise\n",
        "# Compute A = BC one element at a time\n",
        "timer.start()\n",
        "for i in range(256):\n",
        "    for j in range(256):\n",
        "        A[i, j] = torch.dot(B[i, :], C[:, j])\n",
        "timer.stop()"
      ],
      "id": "element-wise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Column-wise Computation"
      ],
      "id": "9ee9b1ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: column-wise\n",
        "# Compute A = BC one column at a time\n",
        "timer.start()\n",
        "for j in range(256):\n",
        "    A[:, j] = torch.mv(B, C[:, j])\n",
        "timer.stop()"
      ],
      "id": "column-wise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Full Matrix Multiplication"
      ],
      "id": "af136c7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: full-matrix\n",
        "# Compute A = BC in one go\n",
        "timer.start()\n",
        "A = torch.mm(B, C)\n",
        "timer.stop()\n",
        "\n",
        "gigaflops = [0.03 / i for i in timer.times]\n",
        "print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n",
        "      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')"
      ],
      "id": "full-matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minibatch Processing\n",
        "\n",
        "### Why Use Minibatches?\n",
        "\n",
        "- Computational efficiency\n",
        "- Statistical properties:\n",
        "  - Maintains gradient expectation\n",
        "  - Reduces variance by factor of $b^{-\\frac{1}{2}}$\n",
        "  - $b$ = batch size\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Batch Size Trade-offs\n",
        "\n",
        "- Too small:\n",
        "  - Poor computational efficiency\n",
        "  - High variance\n",
        "- Too large:\n",
        "  - Diminishing returns in variance reduction\n",
        "  - Memory constraints\n",
        "- Optimal: Balance between:\n",
        "  - Computational efficiency\n",
        "  - Statistical efficiency\n",
        "  - Available memory\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Implementation\n",
        "\n",
        "### Data Loading"
      ],
      "id": "4f55a2a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data-loading\n",
        "#@save\n",
        "d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n",
        "                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n",
        "\n",
        "#@save\n",
        "def get_data_ch11(batch_size=10, n=1500):\n",
        "    data = np.genfromtxt(d2l.download('airfoil'),\n",
        "                         dtype=np.float32, delimiter='\\t')\n",
        "    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))\n",
        "    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\n",
        "                               batch_size, is_train=True)\n",
        "    return data_iter, data.shape[1]-1"
      ],
      "id": "data-loading",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Function"
      ],
      "id": "b0d1ad5c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: training-fn\n",
        "#@save\n",
        "def sgd(params, states, hyperparams):\n",
        "    for p in params:\n",
        "        p.data.sub_(hyperparams['lr'] * p.grad)\n",
        "        p.grad.data.zero_()"
      ],
      "id": "training-fn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Training Function"
      ],
      "id": "62cf06c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: training-fn1\n",
        "\n",
        "def train_ch11(trainer_fn, states, hyperparams, data_iter,\n",
        "               feature_dim, num_epochs=2):\n",
        "    # Initialization\n",
        "    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\n",
        "                     requires_grad=True)\n",
        "    b = torch.zeros((1), requires_grad=True)\n",
        "    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
        "    # Train\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
        "    n, timer = 0, d2l.Timer()\n",
        "    for _ in range(num_epochs):\n",
        "        for X, y in data_iter:\n",
        "            l = loss(net(X), y).mean()\n",
        "            l.backward()\n",
        "            trainer_fn([w, b], states, hyperparams)\n",
        "            n += X.shape[0]\n",
        "            if n % 200 == 0:\n",
        "                timer.stop()\n",
        "                animator.add(n/X.shape[0]/len(data_iter),\n",
        "                             (d2l.evaluate_loss(net, data_iter, loss),))\n",
        "                timer.start()\n",
        "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n",
        "    return timer.cumsum(), animator.Y[0]"
      ],
      "id": "training-fn1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison\n",
        "\n",
        "### Different Batch Sizes\n",
        "::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n"
      ],
      "id": "2a773a99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: batch-comparison1\n",
        "def train_sgd(lr, batch_size, num_epochs=2):\n",
        "    data_iter, feature_dim = get_data_ch11(batch_size)\n",
        "    return train_ch11(\n",
        "        sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)\n",
        "\n",
        "# Compare different approaches\n",
        "gd_res = train_sgd(1, 1500, 10)  # Full batch\n",
        "sgd_res = train_sgd(0.005, 1)    # Single example\n",
        "d2l.set_figsize([6, 3])\n",
        "d2l.plot(*list(map(list, zip(gd_res, sgd_res))),\n",
        "         'time (sec)', 'loss', xlim=[1e-2, 10],\n",
        "         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\n",
        "d2l.plt.gca().set_xscale('log')"
      ],
      "id": "batch-comparison1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "91fcab02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: batch-comparison2\n",
        "mini1_res = train_sgd(.4, 100)   # Medium batch\n",
        "mini2_res = train_sgd(.05, 10)   # Small batch\n",
        "\n",
        "# Plot results\n",
        "d2l.set_figsize([6, 3])\n",
        "d2l.plot(*list(map(list, zip( mini1_res, mini2_res))),\n",
        "         'time (sec)', 'loss', xlim=[1e-2, 10],\n",
        "         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\n",
        "d2l.plt.gca().set_xscale('log')"
      ],
      "id": "batch-comparison2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        ":::\n",
        "## Summary\n",
        "\n",
        "- Vectorization benefits:\n",
        "  - Reduced framework overhead\n",
        "  - Better memory locality\n",
        "  - Improved caching\n",
        "- Minibatch SGD advantages:\n",
        "  - Computational efficiency\n",
        "  - Statistical efficiency\n",
        "  - Memory efficiency\n",
        "- Key considerations:\n",
        "  - Batch size selection\n",
        "  - Learning rate decay\n",
        "  - Hardware constraints\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with different batch sizes and learning rates\n",
        "2. Implement learning rate decay\n",
        "3. Compare with replacement sampling\n",
        "4. Analyze behavior with duplicated data \n",
        "\n",
        "\n",
        "<!-- ---\n",
        "title: \"Momentum in Optimization\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Momentum in Optimization\n",
        "\n",
        "- Momentum is a key optimization technique in deep learning\n",
        "- Addresses challenges in stochastic gradient descent:\n",
        "  - Learning rate sensitivity\n",
        "  - Convergence issues\n",
        "  - Noise handling\n",
        "- Particularly effective for ill-conditioned problems\n",
        "\n",
        "## Basics of Momentum\n",
        "\n",
        "### Leaky Averages\n",
        "\n",
        "- Minibatch SGD averages gradients to reduce variance\n",
        "- Momentum extends this concept using \"leaky averages\":\n",
        "  - Accumulates past gradients\n",
        "  - Weights recent gradients more heavily\n",
        "  - Formula: $\\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}$\n",
        "  - $\\beta \\in (0, 1)$ controls the \"memory\" of past gradients\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Key Benefits\n",
        "\n",
        "- Accelerates convergence\n",
        "- Particularly effective for:\n",
        "  - Ill-conditioned problems\n",
        "  - Narrow canyons in optimization landscape\n",
        "- Provides more stable descent directions\n",
        "- Works well with both:\n",
        "  - Noise-free convex problems\n",
        "  - Stochastic gradient descent\n",
        "\n",
        "## Visualizing the Problem\n",
        "\n",
        "Let's examine an ill-conditioned problem:"
      ],
      "id": "b2da3892"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import torch\n",
        "\n",
        "eta = 0.4\n",
        "def f_2d(x1, x2):\n",
        "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
        "def gd_2d(x1, x2, s1, s2):\n",
        "    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n",
        "\n",
        "d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Challenge\n",
        "\n",
        "- Function $f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2$ is very flat in $x_1$ direction\n",
        "- Gradient in $x_2$ direction:\n",
        "  - Much higher\n",
        "  - Changes more rapidly\n",
        "- Trade-off in learning rate:\n",
        "  - Small rate: Slow convergence in $x_1$\n",
        "  - Large rate: Divergence in $x_2$\n",
        "\n",
        "## Momentum Method\n",
        "\n",
        "### Update Equations\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{v}_t &\\leftarrow \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}, \\\\\n",
        "\\mathbf{x}_t &\\leftarrow \\mathbf{x}_{t-1} - \\eta_t \\mathbf{v}_t.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "### Implementation"
      ],
      "id": "7198b1c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: momentum-implementation\n",
        "def momentum_2d(x1, x2, v1, v2):\n",
        "    v1 = beta * v1 + 0.2 * x1\n",
        "    v2 = beta * v2 + 4 * x2\n",
        "    return x1 - eta * v1, x2 - eta * v2, v1, v2\n",
        "\n",
        "eta, beta = 0.6, 0.5\n",
        "d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"
      ],
      "id": "momentum-implementation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Effect of Momentum Parameter\n",
        "\n",
        "- $\\beta = 0.5$: Good convergence\n",
        "- $\\beta = 0.25$: Barely converges but better than no momentum\n",
        "- $\\beta = 0$: Reduces to regular gradient descent"
      ],
      "id": "11d4d94b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: momentum-beta\n",
        "eta, beta = 0.6, 0.25\n",
        "d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"
      ],
      "id": "momentum-beta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Effective Sample Weight\n",
        "\n",
        "- Sum of weights: $\\sum_{\\tau=0}^\\infty \\beta^\\tau = \\frac{1}{1-\\beta}$\n",
        "- Step size effectively becomes $\\frac{\\eta}{1-\\beta}$\n",
        "- Better behaved descent direction"
      ],
      "id": "4a5368d4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: effective-weight\n",
        "d2l.set_figsize()\n",
        "betas = [0.95, 0.9, 0.6, 0]\n",
        "for beta in betas:\n",
        "    x = d2l.numpy(d2l.arange(40))\n",
        "    d2l.plt.plot(x, beta ** x, label=f'beta = {beta:.2f}')\n",
        "d2l.plt.xlabel('time')\n",
        "d2l.plt.legend();"
      ],
      "id": "effective-weight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Implementation\n",
        "\n",
        "### From Scratch"
      ],
      "id": "9c5fcc6e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: momentum-scratch\n",
        "def init_momentum_states(feature_dim):\n",
        "    v_w = d2l.zeros((feature_dim, 1))\n",
        "    v_b = d2l.zeros(1)\n",
        "    return (v_w, v_b)\n",
        "\n",
        "def sgd_momentum(params, states, hyperparams):\n",
        "    for p, v in zip(params, states):\n",
        "        with torch.no_grad():\n",
        "            v[:] = hyperparams['momentum'] * v + p.grad\n",
        "            p[:] -= hyperparams['lr'] * v\n",
        "        p.grad.data.zero_()"
      ],
      "id": "momentum-scratch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training with Different Parameters"
      ],
      "id": "4c7e82d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: momentum-training\n",
        "def train_momentum(lr, momentum, num_epochs=2):\n",
        "    d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),\n",
        "                   {'lr': lr, 'momentum': momentum}, data_iter,\n",
        "                   feature_dim, num_epochs)\n",
        "\n",
        "data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n",
        "train_momentum(0.02, 0.5)"
      ],
      "id": "momentum-training",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Theoretical Analysis\n",
        "\n",
        "### Quadratic Convex Functions\n",
        "\n",
        "- General form: $h(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x} + \\mathbf{x}^\\top \\mathbf{c} + b$\n",
        "- For positive definite $\\mathbf{Q}$:\n",
        "  - Minimizer at $\\mathbf{x}^* = -\\mathbf{Q}^{-1} \\mathbf{c}$\n",
        "  - Minimum value: $b - \\frac{1}{2} \\mathbf{c}^\\top \\mathbf{Q}^{-1} \\mathbf{c}$\n",
        "- Gradient: $\\partial_{\\mathbf{x}} h(\\mathbf{x}) = \\mathbf{Q} (\\mathbf{x} - \\mathbf{Q}^{-1} \\mathbf{c})$\n",
        "\n",
        "---\n",
        "\n",
        "### Convergence Analysis\n",
        "\n",
        "- For scalar function $f(x) = \\frac{\\lambda}{2} x^2$:\n",
        "  - Gradient descent: $x_{t+1} = (1 - \\eta \\lambda) x_t$\n",
        "  - Convergence when $|1 - \\eta \\lambda| < 1$\n",
        "  - Exponential convergence rate"
      ],
      "id": "58f00245"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: convergence-analysis\n",
        "lambdas = [0.1, 1, 10, 19]\n",
        "eta = 0.1\n",
        "d2l.set_figsize((6, 4))\n",
        "for lam in lambdas:\n",
        "    t = d2l.numpy(d2l.arange(20))\n",
        "    d2l.plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')\n",
        "d2l.plt.xlabel('time')\n",
        "d2l.plt.legend();"
      ],
      "id": "convergence-analysis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Momentum replaces gradients with leaky averages\n",
        "- Key benefits:\n",
        "  - Accelerates convergence\n",
        "  - Works for both noise-free and noisy gradients\n",
        "  - Prevents optimization stalling\n",
        "  - Effective sample size: $\\frac{1}{1-\\beta}$\n",
        "- Implementation requires:\n",
        "  - Additional state vector (velocity)\n",
        "  - Careful parameter tuning\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with different momentum and learning rate combinations\n",
        "2. Analyze gradient descent and momentum for quadratic problems with multiple eigenvalues\n",
        "3. Derive minimum value and minimizer for $h(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x} + \\mathbf{x}^\\top \\mathbf{c} + b$\n",
        "4. Investigate behavior with stochastic gradient descent and minibatch variants \n",
        "\n",
        "\n",
        "<!-- ---\n",
        "title: \"Adam Optimization\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Adam Optimization\n",
        "\n",
        "- Adam combines multiple optimization techniques:\n",
        "  - Stochastic Gradient Descent (SGD)\n",
        "  - Minibatch processing\n",
        "  - Momentum\n",
        "  - Per-coordinate scaling (AdaGrad)\n",
        "  - Learning rate adjustment (RMSProp)\n",
        "- Popular in deep learning due to:\n",
        "  - Robustness\n",
        "  - Effectiveness\n",
        "  - Computational efficiency\n",
        "\n",
        "## Previous Optimization Methods\n",
        "\n",
        "- SGD: Efficient for redundant data\n",
        "- Minibatch SGD: Enables parallel processing\n",
        "- Momentum: Accelerates convergence\n",
        "- AdaGrad: Efficient preconditioning\n",
        "- RMSProp: Decoupled scaling\n",
        "\n",
        "## The Algorithm\n",
        "\n",
        "### State Variables\n",
        "\n",
        "- Uses exponential weighted moving averages\n",
        "- Momentum estimate:\n",
        "  $$\\mathbf{v}_t \\leftarrow \\beta_1 \\mathbf{v}_{t-1} + (1 - \\beta_1) \\mathbf{g}_t$$\n",
        "- Second moment estimate:\n",
        "  $$\\mathbf{s}_t \\leftarrow \\beta_2 \\mathbf{s}_{t-1} + (1 - \\beta_2) \\mathbf{g}_t^2$$\n",
        "- Typical values: $\\beta_1 = 0.9$, $\\beta_2 = 0.999$\n",
        "\n",
        "### Bias Correction\n",
        "\n",
        "- Initial bias towards smaller values\n",
        "- Normalized state variables:\n",
        "  $$\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_1^t}$$\n",
        "  $$\\hat{\\mathbf{s}}_t = \\frac{\\mathbf{s}_t}{1 - \\beta_2^t}$$\n",
        "\n",
        "---\n",
        "\n",
        "### Update Rule\n",
        "\n",
        "- Rescaled gradient:\n",
        "  $$\\mathbf{g}_t' = \\frac{\\eta \\hat{\\mathbf{v}}_t}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon}$$\n",
        "- Parameter update:\n",
        "  $$\\mathbf{x}_t \\leftarrow \\mathbf{x}_{t-1} - \\mathbf{g}_t'$$\n",
        "- Typically $\\epsilon = 10^{-6}$\n",
        "\n",
        "## Implementation\n",
        "\n",
        "### State Initialization"
      ],
      "id": "bd09a18d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import d2l\n",
        "import torch\n",
        "#| label: init-states\n",
        "def init_adam_states(feature_dim):\n",
        "    v_w, v_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n",
        "    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n",
        "    return ((v_w, s_w), (v_b, s_b))"
      ],
      "id": "f4d367bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adam Update"
      ],
      "id": "8c35b487"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: adam-update\n",
        "def adam(params, states, hyperparams):\n",
        "    beta1, beta2, eps = 0.9, 0.999, 1e-6\n",
        "    for p, (v, s) in zip(params, states):\n",
        "        with torch.no_grad():\n",
        "            # Update momentum\n",
        "            v[:] = beta1 * v + (1 - beta1) * p.grad\n",
        "            # Update second moment\n",
        "            s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad)\n",
        "            # Bias correction\n",
        "            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n",
        "            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n",
        "            # Parameter update\n",
        "            p[:] -= hyperparams['lr'] * v_bias_corr / (\n",
        "                torch.sqrt(s_bias_corr) + eps)\n",
        "        p.grad.data.zero_()\n",
        "    hyperparams['t'] += 1"
      ],
      "id": "adam-update",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Training"
      ],
      "id": "18d485ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: training\n",
        "data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n",
        "d2l.train_ch11(adam, init_adam_states(feature_dim),\n",
        "               {'lr': 0.01, 't': 1}, data_iter, feature_dim)"
      ],
      "id": "training",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Concise Implementation"
      ],
      "id": "f71e12d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: concise-impl\n",
        "trainer = torch.optim.Adam\n",
        "d2l.train_concise_ch11(trainer, {'lr': 0.01}, data_iter)"
      ],
      "id": "concise-impl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        " \n",
        "## Summary\n",
        "\n",
        "- Adam combines multiple optimization techniques\n",
        "- Key features:\n",
        "  - Momentum from RMSProp\n",
        "  - Bias correction\n",
        "  - Learning rate control\n",
        "- Yogi variant:\n",
        "  - Addresses convergence issues\n",
        "  - Modified second moment update\n",
        "  - Better variance control\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with learning rate adjustments\n",
        "2. Rewrite momentum updates without bias correction\n",
        "3. Analyze learning rate reduction during convergence\n",
        "4. Construct divergence cases for Adam vs Yogi \n",
        "\n",
        "\n",
        " \n",
        "# Sensor Fusion as a Regression Problem\n",
        "\n",
        "- High-resolution chemical imaging in STEM is limited by inelastic scattering.\n",
        "- HAADF gives high SNR but lacks chemical specificity.\n",
        "- EDX/EELS gives chemistry but is noisy at low dose.\n",
        "- Goal: Fuse both signals for high-quality chemical maps.\n",
        "\n",
        "---\n",
        "\n",
        "## Data Fusion as Inverse Problem\n",
        "\n",
        "**Reconstruction goal:**\n",
        "$$\n",
        "\\hat{x} = \\arg\\min_{x \\geq 0} \\; \\Psi_1(x) + \\lambda_1 \\Psi_2(x) + \\lambda_2 \\text{TV}(x)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\Psi_1$: HAADF model loss\n",
        "- $\\Psi_2$: spectroscopic data fidelity\n",
        "- $\\text{TV}(x)$: regularization term\n",
        "\n",
        "---\n",
        "\n",
        "## First Term: HAADF Consistency\n",
        "\n",
        "**HAADF image model:**\n",
        "$$\n",
        "\\Psi_1(x) = \\frac{1}{2} \\| b_H - A x^\\gamma \\|_2^2\n",
        "$$\n",
        "\n",
        "- $b_H$: measured HAADF signal\n",
        "- $x^\\gamma$: element-wise power (Z-contrast)\n",
        "- $\\gamma \\approx 1.7$: approximates Z-contrast\n",
        "\n",
        "**Interpretation:**\n",
        "Ensure the fused chemical map explains HAADF contrast.\n",
        "\n",
        "---\n",
        "\n",
        "## Second Term: Spectroscopic Fidelity\n",
        "\n",
        "**Poisson noise model for EDX/EELS:**\n",
        "$$\n",
        "\\Psi_2(x) = \\sum_i 1^T x_i - b_i^T \\log(x_i + \\varepsilon)\n",
        "$$\n",
        "\n",
        "- $x_i$: reconstructed map of element $i$\n",
        "- $b_i$: measured EDX/EELS signal for element $i$\n",
        "- $\\varepsilon$: small constant to avoid $\\log(0)$\n",
        "\n",
        "**Interpretation:**\n",
        "Match fused maps with noisy spectroscopic measurements.\n",
        "\n",
        "---\n",
        "\n",
        "## Third Term: Total Variation (TV)\n",
        "\n",
        "**Channel-wise total variation:**\n",
        "$$\n",
        "\\text{TV}(x) = \\sum_i \\|x_i\\|_{TV}\n",
        "$$\n",
        "\n",
        "**Purpose:**\n",
        "- Promote piecewise smooth maps\n",
        "- Reduce noise\n",
        "- Preserve edges\n",
        "\n",
        "**Popular in:**\n",
        "- Compressed sensing\n",
        "- Image denoising\n",
        "\n",
        "---\n",
        "\n",
        "## Summary of Loss Terms\n",
        "\n",
        "| Term | Meaning | Benefit |\n",
        "|------|---------|---------|\n",
        "| $\\Psi_1$ | HAADF consistency | Uses high SNR elastic signal |\n",
        "| $\\Psi_2$ | Spectroscopy fidelity | Honors noisy chemical data |\n",
        "| $\\text{TV}(x)$ | Regularization | Noise suppression and smoothness |\n",
        "\n",
        "All terms are necessary for accurate low-dose chemical recovery.\n",
        "\n",
        "---\n",
        "\n",
        "# Practical Results\n",
        "\n",
        "- Improves SNR by 300–500%.\n",
        "- Reduces required dose by >10×.\n",
        "- Recovers stoichiometry with <15% error.\n",
        "\n",
        " \n",
        "\n",
        "---\n",
        "\n",
        "# Takeaways\n",
        "\n",
        "- Multi-modal fusion = better signal, lower dose.\n",
        "- Expressed as interpretable optimization.\n",
        "- Each term plays a distinct role.\n",
        "\n",
        "**Future outlook:** Combine with additional modalities (e.g., ABF, ptychography).\n",
        "\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "## Overview\n",
        "\n",
        "- Tutorial on fusing EELS/X-EDS maps with HAADF for improved chemical resolution\n",
        "- Part 1 of 2: Atomic resolution HAADF and X-EDS dataset of DyScO$_3$\n",
        "- Python-based workflow with minimal user input (<10 tunable lines)\n",
        "- Quick transformation of datasets into resolution-enhanced chemical maps\n",
        "\n",
        "::: {.callout-note}\n",
        "## Example Output\n",
        "![Raw vs Fused DyScO$_3$](figs/Figure_3_Output.png){width=\"700px\"}\n",
        ":::\n",
        "\n",
        "## Experimental Requirements {.callout-warning}\n",
        "\n",
        "- Need both elastic (e.g., HAADF) and inelastic (e.g., EELS/X-EDS) maps\n",
        "- Elastic signal must provide Z-contrast\n",
        "- Inelastic signal must map all chemistries\n",
        "- All maps must have same dimensionality\n",
        "- Recommendation: Use simultaneously collected HAADF signal\n",
        "\n",
        "::: aside\n",
        "@Schwartz_2022, @manassa2024fused\n",
        ":::\n",
        "\n",
        "## Step 1: Python Imports"
      ],
      "id": "249554e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import data.fusion_utils as utils\n",
        "from scipy.sparse import spdiags\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm \n",
        "import numpy as np"
      ],
      "id": "3690d656",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Loading"
      ],
      "id": "3957fbac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = np.load('data/PTO_Trilayer_dataset.npz')\n",
        "# Define element names and their atomic weights\n",
        "elem_names=['Sc', 'Dy', 'O']\n",
        "elem_weights=[21,66,8]\n",
        "\n",
        "# Parse elastic HAADF data and inelastic chemical maps\n",
        "HAADF = data['HAADF']\n",
        "xx = np.array([],dtype=np.float32)\n",
        "for ee in elem_names:\n",
        "    chemMap = data[ee]\n",
        "    if chemMap.shape != HAADF.shape:\n",
        "        raise ValueError(f\"The dimensions of {ee} chemical map do not match HAADF dimensions.\")\n",
        "    chemMap -= np.min(chemMap); chemMap /= np.max(chemMap)\n",
        "    xx = np.concatenate([xx,chemMap.flatten()])"
      ],
      "id": "c1f1ad1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ::: {.callout-tip}\n",
        "## Loading Alternative Formats\n",
        "- For .dm3, .dm4, or .emd files: Use HyperSpy\n",
        "- Documentation: [HyperSpy IO Guide](https://hyperspy.org/hyperspy-doc/v1.3/user_guide/io.html)\n",
        "::: -->\n",
        "\n",
        "## Step 3: Data Reshaping"
      ],
      "id": "5161e8ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Make Copy of Raw Measurements\n",
        "xx0 = xx.copy()\n",
        "\n",
        "# Parameters\n",
        "gamma = 1.6  # Z-contrast scaling factor\n",
        "(nx, ny) = chemMap.shape; nPix = nx * ny\n",
        "nz = len(elem_names)\n",
        "\n",
        "# Initialize TV Regularizers\n",
        "reg = utils.tvlib(nx,ny)\n",
        "\n",
        "# Normalize HAADF\n",
        "HAADF -= np.min(HAADF); HAADF /= np.max(HAADF)\n",
        "HAADF = HAADF.flatten()\n",
        "\n",
        "# Create Measurement Matrix\n",
        "A = utils.create_weighted_measurement_matrix(nx,ny,nz,elem_weights,gamma,1)"
      ],
      "id": "bb4056c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Cost Function Parameters\n"
      ],
      "id": "9f4dc788"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convergence Parameters\n",
        "lambdaHAADF = 1/nz # 1/nz (do not modify)\n",
        "lambdaChem = 0.08 # 0.05-0.3 (data consistency)\n",
        "lambdaTV = 0.15 # <0.2  Total Variation denoising\n",
        "nIter = 30      # typically converges in 10-15\n",
        "bkg = 2.4e-1    # background subtraction\n",
        "\n",
        "# FGP TV Parameters\n",
        "regularize = True\n",
        "nIter_TV = 3"
      ],
      "id": "80a63ab6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Algorithm Execution"
      ],
      "id": "d1914267"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize\n",
        "xx = xx0.copy()\n",
        "\n",
        "# Cost Functions\n",
        "lsqFun = lambda inData : 0.5 * np.linalg.norm(A.dot(inData**gamma) - HAADF) **2\n",
        "poissonFun = lambda inData : np.sum(xx0 * np.log(inData + 1e-8) - inData)\n",
        "\n",
        "# Initialize Cost Tracking\n",
        "costHAADF = np.zeros(nIter,dtype=np.float32)\n",
        "costChem = np.zeros(nIter, dtype=np.float32)\n",
        "costTV = np.zeros(nIter, dtype=np.float32)\n",
        "\n",
        "# Main Loop\n",
        "for kk in tqdm(range(nIter)):\n",
        "    # Optimization\n",
        "    xx -= gamma * spdiags(xx**(gamma - 1), [0], nz*nx*ny, nz*nx*ny) * \\\n",
        "          lambdaHAADF * A.transpose() * (A.dot(xx**gamma) - HAADF) + \\\n",
        "          lambdaChem * (1 - xx0 / (xx + bkg))\n",
        "    \n",
        "    # Positivity Constraint\n",
        "    xx[xx<0] = 0\n",
        "    \n",
        "    # TV Regularization\n",
        "    if regularize:\n",
        "        for zz in range(nz):\n",
        "            xx[zz*nPix:(zz+1)*nPix] = reg.fgp_tv(\n",
        "                xx[zz*nPix:(zz+1)*nPix].reshape(nx,ny), \n",
        "                lambdaTV, \n",
        "                nIter_TV\n",
        "            ).flatten()\n",
        "            costTV[kk] += reg.tv(xx[zz*nPix:(zz+1)*nPix].reshape(nx,ny))\n",
        "    \n",
        "    # Track Costs\n",
        "    costHAADF[kk] = lsqFun(xx)\n",
        "    costChem[kk] = poissonFun(xx)"
      ],
      "id": "d6fe2f5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Convergence Assessment\n",
        "\n",
        "::: {.callout-important}\n",
        "## Convergence Criteria\n",
        "- All 3 cost functions should asymptotically approach low values\n",
        "- Look for:\n",
        "  - Exponential decay\n",
        "  - Brief overshooting (Lennard-Jones-like)\n",
        "  - Avoid:\n",
        "    - Incomplete convergence\n",
        "    - Severe oscillations\n",
        ":::\n",
        "\n",
        "![Convergence Plot](figs/Figure_4_Convergence.png){width=\"700px\"}\n",
        "\n",
        "## TV Weighting Effects {.callout-attention}\n",
        "\n",
        "![TV Weighting Comparison](figs/Figure_5_TV.png){width=\"700px\"}\n",
        "\n",
        "::: {.callout-warning}\n",
        "## TV Weighting Guidelines\n",
        "- Under-weighting: Results in noisy reconstructions\n",
        "- Over-weighting: Causes blurring and feature loss\n",
        "- Best practice: Err on side of under-weighting\n",
        "  - Noise is familiar to data\n",
        "  - Oversmoothing creates unphysical artifacts\n",
        ":::\n",
        "\n",
        "## Results Visualization"
      ],
      "id": "161e93be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display Cost Functions\n",
        "utils.plot_convergence(costHAADF, lambdaHAADF, \n",
        "                      costChem, lambdaChem, \n",
        "                      costTV, lambdaTV)"
      ],
      "id": "ef53983c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ],
      "id": "73beef5e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show Reconstructed Signal\n",
        "fig, ax = plt.subplots(2,len(elem_names)+1,figsize=(12,6.5))\n",
        "ax = ax.flatten()\n",
        "ax[0].imshow((A.dot(xx**gamma)).reshape(nx,ny),cmap='gray'); ax[0].set_title('HAADF'); ax[0].axis('off')\n",
        "ax[1+len(elem_names)].imshow((A.dot(xx**gamma)).reshape(nx,ny)[70:130,25:85],cmap='gray'); ax[1+len(elem_names)].set_title('HAADF Cropped'); ax[1+len(elem_names)].axis('off')\n",
        "\n",
        "for ii in range(len(elem_names)):\n",
        "    ax[ii+1].imshow(xx[ii*(nx*ny):(ii+1)*(nx*ny)].reshape(nx,ny),cmap='gray')\n",
        "    ax[ii+2+len(elem_names)].imshow(xx[ii*(nx*ny):(ii+1)*(nx*ny)].reshape(nx,ny)[70:130,25:85],cmap='gray')\n",
        "    \n",
        "    ax[ii+1].set_title(elem_names[ii])\n",
        "    ax[ii+1].axis('off')\n",
        "    ax[ii+2+len(elem_names)].set_title(elem_names[ii]+' Cropped')\n",
        "    ax[ii+2+len(elem_names)].axis('off')\n",
        "\n",
        "fig.tight_layout()"
      ],
      "id": "4ba06ba2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Best Practices Summary\n",
        "1. Ensure proper data collection\n",
        "2. Verify dimensional consistency\n",
        "3. Start with recommended parameter ranges\n",
        "4. Monitor convergence carefully\n",
        "5. Validate results against physical expectations\n",
        "::: \n",
        "\n",
        "\n",
        "## References  \n",
        "::: {#refs} \n",
        ":::\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<script>\n",
        "document.getElementById(\"marimo-frame\").onload = function() {\n",
        "    try {\n",
        "        let iframeDoc = document.getElementById(\"marimo-frame\").contentWindow.document;\n",
        "        let marimoBadge = iframeDoc.querySelector(\"div.fixed.bottom-0.right-0.z-50\");\n",
        "        if (marimoBadge) {\n",
        "            marimoBadge.style.display = \"none\";\n",
        "            console.log(\"Marimo badge hidden successfully.\");\n",
        "        } else {\n",
        "            console.log(\"Badge not found.\");\n",
        "        }\n",
        "    } catch (error) {\n",
        "        console.warn(\"Unable to modify iframe content due to CORS restrictions.\");\n",
        "    }\n",
        "};\n",
        "</script>\n",
        "</div>"
      ],
      "id": "f3d90fdd"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}