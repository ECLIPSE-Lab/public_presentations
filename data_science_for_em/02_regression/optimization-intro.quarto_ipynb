{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ---\n",
        "title: \"Optimization and Deep Learning\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Optimization and Deep Learning\n",
        "\n",
        "- Optimization and deep learning are closely related\n",
        "- Deep learning typically involves:\n",
        "  - Defining a loss function\n",
        "  - Using optimization to minimize the loss\n",
        "- Note: Most optimization algorithms minimize by convention\n",
        "  - To maximize: simply flip the sign of the objective\n",
        "\n",
        "## Goals of Optimization vs Deep Learning\n",
        "\n",
        "::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "### Optimization\n",
        "- Primary goal: Minimize objective function\n",
        "- Focus on training error\n",
        "- Direct mathematical approach\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "### Deep Learning\n",
        "- Primary goal: Find suitable model\n",
        "- Focus on generalization error\n",
        "- Must handle finite data\n",
        "- Must prevent overfitting\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Visualizing the Difference\n",
        "\n",
        "Let's examine empirical risk vs. risk:\n"
      ],
      "id": "bce17955"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup3\n",
        "#| code-fold: true\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import numpy as np\n",
        "from mpl_toolkits import mplot3d\n",
        "import torch\n",
        "\n",
        "def f(x):\n",
        "    return x * d2l.cos(np.pi * x)\n",
        "\n",
        "def g(x):\n",
        "    return f(x) + 0.2 * d2l.cos(5 * np.pi * x)\n",
        "\n",
        "def annotate(text, xy, xytext):\n",
        "    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,\n",
        "                           arrowprops=dict(arrowstyle='->'))\n",
        "\n",
        "x = d2l.arange(0.5, 1.5, 0.01)\n",
        "d2l.set_figsize((4.5, 2.5))\n",
        "d2l.plot(x, [f(x), g(x)], 'x', 'risk')\n",
        "annotate('min of\\nempirical risk', (1.0, -1.2), (0.5, -1.1))\n",
        "annotate('min of risk', (1.1, -1.05), (0.95, -0.5))"
      ],
      "id": "setup3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Challenges in Deep Learning Optimization\n",
        "\n",
        "1. Local Minima\n",
        "2. Saddle Points\n",
        "3. Vanishing Gradients\n",
        "\n",
        "## Local Minima\n",
        "\n",
        "- Definition: Point where function value is smaller than nearby points\n",
        "- Global minimum: Smallest value over entire domain\n",
        "- Example function: $f(x) = x \\cdot \\textrm{cos}(\\pi x)$\n"
      ],
      "id": "ae0ebab2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: local-minima1\n",
        "#| code-fold: true\n",
        "x = d2l.arange(-1.0, 2.0, 0.01)\n",
        "d2l.plot(x, [f(x), ], 'x', 'f(x)')\n",
        "annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))\n",
        "annotate('global minimum', (1.1, -0.95), (0.6, 0.8))"
      ],
      "id": "local-minima1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Impact of Local Minima\n",
        "\n",
        "- Deep learning models often have many local optima\n",
        "- Gradient approaches zero near local minimum\n",
        "- Minibatch SGD can help escape local minima\n",
        "  - Natural gradient variation provides \"noise\"\n",
        "  - Can dislodge parameters from local minima\n",
        "\n",
        "## Saddle Points\n",
        "\n",
        "- Characteristics:\n",
        "  - All gradients vanish\n",
        "  - Neither global nor local minimum\n",
        "- Example: $f(x) = x^3$\n",
        "  - First and second derivatives vanish at $x=0$\n",
        "  - Optimization can stall here\n"
      ],
      "id": "b9b00ea9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: saddle-points\n",
        "#| code-fold: true\n",
        "x = d2l.arange(-2.0, 2.0, 0.01)\n",
        "d2l.plot(x, [x**3], 'x', 'f(x)')\n",
        "annotate('saddle point', (0, -0.2), (-0.52, -5.0))"
      ],
      "id": "saddle-points",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Higher-Dimensional Saddle Points\n",
        "\n",
        "- More complex in higher dimensions\n",
        "- Example: $f(x,y) = x^2 - y^2$\n",
        "- Has saddle point at $(0,0)$\n",
        "  - Maximum with respect to $y$\n",
        "  - Minimum with respect to $x$\n"
      ],
      "id": "1ca0a47f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 3d-saddle\n",
        "#| code-fold: true\n",
        "x, y = d2l.meshgrid(\n",
        "    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))\n",
        "z = x**2 - y**2\n",
        "\n",
        "ax = d2l.plt.figure().add_subplot(111, projection='3d')\n",
        "ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})\n",
        "ax.plot([0], [0], [0], 'rx')\n",
        "ticks = [-1, 0, 1]\n",
        "d2l.plt.xticks(ticks)\n",
        "d2l.plt.yticks(ticks)\n",
        "ax.set_zticks(ticks)\n",
        "d2l.plt.xlabel('x')\n",
        "d2l.plt.ylabel('y')"
      ],
      "id": "d-saddle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hessian Matrix Analysis\n",
        "\n",
        "For a $k$-dimensional input vector:\n",
        "\n",
        "- All positive eigenvalues → Local minimum\n",
        "- All negative eigenvalues → Local maximum\n",
        "- Mixed signs → Saddle point\n",
        "\n",
        "## Vanishing Gradients\n",
        "\n",
        "- Most insidious optimization problem\n",
        "- Example: $f(x) = \\tanh(x)$\n",
        "  - At $x = 4$: gradient ≈ 0.0013\n",
        "  - Optimization stalls\n",
        "- Historical context:\n",
        "  - Major challenge before ReLU activation\n",
        "  - Made deep learning training difficult\n"
      ],
      "id": "c8d95acd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: vanishing-gradients\n",
        "#| code-fold: true\n",
        "x = d2l.arange(-2.0, 5.0, 0.01)\n",
        "d2l.plot(x, [d2l.tanh(x)], 'x', 'f(x)')\n",
        "annotate('vanishing gradient', (4, 1), (2, 0.0))"
      ],
      "id": "vanishing-gradients",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Key takeaways:\n",
        "  - Training error minimization ≠ best generalization\n",
        "  - Many local minima exist\n",
        "  - Saddle points are common in non-convex problems\n",
        "  - Vanishing gradients can stall optimization\n",
        "- Good news:\n",
        "  - Robust algorithms exist\n",
        "  - Perfect solutions not always necessary\n",
        "  - Local optima can be useful\n",
        "  - Many practical solutions available\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Consider a simple MLP with a single hidden layer of $d$ dimensions:\n",
        "   - Show that for any local minimum there are at least $d!$ equivalent solutions\n",
        "   - Why does this happen?\n",
        "\n",
        "2. For a symmetric random matrix $\\mathbf{M}$:\n",
        "   - Prove that eigenvalue distribution is symmetric\n",
        "   - Why doesn't this imply $P(\\lambda > 0) = 0.5$?\n",
        "\n",
        "3. Additional challenges in deep learning optimization?\n",
        "\n",
        "4. Balancing a ball on a saddle:\n",
        "   - Why is this hard?\n",
        "   - How might this relate to optimization algorithms? "
      ],
      "id": "3f48ac77"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}