{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ---\n",
        "title: \"Gradient Descent\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- Gradient descent is fundamental to understanding optimization\n",
        "- Key concepts apply to more advanced algorithms\n",
        "- Important considerations:\n",
        "  - Learning rate selection\n",
        "  - Divergence issues\n",
        "  - Preconditioning techniques\n",
        "\n",
        "## One-Dimensional Gradient Descent\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "- For continuously differentiable $f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
        "- Taylor expansion:\n",
        "  $$f(x + \\epsilon) = f(x) + \\epsilon f'(x) + \\mathcal{O}(\\epsilon^2)$$\n",
        "- Moving in negative gradient direction:\n",
        "  - Choose $\\epsilon = -\\eta f'(x)$\n",
        "  - Fixed step size $\\eta > 0$\n",
        "  - Results in: $f(x - \\eta f'(x)) \\lessapprox f(x)$\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation\n"
      ],
      "id": "ff4e4adc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup2\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def f(x):  # Objective function\n",
        "    return x ** 2\n",
        "\n",
        "def f_grad(x):  # Gradient (derivative) of the objective function\n",
        "    return 2 * x"
      ],
      "id": "setup2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Gradient Descent\n"
      ],
      "id": "62741a8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: gd-implementation\n",
        "def gd(eta, f_grad):\n",
        "    x = 10.0\n",
        "    results = [x]\n",
        "    for i in range(10):\n",
        "        x -= eta * f_grad(x)\n",
        "        results.append(float(x))\n",
        "    print(f'epoch 10, x: {x:f}')\n",
        "    return results\n",
        "\n",
        "results = gd(0.2, f_grad)"
      ],
      "id": "gd-implementation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Visualization\n"
      ],
      "id": "aaa0f8a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: gd-visualization\n",
        "def show_trace(results, f):\n",
        "    n = max(abs(min(results)), abs(max(results)))\n",
        "    f_line = d2l.arange(-n, n, 0.01)\n",
        "    d2l.set_figsize()\n",
        "    d2l.plot([f_line, results], [[f(x) for x in f_line], [\n",
        "        f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
        "\n",
        "show_trace(results, f)"
      ],
      "id": "gd-visualization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Rate Effects\n",
        "\n",
        "### Too Small Learning Rate\n",
        "\n",
        "- Slow convergence\n",
        "- More iterations needed\n",
        "- Example with $\\eta = 0.05$:\n"
      ],
      "id": "e4ef8709"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: small-lr\n",
        "show_trace(gd(0.05, f_grad), f)"
      ],
      "id": "small-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Too Large Learning Rate\n",
        "\n",
        "- Solution oscillates\n",
        "- May diverge\n",
        "- Example with $\\eta = 1.1$:\n"
      ],
      "id": "8d56f0da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: large-lr\n",
        "show_trace(gd(1.1, f_grad), f)"
      ],
      "id": "large-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Local Minima\n",
        "\n",
        "- Nonconvex functions have multiple minima\n",
        "- Example: $f(x) = x \\cdot \\cos(cx)$\n",
        "- High learning rates can lead to poor local minima\n"
      ],
      "id": "093c83cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: local-minima\n",
        "c = d2l.tensor(0.15 * np.pi)\n",
        "\n",
        "def f(x):  # Objective function\n",
        "    return x * d2l.cos(c * x)\n",
        "\n",
        "def f_grad(x):  # Gradient of the objective function\n",
        "    return d2l.cos(c * x) - c * x * d2l.sin(c * x)\n",
        "\n",
        "show_trace(gd(2, f_grad), f)"
      ],
      "id": "local-minima",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multivariate Gradient Descent\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "- For $f: \\mathbb{R}^d \\to \\mathbb{R}$\n",
        "- Gradient vector: $\\nabla f(\\mathbf{x}) = [\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}]^\\top$\n",
        "- Taylor expansion:\n",
        "  $$f(\\mathbf{x} + \\boldsymbol{\\epsilon}) = f(\\mathbf{x}) + \\mathbf{\\boldsymbol{\\epsilon}}^\\top \\nabla f(\\mathbf{x}) + \\mathcal{O}(\\|\\boldsymbol{\\epsilon}\\|^2)$$\n",
        "- Update rule: $\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f(\\mathbf{x})$\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation\n"
      ],
      "id": "aff27ed7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "code-block-height": 800
      },
      "source": [
        "#| label: multivariate-gd\n",
        "def train_2d(trainer, steps=20, f_grad=None):\n",
        "    \"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n",
        "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
        "    results = [(x1, x2)]\n",
        "    for i in range(steps):\n",
        "        if f_grad:\n",
        "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n",
        "        else:\n",
        "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n",
        "        results.append((x1, x2))\n",
        "    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n",
        "    return results\n",
        "\n",
        "def show_trace_2d(f, results):\n",
        "    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n",
        "    d2l.set_figsize()\n",
        "    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
        "    x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),\n",
        "                          d2l.arange(-3.0, 1.0, 0.1), indexing='ij')\n",
        "    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
        "    d2l.plt.xlabel('x1')\n",
        "    d2l.plt.ylabel('x2')"
      ],
      "id": "multivariate-gd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Example: Quadratic Function\n"
      ],
      "id": "fdabdb45"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: quadratic-example\n",
        "def f_2d(x1, x2):  # Objective function\n",
        "    return x1 ** 2 + 2 * x2 ** 2\n",
        "\n",
        "def f_2d_grad(x1, x2):  # Gradient of the objective function\n",
        "    return (2 * x1, 4 * x2)\n",
        "\n",
        "def gd_2d(x1, x2, s1, s2, f_grad):\n",
        "    g1, g2 = f_grad(x1, x2)\n",
        "    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n",
        "\n",
        "eta = 0.1\n",
        "show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))"
      ],
      "id": "quadratic-example",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Methods\n",
        "\n",
        "### Newton's Method\n",
        "\n",
        "- Uses second-order information\n",
        "- Taylor expansion with Hessian:\n",
        "  $$f(\\mathbf{x} + \\boldsymbol{\\epsilon}) = f(\\mathbf{x}) + \\boldsymbol{\\epsilon}^\\top \\nabla f(\\mathbf{x}) + \\frac{1}{2} \\boldsymbol{\\epsilon}^\\top \\nabla^2 f(\\mathbf{x}) \\boldsymbol{\\epsilon} + \\mathcal{O}(\\|\\boldsymbol{\\epsilon}\\|^3)$$\n",
        "- Update rule: $\\boldsymbol{\\epsilon} = -\\mathbf{H}^{-1} \\nabla f(\\mathbf{x})$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Implementation\n"
      ],
      "id": "a198da16"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: newton-method\n",
        "c = d2l.tensor(0.5)\n",
        "\n",
        "def f(x):  # Objective function\n",
        "    return d2l.cosh(c * x)\n",
        "\n",
        "def f_grad(x):  # Gradient of the objective function\n",
        "    return c * d2l.sinh(c * x)\n",
        "\n",
        "def f_hess(x):  # Hessian of the objective function\n",
        "    return c**2 * d2l.cosh(c * x)\n",
        "\n",
        "def newton(eta=1):\n",
        "    x = 10.0\n",
        "    results = [x]\n",
        "    for i in range(10):\n",
        "        x -= eta * f_grad(x) / f_hess(x)\n",
        "        results.append(float(x))\n",
        "    print('epoch 10, x:', x)\n",
        "    return results\n",
        "\n",
        "show_trace(newton(), f)"
      ],
      "id": "newton-method",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Nonconvex Example\n"
      ],
      "id": "cb1c9821"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: nonconvex-newton\n",
        "c = d2l.tensor(0.15 * np.pi)\n",
        "\n",
        "def f(x):  # Objective function\n",
        "    return x * d2l.cos(c * x)\n",
        "\n",
        "def f_grad(x):  # Gradient of the objective function\n",
        "    return d2l.cos(c * x) - c * x * d2l.sin(c * x)\n",
        "\n",
        "def f_hess(x):  # Hessian of the objective function\n",
        "    return - 2 * c * d2l.sin(c * x) - x * c**2 * d2l.cos(c * x)\n",
        "\n",
        "show_trace(newton(0.5), f)"
      ],
      "id": "nonconvex-newton",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preconditioning\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- Avoid full Hessian computation\n",
        "- Use diagonal entries only\n",
        "- Update rule: $\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\textrm{diag}(\\mathbf{H})^{-1} \\nabla f(\\mathbf{x})$\n",
        "- Benefits:\n",
        "  - Different learning rates per variable\n",
        "  - Handles scale mismatches\n",
        "  - More efficient than full Newton's method\n",
        "\n",
        "## Summary\n",
        "\n",
        "- Learning rate selection is crucial\n",
        "- Local minima can trap gradient descent\n",
        "- High dimensions require careful learning rate adjustment\n",
        "- Preconditioning helps with scale issues\n",
        "- Newton's method:\n",
        "  - Fast convergence for convex problems\n",
        "  - Requires careful handling for nonconvex problems\n",
        "  - Computationally expensive for large problems\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with different learning rates and objective functions\n",
        "2. Implement line search for convex optimization\n",
        "3. Design a slow-converging 2D objective function\n",
        "4. Implement lightweight Newton's method with preconditioning\n",
        "5. Test algorithms on rotated coordinate systems "
      ],
      "id": "d52e673e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}