{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ---\n",
        "title: \"Stochastic Gradient Descent\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: custom.scss\n",
        "    css: custom.css\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    menu:\n",
        "      side: right\n",
        "      width: wide\n",
        "    template-partials:\n",
        "      - title-slide.html\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    highlight-style: a11y\n",
        "    incremental: false\n",
        "    background-transition: fade\n",
        "    footer: \"©Philipp Pelz - FAU Erlangen-Nürnberg - Data Science for Electron Microscopy\"\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "--- -->\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "\n",
        "- Previously used SGD without detailed explanation\n",
        "- Now diving deeper into its principles\n",
        "- Building on gradient descent fundamentals\n",
        "- Understanding why and how it works\n"
      ],
      "id": "971e9bb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup4\n",
        "#| code-fold: true\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "import math\n",
        "import torch"
      ],
      "id": "setup4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradient Updates\n",
        "\n",
        "### Objective Function\n",
        "\n",
        "- Training dataset with $n$ examples\n",
        "- Loss function $f_i(\\mathbf{x})$ for example $i$\n",
        "- Overall objective:\n",
        "  $$f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\mathbf{x})$$\n",
        "- Full gradient:\n",
        "  $$\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x})$$\n",
        "\n",
        "---\n",
        "\n",
        "### Computational Cost\n",
        "\n",
        "- Gradient descent: $\\mathcal{O}(n)$ per iteration\n",
        "- SGD: $\\mathcal{O}(1)$ per iteration\n",
        "- Update rule:\n",
        "  $$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x})$$\n",
        "- Unbiased estimate:\n",
        "  $$\\mathbb{E}_i \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x})$$\n",
        "\n",
        "### Implementation\n"
      ],
      "id": "69cc4e56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: objective-fn\n",
        "def f(x1, x2):  # Objective function\n",
        "    return x1 ** 2 + 2 * x2 ** 2\n",
        "\n",
        "def f_grad(x1, x2):  # Gradient of the objective function\n",
        "    return 2 * x1, 4 * x2"
      ],
      "id": "objective-fn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ],
      "id": "64376748"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: sgd-implementation\n",
        "def sgd(x1, x2, s1, s2, f_grad):\n",
        "    g1, g2 = f_grad(x1, x2)\n",
        "    # Simulate noisy gradient\n",
        "    g1 += torch.normal(0.0, 1, (1,)).item()\n",
        "    g2 += torch.normal(0.0, 1, (1,)).item()\n",
        "    eta_t = eta * lr()\n",
        "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)"
      ],
      "id": "sgd-implementation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: constant-lr\n",
        "def constant_lr():\n",
        "    return 1\n",
        "\n",
        "eta = 0.1\n",
        "lr = constant_lr  # Constant learning rate\n",
        "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
      ],
      "id": "constant-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Learning Rate\n",
        "\n",
        "### Learning Rate Strategies\n",
        "\n",
        "- Piecewise constant: $\\eta(t) = \\eta_i \\textrm{ if } t_i \\leq t \\leq t_{i+1}$\n",
        "- Exponential decay: $\\eta(t) = \\eta_0 \\cdot e^{-\\lambda t}$\n",
        "- Polynomial decay: $\\eta(t) = \\eta_0 \\cdot (\\beta t + 1)^{-\\alpha}$\n",
        "\n",
        "### Exponential Decay Implementation\n"
      ],
      "id": "e6f3522c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: exponential-lr\n",
        "def exponential_lr():\n",
        "    global t\n",
        "    t += 1\n",
        "    return math.exp(-0.1 * t)\n",
        "\n",
        "t = 1\n",
        "lr = exponential_lr\n",
        "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))"
      ],
      "id": "exponential-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Polynomial Decay Implementation\n"
      ],
      "id": "aef988c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: polynomial-lr\n",
        "def polynomial_lr():\n",
        "    global t\n",
        "    t += 1\n",
        "    return (1 + 0.1 * t) ** (-0.5)\n",
        "\n",
        "t = 1\n",
        "lr = polynomial_lr\n",
        "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
      ],
      "id": "polynomial-lr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradients and Finite Samples\n",
        "\n",
        "### Sampling Strategies\n",
        "\n",
        "- With replacement:\n",
        "  - Probability of choosing element: $1 - e^{-1} \\approx 0.63$\n",
        "  - Increased variance\n",
        "  - Decreased data efficiency\n",
        "- Without replacement:\n",
        "  - Better variance properties\n",
        "  - More efficient data usage\n",
        "  - Default choice in practice\n",
        "\n",
        "## Summary\n",
        "\n",
        "- Key points:\n",
        "  - SGD reduces computational cost to $\\mathcal{O}(1)$\n",
        "  - Learning rate scheduling is crucial\n",
        "  - Convergence guarantees for convex problems\n",
        "  - Sampling without replacement preferred\n",
        "- Practical considerations:\n",
        "  - Dynamic learning rates\n",
        "  - Trade-offs in sampling strategies\n",
        "  - Nonconvex optimization challenges\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Experiment with learning rate schedules\n",
        "2. Analyze noise in gradient updates\n",
        "3. Compare sampling strategies\n",
        "4. Investigate gradient coordinate scaling\n",
        "5. Study local minima in nonconvex functions "
      ],
      "id": "bcf9ede9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}