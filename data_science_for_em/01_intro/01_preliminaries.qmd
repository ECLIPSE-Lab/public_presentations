## Data Manipulation

- Data handling requires two main tasks:
  * Data acquisition
  * Data processing

- Key concepts for data manipulation:
  * $n$-dimensional arrays (tensors) are fundamental
  * Modern deep learning frameworks use tensor classes:
    - `ndarray` in MXNet
    - `Tensor` in PyTorch and TensorFlow
    - Similar to NumPy's `ndarray` with additional features
  * Key advantages of tensor classes:
    - Support automatic differentiation
    - GPU acceleration for numerical computation
    - NumPy only runs on CPUs

## Getting Started 1

- Import PyTorch:
```{python}
import torch
```

- Tensor basics:
  * Vector: tensor with one axis
  * Matrix: tensor with two axes
  * $k^\mathrm{th}$ order tensor: tensor with $k > 2$ axes

- Tensor creation:
  * Use `arange(n)` for evenly spaced values (0 to n-1)
  * Default storage: main memory
  * Default computation: CPU-based

## Getting Started 2

```{python}
x = torch.arange(12, dtype=torch.float32)
x
```

- Tensor elements:
  * Each value is an element
  * Use `numel()` to get total element count
  * Use `shape` attribute to get dimensions

```{python}
x.numel()
x.shape
```

- Reshaping tensors:
  * Use `reshape` to change shape without changing values
  * Example: vector (12,) → matrix (3, 4)
  * Elements maintain order (row-major)

```{python}
X = x.reshape(3, 4)
X
```

## Getting Started 3

- Shape inference:
  * Use `-1` to automatically infer one dimension
  * Example: `x.reshape(-1, 4)` or `x.reshape(3, -1)`
  * Given size $n$ and shape ($h$, $w$), $w = n/h$

- Common tensor initializations:
  * Zeros: `torch.zeros((2, 3, 4))`
  * Ones: `torch.ones((2, 3, 4))`
  * Random (Gaussian): `torch.randn(3, 4)`
  * Custom values: `torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])`

## Indexing and Slicing 1

- Access methods:
  * Indexing (0-based)
  * Negative indexing (from end)
  * Slicing (`start:stop`)
  * Single index/slice applies to axis 0

```{python}
X[-1], X[1:3]
```

- Element modification:
  * Use indexing for assignment
  * Example: `X[1, 2] = 17`

## Indexing and Slicing 2

- Multiple element assignment:
  * Use indexing on left side of assignment
  * `:` selects all elements along an axis
  * Works for vectors and higher-dimensional tensors

```{python}
X[:2, :] = 12
X
```

## Operations 1

- Elementwise operations:
  * Apply scalar operations to each element
  * Work with corresponding element pairs
  * Support unary operators (e.g., $e^x$)
  * Signature: $f: \mathbb{R} \rightarrow \mathbb{R}$

```{python}
torch.exp(x)
```

## Operations 2

- Binary operations:
  * Work on pairs of real numbers
  * Signature: $f: \mathbb{R}, \mathbb{R} \rightarrow \mathbb{R}$
  * Common operators:
    - Addition (`+`)
    - Subtraction (`-`)
    - Multiplication (`*`)
    - Division (`/`)
    - Exponentiation (`**`)

```{python}
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y
```

## Operations 3

- Tensor concatenation:
  * Use `torch.cat` with list of tensors
  * Specify axis for concatenation
  * Shape changes:
    - Axis 0: sum of input axis-0 lengths
    - Axis 1: sum of input axis-1 lengths

```{python}
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```

## Operations 4

- Logical operations:
  * Create binary tensors via logical statements
  * Example: `X == Y` creates tensor of 1s and 0s
  * Sum operation: `X.sum()` reduces to single element

```{python}
X == Y
X.sum()
```

## Broadcasting

- Mechanism for elementwise operations with different shapes:
  * Step 1: Expand arrays along length-1 axes
  * Step 2: Perform elementwise operation

```{python}
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a + b
```

## Saving Memory 1

- Memory allocation issues:
  * Operations create new memory allocations
  * Example: `Y = X + Y` creates new memory
  * Check with `id()` function
  * Undesirable for:
    - Frequent parameter updates
    - Multiple variable references

## Saving Memory 2

- In-place operations:
  * Use slice notation: `Y[:] = <expression>`
  * Use `zeros_like` for initialization
  * Use `X[:] = X + Y` or `X += Y` for efficiency

```{python}
Z = torch.zeros_like(Y)
Z[:] = X + Y
X += Y
```

## Conversion to Other Python Objects

- NumPy conversion:
  * `X.numpy()`: Tensor → NumPy array
  * `torch.from_numpy(A)`: NumPy array → Tensor
  * Shared memory between conversions

- Scalar conversion:
  * Use `item()` or built-in functions
  * Example: `float(a)`, `int(a)`

## Summary

- Tensor class features:
  * Data storage and manipulation
  * Construction routines
  * Indexing and slicing
  * Basic mathematics
  * Broadcasting
  * Memory-efficient operations
  * Python object conversion

## Exercises

1. Experiment with different conditional statements:
   * Try `X < Y` and `X > Y`
   * Observe resulting tensor types

2. Test broadcasting with 3D tensors:
   * Try different shapes
   * Verify results match expectations



