{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic Differentiation\n",
        "\n",
        "- Key points about derivatives in deep learning:\n",
        "  * Essential for optimization algorithms\n",
        "  * Used in training deep networks\n",
        "  * Manual calculation is:\n",
        "    - Tedious\n",
        "    - Error-prone\n",
        "    - More difficult with complex models\n",
        "\n",
        "- Modern deep learning frameworks provide:\n",
        "  * Automatic differentiation (autograd)\n",
        "  * Computational graph tracking\n",
        "  * Backpropagation implementation\n",
        "    - Works backwards through graph\n",
        "    - Applies chain rule\n",
        "    - Efficient gradient computation\n"
      ],
      "id": "2867ba07"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch"
      ],
      "id": "daa13738",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Simple Function\n",
        "\n",
        "- Goal: Differentiate $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to $\\mathbf{x}$\n",
        "- Initial setup:\n"
      ],
      "id": "763a3230"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = torch.arange(4.0)\n",
        "x"
      ],
      "id": "85c8123d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Gradient storage considerations:\n",
        "  * Need space to store gradients\n",
        "  * Avoid new memory allocation for each derivative\n",
        "  * Important because:\n",
        "    - Deep learning requires many derivative computations\n",
        "    - Same parameters used repeatedly\n",
        "    - Memory efficiency crucial\n",
        "  * Gradient shape matches input vector shape\n"
      ],
      "id": "79b8ab79"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x.requires_grad_(True)\n",
        "x.grad  # The gradient is None by default"
      ],
      "id": "85442ea2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "- Function calculation:\n"
      ],
      "id": "826a3adf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ],
      "id": "c54caab1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Gradient computation:\n",
        "  * Use `backward()` method\n",
        "  * Access via `grad` attribute\n",
        "  * Expected result: $4\\mathbf{x}$\n"
      ],
      "id": "87799fbd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y.backward()\n",
        "x.grad\n",
        "x.grad == 4 * x"
      ],
      "id": "f7afecba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Important note about gradient accumulation:\n",
        "  * PyTorch adds new gradients to existing ones\n",
        "  * Useful for optimizing sum of multiple objectives\n",
        "  * Reset with `x.grad.zero_()`\n"
      ],
      "id": "99e25dd2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x.grad.zero_()  # Reset the gradient\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ],
      "id": "299d5081",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backward for Non-Scalar Variables\n",
        "\n",
        "- Vector derivatives:\n",
        "  * Natural interpretation: Jacobian matrix\n",
        "  * Contains partial derivatives of each component\n",
        "  * Higher-order tensors for higher-order inputs\n",
        "\n",
        "- Common use case:\n",
        "  * Sum gradients of each component\n",
        "  * Often needed for batch processing\n",
        "  * Results in vector matching input shape\n",
        "\n",
        "- PyTorch implementation:\n",
        "  * Requires explicit reduction to scalar\n",
        "  * Uses vector $\\mathbf{v}$ for computation\n",
        "  * Computes $\\mathbf{v}^\\top \\partial_{\\mathbf{x}} \\mathbf{y}$\n",
        "  * Argument named `gradient` for historical reasons\n"
      ],
      "id": "b551f10f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n",
        "x.grad"
      ],
      "id": "26057ff3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detaching Computation\n",
        "\n",
        "- Purpose: Move calculations outside computational graph\n",
        "- Use cases:\n",
        "  * Create auxiliary terms without gradients\n",
        "  * Focus on direct influence of variables\n",
        "  * Control gradient flow\n",
        "\n",
        "- Example scenario:\n",
        "  * `z = x * y` and `y = x * x`\n",
        "  * Want direct influence of `x` on `z`\n",
        "  * Solution: Detach `y` to create `u`\n",
        "  * Results in:\n",
        "    - Same value as `y`\n",
        "    - No gradient flow through `u`\n",
        "    - Direct computation of `z = x * u`\n"
      ],
      "id": "02c59ceb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ],
      "id": "d0d5a6ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Important notes:\n",
        "  * Detaches ancestors from graph\n",
        "  * Original graph for `y` persists\n",
        "  * Can still compute gradients for `y`\n"
      ],
      "id": "977c3488"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ],
      "id": "ff8a3897",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradients and Python Control Flow\n",
        "\n",
        "- Key feature: Works with dynamic computation paths\n",
        "- Supports:\n",
        "  * Conditional statements\n",
        "  * Loops\n",
        "  * Arbitrary function calls\n",
        "  * Variable-dependent control flow\n",
        "\n",
        "- Example function:"
      ],
      "id": "0e9d04af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "id": "84552e51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- Implementation details:\n",
        "  * Graph built during execution\n",
        "  * Specific path for each input\n",
        "  * Supports backward pass after execution\n",
        "  * Works with linear functions and piecewise definitions\n"
      ],
      "id": "a48791de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()\n",
        "a.grad == d / a"
      ],
      "id": "9dcd9ab6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Real-world applications:\n",
        "  * Text processing with variable lengths\n",
        "  * Dynamic model architectures\n",
        "  * Statistical modeling\n",
        "  * Impossible to compute gradients a priori\n",
        "\n",
        "## Discussion\n",
        "\n",
        "- Impact of automatic differentiation:\n",
        "  * Massive productivity boost\n",
        "  * Enables complex model design\n",
        "  * Frees practitioners for higher-level tasks\n",
        "\n",
        "- Technical aspects:\n",
        "  * Optimization of autograd libraries\n",
        "  * Compiler and graph manipulation tools\n",
        "  * Memory efficiency\n",
        "  * Computational efficiency\n",
        "\n",
        "- Basic workflow:\n",
        "  1. Attach gradients to target variables\n",
        "  2. Record target value computation\n",
        "  3. Execute backpropagation\n",
        "  4. Access resulting gradient\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Backpropagation behavior:\n",
        "   * Run function twice\n",
        "   * Observe and explain results\n",
        "\n",
        "2. Control flow analysis:\n",
        "   * Change `a` to vector/matrix\n",
        "   * Analyze non-scalar results\n",
        "   * Explain computation changes\n",
        "\n",
        "3. Automatic differentiation practice:\n",
        "   * Plot $f(x) = \\sin(x)$\n",
        "   * Plot derivative using autograd\n",
        "   * Avoid using known derivative formula\n",
        "\n",
        "---\n",
        "\n",
        "4. Chain rule exercise:\n",
        "   * Function: $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$\n",
        "   * Create dependency graph\n",
        "   * Compute derivative using chain rule\n",
        "   * Map terms to dependency graph\n",
        "\n",
        "5. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$. \n",
        "\n",
        "---\n",
        "\n",
        "6. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously. \n"
      ],
      "id": "98f6fd25"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}