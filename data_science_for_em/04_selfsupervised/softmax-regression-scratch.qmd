 
## The Softmax Operation

::: {.callout-warning}
### Why Implement from Scratch?

- Fundamental understanding of softmax regression
- Builds on linear regression components
- Essential for deep learning foundations
:::



### Mathematical Definition

The softmax function transforms input values into probabilities:

$$\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}$$

Where:

- $\mathbf{X}$ is the input matrix
- $i,j$ are matrix indices
- The denominator is called the (log) *partition function*

## Implementation

```{python}
import d2l
import torch 
def softmax(X):
    X_exp = d2l.exp(X)
    partition = d2l.reduce_sum(X_exp, 1, keepdims=True)
    return X_exp / partition  # Broadcasting applied here
```

::: {.callout-warning}
## Important Note
The implementation above is not robust against very large or small arguments. Deep learning frameworks have built-in protections.
:::

## The Model Architecture

### Key Components

- Input: Flattened $28 \times 28$ pixel images (784-dimensional vectors)
- Output: 10 classes (Fashion-MNIST dataset)
- Weights: $784 \times 10$ matrix
- Biases: $1 \times 10$ vector

### Model Implementation

```{python}
import torch.nn as nn

class SoftmaxRegressionScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        # Initialize weights and biases as nn.Parameter
        self.W = nn.Parameter(torch.normal(0, sigma, size=(num_inputs, num_outputs)))
        self.b = nn.Parameter(torch.zeros(num_outputs))
    
    def parameters(self):
        return [self.W, self.b]
```

### Forward Pass

```{python}
@d2l.add_to_class(SoftmaxRegressionScratch)
def forward(self, X):
    X = d2l.reshape(X, (-1, self.W.shape[0]))
    return softmax(d2l.matmul(X, self.W) + self.b)
```

## Cross-Entropy Loss

### Mathematical Definition

The cross-entropy loss is defined as:

$$L(\mathbf{y}, \hat{\mathbf{y}}) = -\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^k y_{ij} \log(\hat{y}_{ij})$$

Where:

- $\mathbf{y}$ is the true label (one-hot encoded)
- $\hat{\mathbf{y}}$ is the predicted probability
- $n$ is the batch size
- $k$ is the number of classes

### Implementation

```{python}
def cross_entropy(y_hat, y):
    return -d2l.reduce_mean(d2l.log(y_hat[list(range(len(y_hat))), y]))
```

## Training Process

### Hyperparameters

- Number of epochs: 10
- Batch size: 256
- Learning rate: 0.1

### Training Code

```{python}
data = d2l.FashionMNIST(batch_size=256)
model = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)
trainer = d2l.Trainer(max_epochs=10)
# trainer.fit(model, data)
```

## Model Evaluation

### Prediction

```{python} 
X, y = next(iter(data.val_dataloader()))
preds = d2l.argmax(model(X), axis=1)
```

### Error Analysis

We focus on misclassified examples to understand model weaknesses.

## Summary

- Implemented softmax regression from scratch
- Used cross-entropy loss for classification
- Trained on Fashion-MNIST dataset
- Achieved reasonable classification performance

## Exercises

1. Numerical Stability
   - Test softmax with input value of 100
   - Test with largest input < -100
   - Implement numerical stability fix

2. Cross-Entropy Implementation
   - Implement alternative cross-entropy function
   - Analyze performance differences
   - Consider domain of logarithm

 

 