 

# Classification Introduction

- From linear regression to classification
- Moving from "how much?" to "which category?" questions
- Examples:
  - Spam vs. inbox classification
  - Customer subscription prediction
  - Image classification (donkey, dog, cat, rooster)
  - Movie recommendation
  - Book section prediction

## Classification Problem Types
### Hard vs. Soft Classification

- **Hard Classification**: Direct assignment to categories
- **Soft Classification**: Probability assessment for each category
- Often blurred in practice - even hard assignments use soft probabilities

### Multi-label Classification

- Multiple labels can be true simultaneously
- Example: News article covering entertainment, business, and space flight
- Not mutually exclusive categories

## Simple Image Classification Example

### Problem Setup

- Input: $2 \times 2$ grayscale image
- Features: $x_1, x_2, x_3, x_4$ (pixel values)
- Categories: "cat", "chicken", "dog"

### Label Representation

Two approaches:

1. Integer encoding: $y \in \{1, 2, 3\}$
2. One-hot encoding: 
   - "cat": $(1, 0, 0)$
   - "chicken": $(0, 1, 0)$
   - "dog": $(0, 0, 1)$

## Linear Model for Classification

### Model Structure

For 4 features and 3 categories:

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3
\end{aligned}
$$

### Vectorized Form

$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$

- $\mathbf{W} \in \mathbb{R}^{3 \times 4}$: weight matrix
- $\mathbf{b} \in \mathbb{R}^3$: bias vector

## The Softmax Function

### Why Softmax?

Problems with direct regression:
 
- No guarantee outputs sum to 1
- No guarantee of non-negative outputs
- No upper bound on probabilities

### Softmax Definition

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o}) \quad \textrm{where}\quad \hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)}$$

Key properties:

- Outputs are non-negative
- Sum to 1
- Preserves ordering: $\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j$

## Vectorization for Efficiency

### Batch Processing

For minibatch $\mathbf{X} \in \mathbb{R}^{n \times d}$:

$$
\begin{aligned} 
\mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}\\ 
\hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O})
\end{aligned}
$$

- Matrix-matrix product $\mathbf{X} \mathbf{W}$ is dominant operation
- Softmax computed rowwise
- Numerical stability considerations

## Loss Function: Cross-Entropy

### Log-Likelihood

For dataset with features $\mathbf{X}$ and one-hot labels $\mathbf{Y}$:

$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
$$

### Cross-Entropy Loss

$$l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j$$

Properties:

- Bounded below by 0
- Zero only with perfect prediction
- Never actually reaches zero for finite weights
