 
# Deep Kernel Learning - Combining Neural Networks and Gaussian Processes
## The Big Question  

::: {.callout-note}
## MacKay's Question (1998)

> "How can Gaussian processes possibly replace neural networks? Have we thrown the baby out with the bathwater?"

- Neural networks: Many design choices, lack of principled framework
- Gaussian processes: Flexible, interpretable, principled learning
- **Can we combine the best of both worlds?**
:::

## The Evolution of ML Paradigms 

::: {.columns}
::: {.column width="50%"}
### Neural Networks
- **Finite** adaptive basis functions
- Multiple layers of highly adaptive features
- Automatic representation discovery
- Inductive biases for specific domains

::: {.callout-tip}
## Key Insight
Neural networks can automatically discover meaningful representations in high-dimensional data
:::

:::

::: {.column width="50%"}
### Gaussian Processes
- **Infinite** fixed basis functions
- Non-parametric flexibility
- Automatic complexity calibration
- Uncertainty quantification

::: {.callout-tip}
## Key Insight
GPs with expressive kernels can discover rich structure without human intervention
:::

:::
:::

## The Deep Kernel Learning Idea  

::: {.callout-important}
## Core Concept
Transform the inputs of a base kernel with a deep architecture to create **scalable expressive closed-form kernels**
:::

::: {.columns}
::: {.column width="60%"}
### Mathematical Formulation

$$k(\mathbf{x}_i, \mathbf{x}_j | \boldsymbol{\theta}) \rightarrow k(g(\mathbf{x}_i, \mathbf{w}), g(\mathbf{x}_j, \mathbf{w}) | \boldsymbol{\theta}, \mathbf{w})$$

Where:

- $g(\mathbf{x}, \mathbf{w})$ = deep architecture (CNN, DNN)
- $k(\cdot, \cdot | \boldsymbol{\theta})$ = base kernel (RBF, Spectral Mixture)
- $\boldsymbol{\gamma} = \{\mathbf{w}, \boldsymbol{\theta}\}$ = all parameters

:::

::: {.column width="40%"}
### Key Benefits
- **Scalable**: $\mathcal{O}(n)$ training, $\mathcal{O}(1)$ prediction
- **Expressive**: Combines deep features with kernel flexibility
- **Non-parametric**: Automatic complexity calibration
- **Unified**: Joint learning through GP marginal likelihood
:::
:::

![Deep Kernel Learning](./img/kernelnetwork.png)

## Deep Kernel Architecture  

::: {.columns}
::: {.column width="50%"}
### Network Structure
1. **Input Layer**: Raw data $\mathbf{x}$
2. **Hidden Layers**: Deep transformation $g(\mathbf{x}, \mathbf{w})$
3. **Kernel Layer**: Infinite basis functions via GP
4. **Output**: Probabilistic predictions

::: {.callout-note}
## Infinite Hidden Units
The GP with base kernel provides an infinite number of basis functions in the final layer
:::

:::

::: {.column width="50%"}
### Learning Objective
Maximize the **marginal likelihood**:

$$\log p(\mathbf{y} | \boldsymbol{\gamma}, X) \propto -[\mathbf{y}^{\top}(K_{\boldsymbol{\gamma}}+\sigma^2 I)^{-1}\mathbf{y} + \log|K_{\boldsymbol{\gamma}} + \sigma^2 I|]$$

**Gradients via chain rule**:
$$\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = \frac{\partial \mathcal{L}}{\partial K_{\boldsymbol{\gamma}}} \frac{\partial K_{\boldsymbol{\gamma}}}{\partial g(\mathbf{x},\mathbf{w})} \frac{\partial g(\mathbf{x},\mathbf{w})}{\partial \mathbf{w}}$$
:::
:::

## Base Kernels: RBF vs Spectral Mixture  

::: {.columns}
::: {.column width="50%"}
### RBF Kernel
$$k_{\text{RBF}}(\mathbf{x}, \mathbf{x}') = \exp\left(-\frac{1}{2} \frac{\|\mathbf{x}-\mathbf{x}'\|^2}{\ell^2}\right)$$

**Properties**:

- Single length-scale parameter $\ell$
- Smooth, stationary
- Limited expressiveness

:::

::: {.column width="50%"}
### Spectral Mixture Kernel
$$k_{\text{SM}}(\mathbf{x}, \mathbf{x}' | \boldsymbol{\theta}) = \sum_{q=1}^Q a_q \frac{|\Sigma_q|^{1/2}}{(2\pi)^{D/2}} \exp\left(-\frac{1}{2} \|\Sigma_q^{1/2} (\mathbf{x}-\mathbf{x}')\|^2\right) \cos\langle \mathbf{x}-\mathbf{x}', 2\pi \boldsymbol{\mu}_q \rangle$$

**Properties**:

- Multiple components $Q$
- Quasi-periodic structure
- Much more expressive
:::
:::

## Scalability: KISS-GP  

::: {.callout-important}
## The Scalability Challenge
Standard GPs: $\mathcal{O}(n^3)$ complexity
**Goal**: Linear scaling $\mathcal{O}(n)$
:::

### KISS-GP Approximation

$$K_{\boldsymbol{\gamma}} \approx M K^{\text{deep}}_{U,U} M^{\top} := K_{\text{KISS}}$$

Where:

- $M$ = sparse interpolation matrix (4 non-zero entries per row)
- $K_{U,U}$ = covariance over inducing points $U$
- Kronecker + Toeplitz structure for fast MVMs

### Computational Benefits
- **Training**: $\mathcal{O}(n + h(m))$ where $h(m) \approx \mathcal{O}(m)$
- **Prediction**: $\mathcal{O}(1)$ per test point
- **Memory**: $\mathcal{O}(n)$ instead of $\mathcal{O}(n^2)$

## Experimental Results: UCI Datasets  

::: {.callout-tip}
## Key Finding
Deep Kernel Learning consistently outperforms both:
- Standalone deep neural networks
- Gaussian processes with expressive kernels
:::

::: {.columns}
::: {.column width="60%"}
### Performance Comparison
- **16 UCI regression datasets**
- **2M+ training examples** (Electric dataset)
- **DKL-SM** achieves best performance on most datasets
- **Minimal runtime overhead** (~10% additional cost)

:::

::: {.column width="40%"}
### Architecture Details
- **Small datasets** ($n < 6,000$): $[d\text{-}1000\text{-}500\text{-}50\text{-}2]$
- **Large datasets** ($n > 6,000$): $[d\text{-}1000\text{-}1000\text{-}500\text{-}50\text{-}2]$
- **SM kernel**: $Q=4$ (small), $Q=6$ (large)
:::
:::
 

## Learned Representations  

::: {.columns}
::: {.column width="50%"}
### Spectral Density Analysis
- **SM kernel**: Discovers two peaks in frequency domain
- **RBF kernel**: Single Gaussian, misses important correlations
- **Result**: SM captures quasi-periodic structure better

:::

::: {.column width="50%"}
### Covariance Matrix Analysis
- **DKL kernels**: Strong correlation for similar orientations
- **Standard RBF**: Diffuse correlations
- **Metric learning**: Learns orientation-aware similarity
:::

::: {.callout-tip}
## Visualization
The learned metric correlates faces with similar rotation angles, overcoming Euclidean distance limitations
:::

:::


## Scalability Analysis  

::: {.columns}
::: {.column width="50%"}
### Training Time Scaling
- **Linear scaling** with data size $n$
- **Slope â‰ˆ 1** in log-log plot
- **KISS-GP** enables large-scale training

:::

::: {.column width="50%"}
### Runtime Comparison
- **DNN**: ~7-5000s (depending on dataset size)
- **DKL**: ~10-5000s (minimal overhead)
- **Additional cost**: ~10% of DNN runtime
:::
::: {.callout-tip}
## Key Benefit
Scalability enables learning from large datasets where expressive representations matter most
:::
:::

## Step Function Recovery 

::: {.callout-important}
## Challenge
Recover step function with sharp discontinuities
:::

::: {.columns}
::: {.column width="60%"}
### Problem Characteristics
- **Multiple discontinuities**
- **Sharp changes** in covariance structure
- **Difficult for smooth kernels**

### Results
- **GP-RBF**: Smooth, misses discontinuities
- **GP-SM**: Better, but still smooth
- **DKL-SM**: Accurately captures discontinuities with reasonable uncertainty

:::

::: {.column width="40%"}
### Key Advantage
DKL provides **posterior predictive distributions** useful for:

- Reinforcement learning
- Bayesian optimization
- Uncertainty quantification

![Step Function Recovery](./img/syn_step_func.png){height=400px}
:::
:::




## Key Contributions 

 
### 1. Scalable Deep Kernels
- Linear scaling $\mathcal{O}(n)$ training
- $\mathcal{O}(1)$ prediction time
- Retains non-parametric representation

### 2. Expressive Power
- Combines deep architectures with kernel flexibility
- Spectral mixture base kernels
- Automatic complexity calibration

### 3. Unified Learning
- Joint optimization through GP marginal likelihood
- No separate pre-training required
- Drop-in replacement for standard kernels
 
 

## Summary 

::: {.callout-important}
## Deep Kernel Learning Successfully Combines:

### Neural Networks
- Automatic representation discovery
- Inductive biases for specific domains
- Scalable training procedures

### Gaussian Processes
- Non-parametric flexibility
- Uncertainty quantification
- Principled learning framework

### Result
**Scalable, expressive, and principled** machine learning approach that consistently outperforms both paradigms alone
:::

 

 

 
## Discussion Points
1. How does DKL compare to other kernel learning approaches?
2. What are the computational trade-offs?
3. When would you choose DKL over standalone DNNs or GPs?
4. How does the choice of base kernel affect performance?
 