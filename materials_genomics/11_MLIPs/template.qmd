---
title: |
  Materials Genomics<br>
  Lecture 11: Machine-Learned Interatomic Potentials
bibliography: ref.bib
# csl: custom.csl
author:
  - name: Prof. Dr. Philipp Pelz
    affiliation: 
      - FAU Erlangen-Nürnberg
      - Institute of Micro- and Nanostructure Research
   
execute: 
  eval: true
  echo: true
format: 
    revealjs: 
        chalkboard: true
        mermaid:
            theme: forest
        # mermaid-format: png
        # scroll-view:
        #     activate: true
        #     snap: mandatory
        #     layout: full 
        width: 1920
        height: 1080
        menu:
            side: right
            width: wide 
        template-partials:
            - title-slide.html
        css: custom.css
        theme: custom.scss
        slide-number: c/t    
        logo: "eclipse_logo_small.png"          
        highlight-style: a11y
        incremental: true 
        background-transition: fade
        footer: "©Philipp Pelz - FAU Erlangen-Nürnberg - Materials Genomics"
---
 


## Welcome

### Week 11 — Machine-Learned Interatomic Potentials

**Goals for today:**

- Understand what ML interatomic potentials are and why we need them  
- Get an overview of GAP, SNAP, MTP, NequIP (and their ideas)  
- See how ML potentials enable simulations of defects, diffusion, and mechanical behavior  
- Conceptually fit a tiny ML potential to toy data (ACE / SNAP-like)  

---

## Outline

1. From DFT to interatomic potentials  
2. What is a machine-learned interatomic potential?  
3. Overview of GAP, SNAP, MTP, NequIP  
4. Applications: defects, diffusion, mechanics  
5. Exercise: fit a tiny ML potential to toy data  
6. Summary  

---

## 1. From DFT to Interatomic Potentials

DFT:

- Accurate: quantum mechanical description  
- Expensive: scales poorly with system size & time  

Typical limits:

- 100–1000 atoms  
- Picoseconds to nanoseconds of MD  

But real phenomena:

- Defect migration  
- Phase transformations  
- Plastic deformation  
need:

- 10³–10⁶ atoms  
- Nanoseconds to microseconds (or more)

---

## Classical Interatomic Potentials

Examples:

- Lennard-Jones  
- EAM (embedded atom method)  
- Tersoff, Stillinger–Weber  

Pros:

- Very fast  
- Suitable for large MD simulations  

Cons:

- Hand-crafted functional forms  
- Limited transferability  
- Hard to capture complex chemistry  

---

## Why ML Interatomic Potentials?

Idea:

- Replace hand-crafted potential forms with **flexible ML models**  
- Fit them to:
  - DFT energies  
  - DFT forces  
  - DFT stresses  

Goal:

- “DFT accuracy at classical MD cost” (approximately)  

This is where GAP, SNAP, MTP, NequIP and friends live.

---

## 2. What Is an ML Interatomic Potential?

Most ML potentials assume:

\[
E_{\text{total}} = \sum_i E_i(\text{local environment of atom } i)
\]

Where:
- Local environment → descriptor (SOAP, SNAP basis, ACE basis, messages in GNNs)  
- \(E_i\) learned by ML (GP, linear model, neural network, etc.)  

From energy:
- Forces = negative gradient w.r.t atomic positions  
- Stresses from derivative w.r.t strain  

---

## Requirements for ML Potentials

They must be:

- **Invariant** under:
  - Global translation  
  - Global rotation  
  - Permutation of identical atoms  
- **Smooth** and differentiable (for forces)  
- **Data-efficient**  
- **Physically reasonable** outside training data (or at least fail gracefully)

Descriptors and architectures are designed to enforce these.

---

## 3. Overview of GAP, SNAP, MTP, NequIP

We won’t go into heavy math, just core ideas.

---

## 3.1 GAP — Gaussian Approximation Potentials

Key ingredients:

- Descriptors: SOAP (Smooth Overlap of Atomic Positions)  
- Model: Gaussian Process regression  

SOAP encodes local atomic environments:

- Radial + angular information  
- Rotation-invariant  
- Smooth with respect to atomic displacements  

GAP:

- Learns mapping: SOAP → local energy  
- With GP, giving:
  - Mean prediction  
  - Uncertainty estimate  

Pros:

- High accuracy  
- Built-in uncertainty  
- Strong use in many materials systems  

Cons:

- Computationally heavier than simple classical potentials  
- Kernel method scaling can be an issue for huge datasets  

---

## 3.2 SNAP — Spectral Neighbor Analysis Potential

Key ingredients:

- Descriptors: bispectrum coefficients (spherical harmonics of neighbor density)  
- Model: usually linear or low-order polynomial in descriptors  

SNAP:

- Expands neighbor density similarly to SOAP but uses bispectrum  
- Captures three-body correlations  

Pros:

- Structured, analytic descriptors  
- Often uses simple linear models → relatively fast  

Cons:

- Descriptor design complex  
- May need many coefficients for high accuracy  

---

## 3.3 MTP — Moment Tensor Potentials

Key idea:

- Use **moment tensor descriptors** for local environments:
  - Tensorial moments of neighbor positions and species  
  - Systematically improvable basis  

Model:

- Linear combinations of these basis descriptors  

Pros:

- Systematically improvable  
- Good trade-off between speed and accuracy  

Cons:

- Requires careful basis selection & regularization  
- Implementation complexity  

---

## 3.4 NequIP — Equivariant Neural Networks

Key idea:

- Use **equivariant message-passing neural networks**:
  - Inputs: positions + atomic types  
  - Outputs: energies and forces  
  - Network layers respect rotational symmetry  

NequIP:

- SE(3)-equivariant architecture  
- Learns its own local descriptors implicitly  
- Very high accuracy often with fewer parameters/data  

Pros:

- Strong performance for complex chemistries  
- Direct force learning  
- Equivariance encodes physics (rotations, reflections)  

Cons:

- More complex neural network machinery  
- Heavier training cost  
- Requires GPU compute for training  

---

## Summary of Models

| Potential | Descriptor | Model Type       | Notable Features        |
|----------|------------|------------------|-------------------------|
| GAP      | SOAP       | Gaussian Process | Uncertainty, data efficiency |
| SNAP     | Bispectrum | Linear/Polynomial| Analytic basis          |
| MTP      | Moment tensors | Linear     | Systematically improvable |
| NequIP   | Learned equivariant features | GNN | End-to-end, very expressive |

Different tools for different accuracy/speed needs.

---

## 4. Applications: Defects, Diffusion, Mechanical Behavior

ML potentials are used when:

- DFT is too expensive  
- Classical potentials are too inaccurate  

---

## 4.1 Defects

Examples:

- Vacancies, interstitials  
- Dislocations  
- Grain boundaries  

ML potentials can:

- Explore defect formation energies and structures  
- Map core structures of dislocations  
- Compute interactions of defects at large scales  

Why important:

- Defects control mechanical strength, diffusion, failure mechanisms  

---

## 4.2 Diffusion

Examples:

- Ionic diffusion in solid electrolytes  
- Hydrogen diffusion in metals  
- Vacancy-mediated diffusion in alloys  

ML potentials enable:

- Long MD runs at realistic temperatures  
- Multiple diffusion events  
- Calculation of diffusion coefficients, activation energies  

DFT alone often too expensive for many-hop trajectories.

---

## 4.3 Mechanical Behavior

Examples:

- Stress–strain curves  
- Yield behavior and plasticity  
- Crack propagation  
- Nanoindentation simulations  

ML potentials can:

- Simulate deformation of large supercells  
- Capture complex bonding changes under load  
- Provide insight into failure mechanisms  

They sit between:

- DFT (too small/short)  
- Classical potentials (too crude)  

---

## 5. Exercise: Fit a Tiny ML Potential to Toy Data

Goal:

- Show the **workflow** of fitting an ML potential on a small dataset  
- Not about production-grade accuracy, but about the **pipeline**  

We’ll do this conceptually with ACE / SNAP-style model.

---

## Step 1 — Generate Toy Data

Choose a simple system:

- E.g., FCC Al or Lennard-Jones-like toy system  

Generate configurations:

- Equilibrium structure at different volumes  
- Small random displacements of atoms  
- Simple finite-temperature MD trajectory from an existing potential or DFT mini-run  

For each configuration:

- Energies  
- Forces  

**PYTHONHERE**

(Conceptual: produce a small dataset of structures + E + F.)

---

## Step 2 — Compute Local Descriptors

Define a local descriptor:

- Simplified SNAP-like bispectrum  
- Or ACE-like basis (atomic cluster expansion)  
- Or even very simple radial basis features of neighbor distances  

For each atom in each configuration:

- Compute descriptor vector \(\mathbf{d}_i\)  

Goal:

- Represent local environment numerically  

**PYTHONHERE**

---

## Step 3 — Define the Local Energy Model

Simple linear model:

\[
E_i = \mathbf{w} \cdot \mathbf{d}_i
\]

Total energy:

\[
E_{\text{tot}} = \sum_i E_i
\]

Loss function:
- Fit to both energies and forces:

\[
\mathcal{L} = \lambda_E \sum_{\text{configs}} (E_{\text{pred}} - E_{\text{ref}})^2
+ \lambda_F \sum_{\text{configs}, i} \|\mathbf{F}_{i,\text{pred}} - \mathbf{F}_{i,\text{ref}}\|^2
\]

Forces from:
- Analytical derivatives of \(E_{\text{tot}}\) w.r.t. atomic positions (conceptually)

---

## Step 4 — Fit the Parameters

Treat \(\mathbf{w}\) as fit parameters:

- Use linear regression (if descriptors linear in parameters)  
- Or regularized regression (Ridge / LASSO)  

**PYTHONHERE**

Fit on:

- Training set of configurations  
- Optionally hold out validation set  

Outcome:
- A tiny ML potential: descriptor + weights  

---

## Step 5 — Test the Potential

Test on:

- New configurations:
  - Slightly different volumes  
  - Slightly different temperatures  

Compare:

- Predicted energies vs reference  
- Predicted forces vs reference  
- Qualitative behavior (e.g., equilibrium lattice parameter, elastic response)  

**PYTHONHERE**

---

## Step 6 — Discussion

Questions:

- Does the potential reproduce the equation of state?  
- Are forces qualitatively correct?  
- How many configurations did we need?  
- What happens if we train on too narrow a configuration space?  

This illustrates:

- Data needs & coverage  
- Generalization vs overfitting  
- The idea of active learning (selecting new configurations to improve the potential)  

---

## 6. Summary

**Today you learned:**

- ML interatomic potentials approximate DFT energies & forces from local environments  
- GAP, SNAP, MTP, NequIP are key families with different descriptors and models  
- ML potentials enable simulations of defects, diffusion, and mechanical behavior at scale  
- Fitting a tiny ML potential involves:
  - Generating reference data  
  - Choosing descriptors  
  - Fitting a model to energies & forces  
  - Validating on new configurations  

Next week:  

**Week 12 — Active Learning & Closed-Loop Potential Improvement**  
(Using ML uncertainty to choose new DFT calculations.)

---

## Questions?

Use the chalkboard!

<div>
<script>
document.getElementById("marimo-frame").onload = function() {
    try {
        let iframeDoc = document.getElementById("marimo-frame").contentWindow.document;
        let marimoBadge = iframeDoc.querySelector("div.fixed.bottom-0.right-0.z-50");
        if (marimoBadge) {
            marimoBadge.style.display = "none";
            console.log("Marimo badge hidden successfully.");
        } else {
            console.log("Badge not found.");
        }
    } catch (error) {
        console.warn("Unable to modify iframe content due to CORS restrictions.");
    }
};
</script>
</div>