[
  {
    "objectID": "test_python.html",
    "href": "test_python.html",
    "title": "ECLIPSE Presentations",
    "section": "",
    "text": "import sys\nprint(sys.executable)\nprint(sys.path)\n\n/home/philipp/miniforge3/bin/python\n['/home/philipp/projects/_public_presentations', '/home/philipp/miniforge3/lib/python310.zip', '/home/philipp/miniforge3/lib/python3.10', '/home/philipp/miniforge3/lib/python3.10/lib-dynload', '', '/home/philipp/miniforge3/lib/python3.10/site-packages', '/home/philipp/projects/scatterem']"
  },
  {
    "objectID": "test_d2l.html",
    "href": "test_d2l.html",
    "title": "ECLIPSE Presentations",
    "section": "",
    "text": "import sys\nprint(\"Python path:\", sys.path)\ntry:\n    from d2l import torch as d2l\n    print(\"Successfully imported d2l\")\nexcept ImportError as e:\n    print(\"Error importing d2l:\", str(e))\n\nPython path: ['/home/philipp/projects/_public_presentations', '/home/philipp/miniforge3/lib/python310.zip', '/home/philipp/miniforge3/lib/python3.10', '/home/philipp/miniforge3/lib/python3.10/lib-dynload', '', '/home/philipp/miniforge3/lib/python3.10/site-packages', '/home/philipp/projects/scatterem']\nSuccessfully imported d2l\n\n\n/home/philipp/miniforge3/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n/home/philipp/miniforge3/lib/python3.10/site-packages/torch/cuda/__init__.py:758: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/c10/cuda/CUDAFunctions.cpp:108.)\n  return torch._C._cuda_getDeviceCount() if nvml_count &lt; 0 else nvml_count"
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#a-simple-gaussian-process-2",
    "href": "data_science_for_em/05/05_gp2.html#a-simple-gaussian-process-2",
    "title": "Definition 1",
    "section": "A Simple Gaussian Process 2",
    "text": "A Simple Gaussian Process 2\ndef lin_func(x, n_sample):\n    preds = np.zeros((n_sample, x.shape[0]))\n    for ii in range(n_sample):\n        w = np.random.normal(0, 1, 2)\n        y = w[0] + w[1] * x\n        preds[ii, :] = y\n    return preds\n\nx_points = np.linspace(-5, 5, 50)\nouts = lin_func(x_points, 10)\nlw_bd = -2 * np.sqrt((1 + x_points ** 2))\nup_bd = 2 * np.sqrt((1 + x_points ** 2))\n\nd2l.set_figsize((12,5))\nd2l.plt.fill_between(x_points, lw_bd, up_bd, alpha=0.25)\nd2l.plt.plot(x_points, np.zeros(len(x_points)), linewidth=4, color='black')\nd2l.plt.plot(x_points, outs.T)\nd2l.plt.xlabel(\"x\", fontsize=20)\nd2l.plt.ylabel(\"f(x)\", fontsize=20)\nd2l.plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf \\(w_0\\) and \\(w_1\\) are instead drawn from \\(\\mathcal{N}(0,\\alpha^2)\\), how do you imagine varying \\(\\alpha\\) affects the distribution over functions?"
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#from-weight-space-to-function-space-1",
    "href": "data_science_for_em/05/05_gp2.html#from-weight-space-to-function-space-1",
    "title": "Definition 1",
    "section": "From Weight Space to Function Space 1",
    "text": "From Weight Space to Function Space 1\n\nwe saw how a distribution over parameters in a modelinduces a distribution over functions.\noften have ideas about the functions we want to model — whether they’re smooth, periodic, quickly varying, etc. — relatively tedious to reason about the parameters, which are largely uninterpretable.\nGPs provide an easy mechanism to reason directly about functions.\nGaussian distribution is entirely defined by its first two moments, its mean and covariance matrix, a Gaussian process by extension is defined by its mean function and covariance function.\n\nIn the above example, the mean function\n\\[m(x) = E[f(x)] = E[w_0 + w_1x] = E[w_0] + E[w_1]x = 0+0 = 0.\\]\nSimilarly, the covariance function is\n\\[k(x,x') = \\mathrm{Cov}(f(x),f(x')) = E[f(x)f(x')]-E[f(x)]E[f(x')] = \\\\ E[w_0^2 + w_0w_1x' + w_1w_0x + w_1^2xx'] = 1 + xx'.\\]"
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#from-weight-space-to-function-space-2",
    "href": "data_science_for_em/05/05_gp2.html#from-weight-space-to-function-space-2",
    "title": "Definition 1",
    "section": "From Weight Space to Function Space 2",
    "text": "From Weight Space to Function Space 2\n\ndistribution over functions can now be directly specified and sampled from, without needing to sample from the distribution over parameters.\nFor example, to draw from \\(f(x)\\), we can simply form our multivariate Gaussian distribution associated with any collection of \\(x\\) we want to query, and sample from it directly.\nvery advantageous\nsame derivation for the simple straight line model above can be applied to find the mean and covariance function for any model of the form \\(f(x) = w^{\\top} \\phi(x)\\), with \\(w \\sim \\mathcal{N}(u,S)\\).\nIn this case, the mean function \\(m(x) = u^{\\top}\\phi(x)\\), and the covariance function \\(k(x,x') = \\phi(x)^{\\top}S\\phi(x')\\). Since \\(\\phi(x)\\) can represent a vector of any non-linear basis functions, we are considering a very general model class, including models with an even an infinite number of parameters."
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-1",
    "href": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-1",
    "title": "Definition 1",
    "section": "The Radial Basis Function (RBF) Kernel 1",
    "text": "The Radial Basis Function (RBF) Kernel 1\n\nradial basis function (RBF) kernel is the most popular covariance function for Gaussian processes\nkernel has the form \\(k_{\\text{RBF}}(x,x') = a^2\\exp\\left(-\\frac{1}{2\\ell^2}||x-x'||^2\\right)\\), where \\(a\\) is an amplitude parameter, and \\(\\ell\\) is a lengthscale hyperparameter.\n\nLet’s derive this kernel starting from weight space. Consider the function\n\\[f(x) = \\sum_{i=1}^J w_i \\phi_i(x), w_i  \\sim \\mathcal{N}\\left(0,\\frac{\\sigma^2}{J}\\right), \\phi_i(x) = \\exp\\left(-\\frac{(x-c_i)^2}{2\\ell^2 }\\right).\\]\n\\(f(x)\\) is a sum of radial basis functions, with width \\(\\ell\\), centred at the points \\(c_i\\), as shown in the following figure.\n\nWe can recognize \\(f(x)\\) as having the form \\(w^{\\top} \\phi(x)\\), where \\(w = (w_1,\\dots,w_J)^{\\top}\\) and \\(\\phi(x)\\) is a vector containing each of the radial basis functions. The covariance function of this Gaussian process is then\n\n\\[k(x,x') = \\frac{\\sigma^2}{J} \\sum_{i=1}^{J} \\phi_i(x)\\phi_i(x').\\]"
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-2",
    "href": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-2",
    "title": "Definition 1",
    "section": "The Radial Basis Function (RBF) Kernel 2",
    "text": "The Radial Basis Function (RBF) Kernel 2\n\nwhat happens as we take the number of parameters (and basis functions) to infinity. Let \\(c_J = \\log J\\), \\(c_1 = -\\log J\\), and \\(c_{i+1}-c_{i} = \\Delta c = 2\\frac{\\log J}{J}\\), and \\(J \\to \\infty\\). The covariance function becomes the Riemann sum:\n\n\\[k(x,x') = \\lim_{J \\to \\infty} \\frac{\\sigma^2}{J} \\sum_{i=1}^{J} \\phi_i(x)\\phi_i(x') = \\int_{c_0}^{c_\\infty} \\phi_c(x)\\phi_c(x') dc.\\]\nBy setting \\(c_0 = -\\infty\\) and \\(c_\\infty = \\infty\\), we spread the infinitely many basis functions across the whole real line, each a distance \\(\\Delta c \\to 0\\) apart:\n\\[k(x,x') = \\int_{-\\infty}^{\\infty} \\exp(-\\frac{(x-c)^2}{2\\ell^2}) \\exp(-\\frac{(x'-c)^2}{2\\ell^2 }) dc = \\sqrt{\\pi}\\ell \\sigma^2 \\exp(-\\frac{(x-x')^2}{2(\\sqrt{2} \\ell)^2}) \\propto k_{\\text{RBF}}(x,x').\\]\n\nBy moving into the function space representation, we have derived how to represent a model with an infinite number of parameters, using a finite amount of computation.\nGP with an RBF kernel is a universal approximator, capable of representing any continuous function to arbitrary precision."
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-3",
    "href": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-3",
    "title": "Definition 1",
    "section": "The Radial Basis Function (RBF) Kernel 3",
    "text": "The Radial Basis Function (RBF) Kernel 3\n\nWe can intuitively see why from the above derivation.\nWe can collapse each radial basis function to a point mass taking \\(\\ell \\to 0\\), and give each point mass any height we wish.\nGP with an RBF kernel is a model with an infinite number of parameters and much more flexibility than any finite neural network\nall the fuss about overparametrized neural networks is misplaced?\nGPs with RBF kernels do not overfit, and in fact provide especially compelling generalization performance on small datasets.\nexamples in Zhang 2021, such as the ability to fit images with random labels perfectly, but still generalize well on structured problems, (can be perfectly reproduced using Gaussian processes) Wilson 2020.\nNeural networks are not as distinct as we make them out to be."
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-4",
    "href": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-4",
    "title": "Definition 1",
    "section": "The Radial Basis Function (RBF) Kernel 4",
    "text": "The Radial Basis Function (RBF) Kernel 4\n\nbuild further intuition about GPs with RBF kernels, and hyperparameters such as length-scale, by sampling directly from the distribution over functions.\nsimple procedure:\n\n\nChoose the input \\(x\\) points we want to query the GP: \\(x_1,\\dots,x_n\\).\nEvaluate \\(m(x_i)\\), \\(i = 1,\\dots,n\\), and \\(k(x_i,x_j)\\) for \\(i,j = 1,\\dots,n\\) to respectively form the mean vector and covariance matrix \\(\\mu\\) and \\(K\\), where \\((f(x_1),\\dots,f(x_n)) \\sim \\mathcal{N}(\\mu, K)\\).\nSample from this multivariate Gaussian distribution to obtain the sample function values.\nSample more times to visualize more sample functions queried at those points.\n\nWe illustrate this process in the figure below."
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-5",
    "href": "data_science_for_em/05/05_gp2.html#the-radial-basis-function-rbf-kernel-5",
    "title": "Definition 1",
    "section": "The Radial Basis Function (RBF) Kernel 5",
    "text": "The Radial Basis Function (RBF) Kernel 5\n\ndef rbfkernel(x1, x2, ls=4.):  #@save\n    dist = distance_matrix(np.expand_dims(x1, 1), np.expand_dims(x2, 1))\n    return np.exp(-(1. / ls / 2) * (dist ** 2))\n\nx_points = np.linspace(0, 5, 50)\nmeanvec = np.zeros(len(x_points))\ncovmat = rbfkernel(x_points,x_points, 1)\n\nprior_samples= np.random.multivariate_normal(meanvec, covmat, size=5);\nd2l.plt.plot(x_points, prior_samples.T, alpha=0.5)\nd2l.plt.show()"
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#the-neural-network-kernel-1",
    "href": "data_science_for_em/05/05_gp2.html#the-neural-network-kernel-1",
    "title": "Definition 1",
    "section": "The Neural Network Kernel 1",
    "text": "The Neural Network Kernel 1\n\nResearch on Gaussian processes in machine learning was triggered by research on neural networks.\nWe can derive the neural network kernel as follows.\n\nConsider a neural network function \\(f(x)\\) with one hidden layer:\n\\[f(x) = b + \\sum_{i=1}^{J} v_i h(x; u_i).\\]\n\\(b\\) is a bias, \\(v_i\\) are the hidden to output weights, \\(h\\) is any bounded hidden unit transfer function, \\(u_i\\) are the input to hidden weights, and \\(J\\) is the number of hidden units.\n\nLet \\(b\\) and \\(v_i\\) be independent with zero mean and variances \\(\\sigma_b^2\\) and \\(\\sigma_v^2/J\\), respectively, and let the \\(u_i\\) have independent identical distributions.\nuse the central limit theorem to show that any collection of function values \\(f(x_1),\\dots,f(x_n)\\) has a joint multivariate Gaussian distribution."
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#the-neural-network-kernel-2",
    "href": "data_science_for_em/05/05_gp2.html#the-neural-network-kernel-2",
    "title": "Definition 1",
    "section": "The Neural Network Kernel 2",
    "text": "The Neural Network Kernel 2\nThe mean and covariance function of the corresponding Gaussian process are:\n\\[m(x) = E[f(x)] = 0\\]\n\\[k(x,x') = \\text{cov}[f(x),f(x')] = E[f(x)f(x')] = \\sigma_b^2 + \\frac{1}{J} \\sum_{i=1}^{J} \\sigma_v^2 E[h_i(x; u_i)h_i(x'; u_i)]\\]\nIn some cases, we can essentially evaluate this covariance function in closed form. Let \\(h(x; u) = \\text{erf}(u_0 + \\sum_{j=1}^{P} u_j x_j)\\), where \\(\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{z} e^{-t^2} dt\\), and \\(u \\sim \\mathcal{N}(0,\\Sigma)\\). Then \\(k(x,x') = \\frac{2}{\\pi} \\text{sin}(\\frac{2 \\tilde{x}^{\\top} \\Sigma \\tilde{x}'}{\\sqrt{(1 + 2 \\tilde{x}^{\\top} \\Sigma \\tilde{x})(1 + 2 \\tilde{x}'^{\\top} \\Sigma \\tilde{x}')}})\\).\n\nRBF kernel is stationary, meaning that it is translation invariant, and therefore can be written as a function of \\(\\tau = x-x'\\).\nIntuitively, stationarity means that the high-level properties of the function, such as rate of variation, do not change as we move in input space.\nThe neural network kernel, however, is non-stationary."
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#summary",
    "href": "data_science_for_em/05/05_gp2.html#summary",
    "title": "Definition 1",
    "section": "Summary",
    "text": "Summary\n\nfirst step in performing Bayesian inference involves specifying a prior\nGPs can be used to specify a whole prior over functions.\nStarting from a traditional “weight space” view of modelling, induce a prior over functions by starting with the functional form of a model, and introducing a distribution over its parameters.\nalternatively specify a prior distribution directly in function space, with properties controlled by a kernel.\nfunction-space approach has many advantages. We can build models that actually correspond to an infinite number of parameters, but use a finite amount of computation!\nmodels have a great amount of flexibility, but also make strong assumptions about what types of functions are a priori likely, leading to relatively good generalization on small datasets.\nassumptions of models in function space controlled by kernels: encode higher level properties of functions, such as smoothness and periodicity"
  },
  {
    "objectID": "data_science_for_em/05/05_gp2.html#exercises",
    "href": "data_science_for_em/05/05_gp2.html#exercises",
    "title": "Definition 1",
    "section": "Exercises",
    "text": "Exercises\n\nDraw sample prior functions from a GP with an Ornstein-Uhlenbeck (OU) kernel, \\(k_{\\text{OU}}(x,x') = \\exp\\left(-\\frac{1}{2\\ell}||x - x'|\\right)\\). If you fix the lengthscale \\(\\ell\\) to be the same, how do these functions look different than sample functions from a GP with an RBF kernel?\nHow does changing the amplitude \\(a^2\\) of the RBF kernel affect the distribution over functions?\nSuppose we form \\(u(x) = f(x) + 2g(x)\\), where \\(f(x) \\sim \\mathcal{GP}(m_1,k_1)\\) and \\(g(x) \\sim \\mathcal{GP}(m_2,k_2)\\). Is \\(u(x)\\) a Gaussian process, and if so, what is its mean and covariance function?\nSuppose we form \\(g(x) = a(x)f(x)\\), where \\(f(x) \\sim \\mathcal{GP}(0,k)\\) and \\(a(x) = x^2\\). Is \\(g(x)\\) a Gaussian process, and if so, what is its mean and covariance function? What is the effect of \\(a(x)\\)? What do sample functions drawn from \\(g(x)\\) look like?\nSuppose we form \\(u(x) = f(x)g(x)\\), where \\(f(x) \\sim \\mathcal{GP}(m_1,k_1)\\) and \\(g(x) \\sim \\mathcal{GP}(m_2,k_2)\\). Is \\(u(x)\\) a Gaussian process, and if so, what is its mean and covariance function?"
  }
]