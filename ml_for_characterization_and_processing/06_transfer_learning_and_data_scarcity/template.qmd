---
title: |
  ML for Characterization and Processing<br>
  Lecture 6: Transfer Learning and Data Scarcity
bibliography: ref.bib
# csl: custom.csl
author:
  - name: Prof. Dr. Philipp Pelz
    affiliation: 
      - FAU Erlangen-Nürnberg
      - Institute of Micro- and Nanostructure Research
   
execute: 
  eval: true
  echo: true
format: 
    revealjs: 
        chalkboard: true
        mermaid:
            theme: forest
        # mermaid-format: png
        # scroll-view:
        #     activate: true
        #     snap: mandatory
        #     layout: full 
        width: 1920
        height: 1080
        menu:
            side: right
            width: wide 
        template-partials:
            - title-slide.html
        css: custom.css
        theme: custom.scss
        slide-number: c/t    
        logo: "eclipse_logo_small.png"          
        highlight-style: a11y
        incremental: true 
        background-transition: fade
        footer: "©Philipp Pelz - FAU Erlangen-Nürnberg - ML for Characterization and Processing"
---
 
 

## Welcome

### Week 6 — Transfer Learning & Data Scarcity in Materials Characterization

**Goals for today:**

- Understand why materials datasets are small  
- Learn how transfer learning helps (and when it fails)  
- Compare ImageNet vs modern foundation models  
- Explore self-supervised learning for microstructures  
- Learn how to train models with ~200 images  

---

## Outline

1. The data scarcity problem  
2. Transfer learning basics  
3. ImageNet vs domain-specific pretraining  
4. Foundation models: SAM, ViTs, MAE, DINO  
5. Self-supervised pretraining for microstructures  
6. How to train with 200 images  
7. Summary  

---

## 1. The Data Scarcity Problem

Materials datasets are tiny:

- SEM/TEM: 50–300 images  
- EBSD: 10–40 maps  
- AM melt pool videos: few sequences  
- XRD: typically <200 patterns  

Why?

- Experiments cost time & money  
- Labeling requires expert knowledge  
- High variability, limited reproducibility  
- Proprietary/industrial constraints  

**Deep learning was designed for 1–10 million images.  
We have 100–500.**

---

## Why Data Scarcity Breaks Vanilla Deep Learning

- Overfitting  
- Domain shift sensitivity  
- Poor generalization across instruments/materials  
- Lack of labeled examples  

**Solution:**  
Use transfer learning and self-supervised learning.

---

## 2. Transfer Learning Basics

Transfer learning = start from a pretrained model → adapt to your task.

Modes:

- **Feature extraction:** freeze most layers  
- **Fine-tuning:** allow deeper layers to adapt  
- **Full-domain adaptation:** retrain backbone with small LR  

Benefits:

- Reduces labeled data needs  
- Stabilizes training  
- Improves generalization  
- Helps with noisy labels  

---

## Why Transfer Learning Helps

Early CNN layers learn:

- Edges  
- Corners  
- Local textures  

These are universal and useful for:

- SEM  
- Optical microscopy  
- Fractography  
- Corrosion patterns  

But not always suitable for:

- TEM phase contrast  
- EBSD patterns  
- Periodic crystallographic contrast  

---

## 3. ImageNet vs Domain-Specific Pretraining

ImageNet models know:

- Dogs  
- Trees  
- Skies  
- Human-made objects  

But they do **not** know:

- Grains  
- Precipitates  
- Dislocation cells  
- Kikuchi patterns  
- AM melt pool textures  

---

## When ImageNet Transfer Works

- SEM with strong edge contrast  
- Optical microscopy  
- Fracture surfaces  
- Corrosion morphology  
- Low-frequency textures  

---

## When ImageNet Transfer Fails

- TEM: contrast oscillations, phase contrast, thickness effects  
- EBSD: crystallographic symmetry unseen in ImageNet  
- High-frequency periodic microstructures  
- Diffraction-like contrast  

**Conclusion:**  
ImageNet helps only if your data has natural-image statistics.

---

## 4. Foundation Models for Vision

Foundation models = large pretrained models designed to generalize.

Examples:

- Segment Anything Model (SAM)  
- Vision Transformers (ViT)  
- Masked Autoencoders (MAE)  
- DINO / DINOv2  
- CLIP  

These models learn broad visual priors independent of specific labels.

---

## Segment Anything Model (SAM)

Trained on 1B segmentation masks.

Strengths:

- Great for interactive segmentation  
- Very good boundary detection  
- Useful for semi-automated labeling  
- Zero-shot region proposals  

Weaknesses for materials:

- Underperforms on TEM  
- Weak on low-contrast SEM  
- Misinterprets crystallographic boundaries  
- Trained on natural imagery, not microstructures  

**SAM = Fantastic annotation tool, mediocre zero-shot microstructure segmenter.**

---

## SAM for Microscopy

Adaptations exist:

- Fine-tuned SAM for biological microscopy  
- Experimental SAM variants for SEM/TEM  

Uses in materials:

- Labeling precipitates with clicks  
- Creating weak labels for grain boundaries  
- Assisting segmentation of pores/defects  

**SAM accelerates annotation, not model performance.**

---

## Vision Transformers (ViT)

ViTs treat images as patches (tokens).  
Excellent at:
- Capturing long-range dependencies  
- Global texture representation  
- Morphology-level patterns  

Weak points:
- Need substantial pretraining  
- Overfit instantly with <1000 images  

ViTs + SSL = state-of-the-art representation learning for microstructures.

---

## Masked Autoencoders (MAE)

MAE masks ~75% of the image → model reconstructs missing content.

Perfect for microstructures:

- Learns grain texture prior  
- Learns precipitate morphology  
- Learns melt pool periodicity  
- Learns EBSD-like orientation textures  

MAE pretraining works even with **small unlabeled datasets**.

---

## DINO / DINOv2

Self-distillation for representation learning.

DINO strengths:

- Exceptional clustering of microstructure types  
- Good generalization across materials systems  
- Strong for similarity search and embedding extraction  

DINOv2 is a strong foundation model for materials, but benefits from domain-adapted fine-tuning.

---

## Foundation Model Summary

| Model | Usefulness for Materials | Notes |
|-------|---------------------------|-------|
| ImageNet ResNet | ★★☆☆☆ | Helps for SEM/optical only |
| SAM | ★★★☆☆ | Best annotation helper |
| ViT | ★★★★☆ | Needs pretraining; great for textures |
| MAE | ★★★★★ | Best SSL for microstructure |
| DINO / DINOv2 | ★★★★★ | Great embeddings and clustering |
| CLIP | ★★☆☆☆ | Limited value without text modality |

---

## 5. Self-Supervised Pretraining for Microstructures

SSL learns from *unlabeled* micrographs, ideal for data-scarce fields.

Approaches:

- Contrastive learning  
- Masked image modeling (MAE)  
- Rotation prediction  
- Jigsaw tasks  

---

## Why SSL is a Game-Changer

SSL learns:

- Grain boundary statistics  
- Phase textures  
- Precipitate morphology  
- Melt pool periodicity  
- Noise characteristics  
- Crystallographic patterns (if present)  

No human labels required.

---

## Contrastive Learning

Two augmented views → similar embedding.  
Different microstructures → distant embeddings.

Augmentations:

- Rotation (if allowed by physics)  
- Noise injection  
- Blur  
- Intensity remapping  
- Cropping  

**PYTHONHERE**  
(Contrastive learning workflow)

---

## Masked Microstructure Modeling (MAE)

Mask most of the micrograph → reconstruct.

Why it excels:

- SEM/TEM images have strong local redundancy  
- Grain boundaries can be reconstructed from context  
- AM cell patterns are periodic  
- Precipitates have predictable shapes  

MAE = best current method for pretraining microstructure backbones.

---

## Hybrid SSL + Supervised Fine-Tuning

Pipeline:

1. Gather unlabeled micrographs  
2. SSL pretraining (MAE, DINO)  
3. Extract embeddings  
4. Fine-tune on 50–200 labeled images  
5. Validate on sample-level split  

SSL reduces labeled-data needs by 5–10×.

---

## 6. How to Train with Only 200 Images

### Key steps:

1. **Patch extraction** (but split by specimen!)  
2. **Strong augmentations**: rotation, noise, blur, contrast  
3. **Use SSL** before supervised training  
4. **Use small backbones** (avoid huge models)  
5. **Regularization**: dropout, weight decay  
6. **Low LR fine-tuning**  
7. **Early stopping**  
8. **Sample-level validation**  

---

## Practical Pipeline

1. Collect 100–200 micrographs  
2. Extract 10,000–30,000 patches  
3. Pretrain backbone with SSL  
4. Train downstream task (classification, segmentation)  
5. Evaluate on new specimens  
6. Visualize embeddings to ensure physical meaning  

---

## 7. Summary

### Key insights:

- Materials datasets are small → transfer learning is essential  
- ImageNet helps, but only for SEM/optical  
- Foundation models (SAM, ViTs, MAE, DINO) are promising but require adaptation  
- Self-supervised learning (especially MAE/DINO) is ideal for microstructures  
- With SSL + augmentation, you can train robust models with 200 images  
- Microstructure ML is moving toward foundation-model-driven workflows  

Next week:  
**Week 7 — Multi-Modal Fusion: Combining Images, Spectra & Processing Data**

---

## Questions?

Use the chalkboard!


<div>
<script>
document.getElementById("marimo-frame").onload = function() {
    try {
        let iframeDoc = document.getElementById("marimo-frame").contentWindow.document;
        let marimoBadge = iframeDoc.querySelector("div.fixed.bottom-0.right-0.z-50");
        if (marimoBadge) {
            marimoBadge.style.display = "none";
            console.log("Marimo badge hidden successfully.");
        } else {
            console.log("Badge not found.");
        }
    } catch (error) {
        console.warn("Unable to modify iframe content due to CORS restrictions.");
    }
};
</script>
</div>