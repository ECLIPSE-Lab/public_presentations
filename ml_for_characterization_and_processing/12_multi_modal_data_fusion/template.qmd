---
title: |
  ML for Characterization and Processing<br>
  Lecture 12: Multi-Modal Data Fusion
bibliography: ref.bib
# csl: custom.csl
author:
  - name: Prof. Dr. Philipp Pelz
    affiliation: 
      - FAU Erlangen-Nürnberg
      - Institute of Micro- and Nanostructure Research
   
execute: 
  eval: true
  echo: true
format: 
    revealjs: 
        chalkboard: true
        mermaid:
            theme: forest
        # mermaid-format: png
        # scroll-view:
        #     activate: true
        #     snap: mandatory
        #     layout: full 
        width: 1920
        height: 1080
        menu:
            side: right
            width: wide 
        template-partials:
            - title-slide.html
        css: custom.css
        theme: custom.scss
        slide-number: c/t    
        logo: "eclipse_logo_small.png"          
        highlight-style: a11y
        incremental: true 
        background-transition: fade
        footer: "©Philipp Pelz - FAU Erlangen-Nürnberg - ML for Characterization and Processing"
---
 
 

## Welcome

### Week 12 — Multi-Modal Data Fusion

**Goals for today:**

- Understand why materials science requires multi-modal fusion  
- Represent images, spectra, and process logs in ML  
- Learn early fusion vs late fusion  
- Explore hybrid and cross-modal models  
- Build complete multi-modal pipelines  

---

## Outline

1. Why multi-modal data?  
2. Representations of each modality  
3. Early fusion  
4. Late fusion  
5. Hybrid & cross-modal learning  
6. Practical pipelines  
7. Summary  

---

## 1. Why Multi-Modal Data?

Materials data is rarely single-channel.

Examples:

- **Images** → morphology, grain boundaries, defects  
- **Spectra** → chemistry, bonding, phases  
- **Process logs** → energy input, kinetics, melt pool dynamics  
- **Composition** → alloying effects  

Fusion enables:

- Better accuracy  
- Better robustness  
- Better interpretability  
- More physics-consistent predictions  

---

## A Core Idea

Different modalities capture **different physics**.

Example:  

AM porosity prediction needs:
- Melt pool signals (process)  
- SEM pore images (structure)  
- XRD phase changes (chemistry)  

Only **jointly** do they describe the system.

---

## 2. Representations of Each Modality

### Images → learned embeddings
- CNN features  
- Transformer patches  
- Texture, edges, shapes  

### Spectra → vector embeddings
- PCA/NMF/ICA components  
- Peak descriptors  
- Spectral autoencoder embeddings  

### Process logs → temporal embeddings
- RNN or Transformer sequences  
- Engineered features (cooling rate, cycle count)  

### Metadata → tabular features
- Composition  
- Machine settings  
- Geometry  

---

## Representation Summary

All modalities eventually become **vectors**:

- \( z_{\text{img}} \)  
- \( z_{\text{spec}} \)  
- \( z_{\text{proc}} \)  

Fusion combines these vectors into a unified representation.

---

## 3. Early Fusion

### Concept
Fuse latent features **before** prediction.

\[
z = [z_{\text{img}},\; z_{\text{spec}},\; z_{\text{proc}}]
\]

Then feed into:
- MLP  
- Transformer  
- Gaussian Process  
- Classifier / regressor  

---

## Why Early Fusion?

Advantages:
- Captures interactions across modalities  
- Learns joint physics  
- Often highest accuracy  

Examples:
- EBSD + EDS → phase identification  
- AM melt pool + SEM → porosity prediction  
- T(t) + XRD → grain size prediction  

---

## Early Fusion Techniques

- Simple concatenation  
- Learned weighted fusion  
- Attention-based cross-modal fusion  
- Cross-modal transformers  
- Joint autoencoders  

Placeholder:

**PYTHONHERE**

(Embedding concatenation demo)

---

## Challenges of Early Fusion

- Requires larger datasets  
- Needs aligned modalities  
- Sensitive to missing data  
- Modalities differ in noise, scale, resolution  

→ Good when data is moderate/large and modalities interact strongly.

---

## 4. Late Fusion

### Concept
Fuse **predictions**, not features.

Each modality → independent model:
- Image model → \(p_1\)  
- Spectrum model → \(p_2\)  
- Process model → \(p_3\)  

Final outcome:
\[
p = w_1 p_1 + w_2 p_2 + w_3 p_3
\]

---

## Why Late Fusion?

Benefits:
- Works with small datasets  
- Easy to interpret  
- Robust to missing modalities  
- Independent modality training  

Examples:
- Separate XRD and SEM classifiers → fused for phase ID  
- Melt pool regressors + pore CNNs → fused porosity score  

---

## Strategies for Late Fusion

- Weighted averaging  
- Stacking with meta-model  
- Majority voting (classification)  
- Uncertainty weighting  

Late fusion is the “ensemble” of multimodal ML.

---

## When Late Fusion Wins

- Modalities don't interact strongly  
- Very small datasets  
- Noise varies between modalities  
- Interpretability is important  

---

## 5. Hybrid & Cross-Modal Learning

Middle ground between early and late fusion.

---

## Cross-Modal Attention

Models learn:
- Which spectral features relate to which microstructural regions  
- How process parameters influence spectral shifts  
- Which modalities should guide others  

Useful for:
- EBSD + EDS  
- SEM + XRD  
- Melt pool signals + AM images  

---

## Multi-Modal Autoencoders

One shared latent space reconstructs:
- Image  
- Spectra  
- Process history  

Applications:
- Discovery  
- Clustering  
- Noise reduction  
- Missing-modality imputation  

---

## Zero-Shot & Missing-Modality Learning

Models learn to infer:
- Spectra from images  
- Microstructure from processing  
- XRD from chemistry  
- EDS from TEM images  

This mimics how experts think.

---

## 6. Practical Multi-Modal Pipelines

### Pipeline A: EBSD + EDS → Phase ID
1. EBSD → orientation features  
2. EDS → chemical composition  
3. Early fusion → MLP  
4. Predict phase  

---

### Pipeline B: AM Melt Pool + SEM → Porosity
1. Melt pool log → RNN/Transformer  
2. SEM pore features → CNN  
3. Late fusion → porosity probability  

---

### Pipeline C: Heat-Treatment T(t) + XRD
1. Extract kinetic features from T(t)  
2. PCA/NMF embed XRD  
3. Fuse → Gaussian Process surrogate  
4. Predict grain size or phase fraction  

---

### Example fusion placeholder

**PYTHONHERE**

---

## 7. Summary

### Key insights:
- Materials problems are inherently multi-modal  
- Images, spectra, and process logs encode complementary physics  
- Early fusion: highest accuracy, needs more data  
- Late fusion: robust, interpretable, works with small sets  
- Hybrid fusion captures cross-modal relationships  
- Multi-modal ML underpins automation and closed-loop experiments  

Next week:  
**Week 13 — Closed-Loop Autonomous Experiments & Active Learning**

---

## Questions?

Use the chalkboard!


<div>
<script>
document.getElementById("marimo-frame").onload = function() {
    try {
        let iframeDoc = document.getElementById("marimo-frame").contentWindow.document;
        let marimoBadge = iframeDoc.querySelector("div.fixed.bottom-0.right-0.z-50");
        if (marimoBadge) {
            marimoBadge.style.display = "none";
            console.log("Marimo badge hidden successfully.");
        } else {
            console.log("Badge not found.");
        }
    } catch (error) {
        console.warn("Unable to modify iframe content due to CORS restrictions.");
    }
};
</script>
</div>