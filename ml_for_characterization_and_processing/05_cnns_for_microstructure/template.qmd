---
title: |
  ML for Characterization and Processing<br>
  Lecture 5: Convolutional Neural Networks for Microstructure Classification
bibliography: ref.bib
# csl: custom.csl
author:
  - name: Prof. Dr. Philipp Pelz
    affiliation: 
      - FAU Erlangen-Nürnberg
      - Institute of Micro- and Nanostructure Research
   
execute: 
  eval: true
  echo: true
format: 
    revealjs: 
        chalkboard: true
        mermaid:
            theme: forest
        # mermaid-format: png
        # scroll-view:
        #     activate: true
        #     snap: mandatory
        #     layout: full 
        width: 1920
        height: 1080
        menu:
            side: right
            width: wide 
        template-partials:
            - title-slide.html
        css: custom.css
        theme: custom.scss
        slide-number: c/t    
        logo: "eclipse_logo_small.png"          
        highlight-style: a11y
        incremental: true 
        background-transition: fade
        footer: "©Philipp Pelz - FAU Erlangen-Nürnberg - ML for Characterization and Processing"
---
 
## Welcome

### Week 5 — Convolutional Neural Networks for Microstructure Classification

**Goals for today:**

- Understand why CNNs work well for microstructures  
- Interpret CNN filters as microstructure detectors  
- Explore applications: grain boundaries, precipitates, melt pool defects  
- Learn training considerations specific to materials datasets  

---

## Outline

1. Why CNNs for microstructure?  
2. CNN fundamentals with materials intuition  
3. CNN filters as microstructure interpreters  
4. Example applications  
5. Training considerations for materials  
6. Summary  

---

## 1. Why CNNs for Microstructure?

Materials micrographs are hierarchical:

- Atomic contrast  
- Nanoscale morphology  
- Grain boundaries  
- Phase aggregates  
- Long-range texture  

CNNs respect:

- Spatial locality  
- Translation invariance  
- Multi-scale patterns  

Exactly what microstructure requires.

---

## Microstructure = Hierarchical Structure

Layers of structural information:

- Pixel-level: edges, boundaries  
- Intermediate: precipitates, pores  
- High-level: textures, phases, grain morphology  

CNNs learn this hierarchy automatically.

---

## 2. CNN Fundamentals (Materials Intuition)

No heavy math — intuitive, physical interpretations only.

---

## Convolutions as Learned Lineal-Intercept Statistics

A convolutional filter detects:

- Edges  
- Boundaries  
- Blobs  
- Oriented lines  

These correspond directly to metallurgical features:

- Grain boundaries  
- Precipitate edges  
- Melt pool cellular patterns  
- Slip bands  

---

## Feature Maps

Each convolution layer outputs a **map of where patterns occur**.

Early layers:  

- Edge locations  
- Grain-boundary contrast  

Middle layers:  

- Precipitate shape clusters  
- Pore contours  

Deep layers:  

- Martensitic laths  
- Cellular melt pool patterns  
- Grain morphology class  

---

## Pooling: Building Invariance

Pooling reduces sensitivity to:

- Minor shifts  
- Noise  
- Illumination variations  
- Sample prep artifacts  

This resembles how metallurgists summarize structure over regions.

---

## Fully Connected Layers = Decision Logic

After convolutional layers extract representations, dense layers classify based on:

- Presence of features  
- Relationships between patterns  
- Global structural identity  

---

## 3. CNN Filters as Microstructure Interpreters

Interpretation is the core motivation for using CNNs in materials science.

---

## First-Layer Filters

They learn:

- Sobel-like edge detectors  
- Small blob detectors  
- Orientation-specific filters  

These rediscover:

- Grain-boundary detection  
- Precipitate outline detection  
- Lineal-intercept directions  

---

## Mid-Layer Filters

They respond to:

- Precipitate clusters  
- Melt pool cell walls  
- Orientation-related textures  
- Martensite packet patterns  

These layers form “microstructure concepts.”

---

## Deep-Layer Filters

They detect:

- Bulk morphology (bainite/pearlite/martensite)  
- Columnar vs equiaxed grain structures  
- AM melt pool signature  
- Segregation bands  

Deep layers identify **microstructure class**, not individual pixels.

---

## Visualizing Filters and Activations

Useful tools:

- Activation maps  
- Filter visualizations  
- Saliency maps  
- Grad-CAM overlays  

These reveal *what the CNN pays attention to*.

**PYTHONHERE**  
(Filters visualization code placeholder)

---

## 4. Example Applications in Materials Science

We now explore three practical CNN tasks.

---

## Application 1: Grain-Boundary Segmentation

### Why important:

- Grain size ↔ mechanical strength  
- Grain boundaries ↔ corrosion, creep, fatigue  
- Required for quantitative microstructure models  

### CNN role:

- Replace manual thresholding  
- Replace watershed segmentation  
- Handle uneven etching, contrast drift, noise  

### CNN learns:

- Boundary edges  
- Confounding phase boundaries  
- Triple junctions  
- Boundary curvature patterns  

---

## Application 2: Precipitate Detection

### Why important:

- Precipitate size/spacing → creep resistance  
- Strengthening mechanisms depend on morphology  
- Manual annotation is extremely slow  

### CNN role:

- Pixelwise segmentation  
- Object detection  
- Counting and sizing in one pass  

### CNN learns:

- Blob-like features  
- Precipitate shape classes (spherical, rod-like, plate-like)  
- Distinguish pores vs precipitates  

---

## Application 3: Melt Pool Defect Classification (Additive Manufacturing)

### Why important:

- Porosity and lack-of-fusion defects → fatigue failure  
- Melt pool instability is a production-limiting factor  

### CNN role:

- Classify melt pool defects  
- Localize pores  
- Analyze layer-wise microstructure consistency  

### CNN learns:

- AM cell boundaries  
- Thermal patterns  
- Microporosity signatures  
- Distinguishes surface artifacts from true subsurface defects  

---

## 5. Training Considerations for Materials

Materials datasets come with unique constraints.

---

## Small Datasets

Mitigations:

- Patch extraction  
- Rotations, flips  
- Intensity jitter (simulate etching variation)  
- Noise injection (simulate detector noise)  
- Transfer learning (limited but sometimes valuable)  

---

## Domain Shift

Major sources:

- Magnification  
- Detector type  
- Etching quality  
- Sample thickness  
- Operator differences  

Strategies:

- Style-augmentation  
- Domain adaptation  
- Normalization that accounts for per-image statistics  

---

## Label Noise

Grain boundaries are fuzzy.  
Phases overlap.  
Annotators disagree.

Solutions:

- Label smoothing  
- Multi-annotator consensus  
- Soft boundaries  
- Uncertainty modeling  

---

## Preventing Overfitting

Key techniques:

- Sample-level splits  
- Instrument-level splits  
- Visualizing CNN attention to detect “cheating”  
- Cross-material testing  

---

## 6. Summary

### Key insights today:

- CNNs naturally match the hierarchical structure of microstructures  
- Learned filters correspond to physically meaningful features  
- Grain boundaries, precipitates, and AM defects are ideal CNN tasks  
- Materials datasets require special augmentations and careful splitting  
- CNNs extend classical quantification into powerful, learned representations  

Next week:  
**Transfer Learning, Pretraining, and Data Scarcity in Materials ML**

---

## Questions?

Use the chalkboard!


<div>
<script>
document.getElementById("marimo-frame").onload = function() {
    try {
        let iframeDoc = document.getElementById("marimo-frame").contentWindow.document;
        let marimoBadge = iframeDoc.querySelector("div.fixed.bottom-0.right-0.z-50");
        if (marimoBadge) {
            marimoBadge.style.display = "none";
            console.log("Marimo badge hidden successfully.");
        } else {
            console.log("Badge not found.");
        }
    } catch (error) {
        console.warn("Unable to modify iframe content due to CORS restrictions.");
    }
};
</script>
</div>