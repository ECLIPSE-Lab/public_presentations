---
title: |
  ML for Characterization and Processing<br>
  Lecture 3: Experimental Data Quality & ML-Readiness
bibliography: ref.bib
# csl: custom.csl
author:
  - name: Prof. Dr. Philipp Pelz
    affiliation: 
      - FAU Erlangen-Nürnberg
      - Institute of Micro- and Nanostructure Research
   
execute: 
  eval: true
  echo: true
format: 
    revealjs: 
        chalkboard: true
        mermaid:
            theme: forest
        # mermaid-format: png
        # scroll-view:
        #     activate: true
        #     snap: mandatory
        #     layout: full 
        width: 1920
        height: 1080
        menu:
            side: right
            width: wide 
        template-partials:
            - title-slide.html
        css: custom.css
        theme: custom.scss
        slide-number: c/t    
        logo: "eclipse_logo_small.png"          
        highlight-style: a11y
        incremental: true 
        background-transition: fade
        footer: "©Philipp Pelz - FAU Erlangen-Nürnberg - ML for Characterization and Processing"
---
 

## Welcome

### Week 3 — Experimental Data Quality & ML-Readiness

**Goals for today:**

- Understand annotation & segmentation challenges  
- Explore inter-annotator variance and label noise  
- Learn how train/test leakage appears in materials workflows  
- Practice detecting leakage and uncertainty  

---

## Outline

1. Why data quality matters  
2. Annotation & segmentation in materials science  
3. Inter-annotator variance  
4. Train/test leakage  
5. Mini-lab  
6. Summary  

---

## 1. Why Data Quality Matters

### Materials ML is uniquely vulnerable:

- Data are **scarce**  
- Data are **expensive**  
- Data are **noisy** (physics + instrumentation)  
- Data are **biased** (sample prep, operators, instruments)  
- Labels are often **ambiguous**  

**Most ML failures in materials science come from the data, not the model.**

---

## Examples of data-induced ML failures

- A grain-size model learns *etching quality*, not grain boundaries  
- A phase classifier learns *magnification*, not material differences  
- Hardness predictor learns *lab identity*, not microstructure  
- EBSD segmentation model learns *pattern brightness*, not orientation  

**Data provenance matters.**

---

## 2. Annotation & Segmentation

### Why annotation is difficult

Materials images rarely have clear truth:

- Fuzzy grain boundaries  
- Phase interpenetration  
- Contrast changes across field of view  
- TEM CTF effects create false edges  

Labels are **uncertain**, not absolute.

---

## Types of annotations

- Segmentation masks (grains, phases, precipitates)  
- Keypoints (dislocations, voids)  
- Bounding boxes (defects, inclusions)  
- Pixelwise labels (EBSD, STEM-ADF)  
- Continuous labels (grain size, porosity, hardness)  

Each carries different uncertainty.

---

## Problems with manual segmentation

- Different operators → different boundaries  
- Inconsistent grain definitions  
- Ambiguous phases  
- Annotator fatigue  
- Zoom-level dependence  

This creates **annotation bias**.

---

## Automatic segmentation pitfalls

Algorithms create *algorithmic bias*:

- Watershed → small-grain oversegmentation  
- Thresholding → sensitivity to illumination  
- Region-growing → thick or eroded boundaries  
- Deep learning masks → inconsistent edge softness  

**ML can end up learning the algorithm, not the physics.**

---

## Improving segmentation quality

Recommended strategies:

- Overlay masks on images for inspection  
- Use multiple segmentation algorithms  
- Create consensus masks  
- Use weak labels + refinement networks  
- Track segmentation uncertainty  
- Avoid “hard” labels when boundaries are fuzzy  

---

## 3. Inter-Annotator Variance

### Three people annotate the same grain map…

You will see:

- Different boundary placement  
- Different interpretation of small grains  
- Rounding artifacts  
- Disagreement on phase boundaries  

**Labels are distributions, not truths.**

---

## Quantifying label uncertainty

Useful metrics:

- IoU (Intersection over Union)  
- Boundary displacement error  
- Precision/recall on morphological features  
- Krippendorff’s alpha (general reliability metric)  

High disagreement → low achievable ML accuracy.

---

## Strategies to mitigate variance

- Use **multiple annotators**  
- Combine masks with majority vote  
- Train with **soft labels** instead of hard masks  
- Apply label smoothing  
- Model label uncertainty directly  
- Pretrain with self-supervised learning (less label reliance)

---

## 4. Train/Test Leakage  
### The most common and most devastating mistake in materials ML.

**Leakage = information that sneaks from training to test sets.**

Materials ML often *accidentally cheats*.

---

## Leakage Scenario 1  
### Adjacent crops from the same micrograph

If training tiles and test tiles come from the same image:

- The model memorizes texture  
- Results look excellent (97% accuracy)  
- True generalization is poor (≈ 50%)

---

## Leakage Scenario 2  
### Same sample, different magnifications

Model learns **magnification**, not material phase.

Examples:

- SEM images with huge contrast shifts  
- TEM images at 50k vs 300k magnification  
- EBSD images with different step sizes  

---

## Leakage Scenario 3  
### Same alloy batch, different processing

ML learns:

- Etching variations  
- Surface roughness  
- Charging artifacts  
- Operator patterns  
- Instrument settings  

Not the microstructure.

---

## Leakage Scenario 4  
### Metadata leakage

Dangerous metadata includes:

- Filenames (“sample3_phaseA.png”)  
- Scan numbers  
- Beam current  
- Acquisition date  
- Detector gain  

This is **non-physical information** the model should not have.

---

## Leakage Scenario 5  
### Algorithmic leakage

Using **the same segmentation algorithm**:

- to generate labels  
- and to extract features  
- and to evaluate model performance

ML learns the algorithm’s quirks, not ground truth.

---

## How to detect leakage

Ask:

- Do train/test splits contain **the same physical sample**?  
- Do splits contain **adjacent crops** of the same micrograph?  
- Do images contain **scale bars or magnification markers**?  
- Are filenames encoded with labels?  
- Do training and test datasets come from distinct batches or instruments?

---

## How to prevent leakage

### Golden rule:
**Split by physical sample, not by image tile.**

Recommended split levels:

- Sample-level split ✔  
- Batch-level split ✔  
- Instrument-level split ✔  
- Tile-level split ✘ (NEVER)  

For EBSD:  
- Split by scan, not by pattern.

For TEM/SEM:  
- Split by specimen or by session.

---

## Demonstration: Leakage in code (conceptual)



<div>
<script>
document.getElementById("marimo-frame").onload = function() {
    try {
        let iframeDoc = document.getElementById("marimo-frame").contentWindow.document;
        let marimoBadge = iframeDoc.querySelector("div.fixed.bottom-0.right-0.z-50");
        if (marimoBadge) {
            marimoBadge.style.display = "none";
            console.log("Marimo badge hidden successfully.");
        } else {
            console.log("Badge not found.");
        }
    } catch (error) {
        console.warn("Unable to modify iframe content due to CORS restrictions.");
    }
};
</script>
</div>